<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="https://kubevirt.io//feed/news.xml" rel="self" type="application/atom+xml" /><link href="https://kubevirt.io//" rel="alternate" type="text/html" /><updated>2023-09-05T09:17:11+00:00</updated><id>https://kubevirt.io//feed/news.xml</id><title type="html">KubeVirt.io | News</title><subtitle>Virtual Machine Management on Kubernetes</subtitle><entry><title type="html">Managing KubeVirt VMs with Ansible</title><link href="https://kubevirt.io//2023/Managing-KubeVirt-VMs-with-Ansible.html" rel="alternate" type="text/html" title="Managing KubeVirt VMs with Ansible" /><published>2023-09-05T00:00:00+00:00</published><updated>2023-09-05T00:00:00+00:00</updated><id>https://kubevirt.io//2023/Managing-KubeVirt-VMs-with-Ansible</id><content type="html" xml:base="https://kubevirt.io//2023/Managing-KubeVirt-VMs-with-Ansible.html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>Infrastructure teams managing virtual machines (VMs) and the end users of these systems make use of a variety of tools as part of their day-to-day world. One such tool that is shared amongst these two groups is Ansible, an agentless automation tool for the enterprise. To simplify both the adoption and usage of KubeVirt as well as to integrate seamlessly into existing workflows, the KubeVirt community is excited to introduce the release of the first version of the KubeVirt collection for <a href="https://docs.ansible.com/ansible/latest/index.html">Ansible</a>, called <code class="language-plaintext highlighter-rouge">kubevirt.core</code>, which includes a number of tools that you do not want to miss.</p>

<p>This article will review some of the features and their use associated with this initial release.</p>

<p>Note: There is also a video version of this blog, which can be found on the <a href="https://youtu.be/GVROaPgJD_8">KubeVirt YouTube channel</a>.</p>

<h2 id="motivation">Motivation</h2>

<p>Before diving into the featureset of the collection itself, let’s review why the collection was created in the first place.</p>

<p>While adopting KubeVirt and Kubernetes has the potential to disrupt the workflows of teams that typically manage VM infrastructure, including the end users themselves, many of the same paradigms remain:</p>

<ul>
  <li>Kubernetes and the resources associated with KubeVirt can be represented in a declarative fashion.</li>
  <li>In many cases, communicating with KubeVirt VMs makes use of the same protocols and schemes as non-Kubernetes-based environments.</li>
  <li>The management of VMs still represents a challenge.</li>
</ul>

<p>For these reasons and more, it is only natural that a tool, like Ansible, is introduced within the KubeVirt community. Not only can it help manage KubeVirt and Kubernetes resources, like <code class="language-plaintext highlighter-rouge">VirtualMachines</code>, but also to enable the extensive Ansible ecosystem for managing guest configurations.</p>

<h2 id="included-capabilities">Included capabilities</h2>

<p>As part of the initial release, an <a href="https://docs.ansible.com/ansible/latest/plugins/inventory.html">Ansible Inventory plugin</a> and management module is included. They are available in the same distribution location containing Ansible automation content, <a href="https://galaxy.ansible.com/kubevirt/core">Ansible Galaxy</a>. The resources encompassing the collection itself are detailed in the following sections.</p>

<h3 id="inventory">Inventory</h3>

<p>To work with KubeVirt VMs in Ansible, they need to be available in Ansible’s hosts <a href="https://docs.ansible.com/ansible/latest/inventory_guide/intro_inventory.html">inventory</a>. Since KubeVirt is already using the Kubernetes API to manage VMs, it would be nice to leverage this API to discover hosts with Ansible too. This is where the <a href="https://docs.ansible.com/ansible/latest/inventory_guide/intro_dynamic_inventory.html">dynamic inventory</a> of the <code class="language-plaintext highlighter-rouge">kubevirt.core</code> collection comes into play.</p>

<p>The dynamic inventory capability allows you to query the Kubernetes API for available VMs in a given namespace or namespaces, along with additional filtering options, such as labels. To allow Ansible to find the right connection parameters for a VM, the network name of a secondary interface can also be specified.</p>

<p>Under the hood, the dynamic inventory uses either your default kubectl credentials or credentials specified in the inventory parameters to establish the connection with a cluster.</p>

<h3 id="managing-vms">Managing VMs</h3>

<p>While working with existing VMs is already quite useful, it would be even better to control the entire lifecycle of KubeVirt <code class="language-plaintext highlighter-rouge">VirtualMachines</code> from Ansible. This is made possible by the <code class="language-plaintext highlighter-rouge">kubevirt_vm</code> module provided by the <code class="language-plaintext highlighter-rouge">kubevirt.core</code> collection.</p>

<p>The <code class="language-plaintext highlighter-rouge">kubevirt_vm</code> module is a thin wrapper around the <a href="https://docs.ansible.com/ansible/latest/collections/kubernetes/core/k8s_module.html">kubernetes.core.k8s</a> module and it allows you to control the essential fields of a KubeVirt <code class="language-plaintext highlighter-rouge">VirtualMachine</code>’s specification. In true Ansible fashion, this module tries to be as idempotent as possible and only makes changes to objects within Kubernetes if necessary. With its <code class="language-plaintext highlighter-rouge">wait</code> feature, it is possible to delay further tasks until a VM was successfully created or updated and the VM is in the ready state or was successfully deleted.</p>

<h2 id="getting-started">Getting started</h2>

<p>Now that we’ve provided an introduction to the featureset, it is time to illustrate how you can get up to speed using the collection including a few examples to showcase the capabilities provided by the collection.</p>

<h3 id="prerequisites">Prerequisites</h3>

<p>Please note that as a prerequisite, Ansible needs to be installed and configured along with a working Kubernetes cluster with KubeVirt and the <a href="https://github.com/kubevirt/cluster-network-addons-operator">KubeVirt Cluster Network Addons Operator</a>. The cluster also needs to have a <a href="https://kubevirt.io/user-guide/virtual_machines/interfaces_and_networks/#bridge">secondary network configured</a>, which can be attached to VMs so that the machine can be reached from the Ansible control node.</p>

<h3 id="items-covered">Items covered</h3>

<ol>
  <li>Installing the collection from Ansible Galaxy</li>
  <li>Creating a Namespace and a Secret with an SSH public key</li>
  <li>Creating a VM</li>
  <li>Listing available VMs</li>
  <li>Executing a command on the VM</li>
  <li>Removing the previously created resources</li>
</ol>

<h3 id="walkthrough">Walkthrough</h3>

<p>First, install the <code class="language-plaintext highlighter-rouge">kubevirt.core</code> collection from Ansible Galaxy:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ansible-galaxy collection <span class="nb">install </span>kubevirt.core
</code></pre></div></div>

<p>This will also install the <code class="language-plaintext highlighter-rouge">kubernetes.core</code> collection as a dependency.</p>

<p>Second, create a new Namespace and a Secret containing a public key for SSH authentication:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ssh-keygen <span class="nt">-f</span> my-key
kubectl create namespace kubevirt-ansible
kubectl create secret generic my-pub-key <span class="nt">--from-file</span><span class="o">=</span><span class="nv">key1</span><span class="o">=</span>my-key.pub <span class="nt">-n</span> kubevirt-ansible
</code></pre></div></div>

<p>With the collection now installed and the public key pair created, create a file called <code class="language-plaintext highlighter-rouge">play-create.yml</code> containing an Ansible playbook to deploy a new VM called <code class="language-plaintext highlighter-rouge">testvm</code>:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="pi">-</span> <span class="na">hosts</span><span class="pi">:</span> <span class="s">localhost</span>
  <span class="na">connection</span><span class="pi">:</span> <span class="s">local</span>
  <span class="na">tasks</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">Create VM</span>
    <span class="na">kubevirt.core.kubevirt_vm</span><span class="pi">:</span>
      <span class="na">state</span><span class="pi">:</span> <span class="s">present</span>
      <span class="na">name</span><span class="pi">:</span> <span class="s">testvm</span>
      <span class="na">namespace</span><span class="pi">:</span> <span class="s">kubevirt-ansible</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">app</span><span class="pi">:</span> <span class="s">test</span>
      <span class="na">instancetype</span><span class="pi">:</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">u1.medium</span>
      <span class="na">preference</span><span class="pi">:</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">fedora</span>
      <span class="na">spec</span><span class="pi">:</span>
        <span class="na">domain</span><span class="pi">:</span>
          <span class="na">devices</span><span class="pi">:</span>
            <span class="na">interfaces</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">default</span>
              <span class="na">masquerade</span><span class="pi">:</span> <span class="pi">{}</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">secondary-network</span>
              <span class="na">bridge</span><span class="pi">:</span> <span class="pi">{}</span>
        <span class="na">networks</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">default</span>
          <span class="na">pod</span><span class="pi">:</span> <span class="pi">{}</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">secondary-network</span>
          <span class="na">multus</span><span class="pi">:</span>
            <span class="na">networkName</span><span class="pi">:</span> <span class="s">secondary-network</span>
        <span class="na">accessCredentials</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">sshPublicKey</span><span class="pi">:</span>
            <span class="na">source</span><span class="pi">:</span>
              <span class="na">secret</span><span class="pi">:</span>
                <span class="na">secretName</span><span class="pi">:</span> <span class="s">my-pub-key</span>
            <span class="na">propagationMethod</span><span class="pi">:</span>
              <span class="na">configDrive</span><span class="pi">:</span> <span class="pi">{}</span>
        <span class="na">volumes</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">containerDisk</span><span class="pi">:</span>
            <span class="na">image</span><span class="pi">:</span> <span class="s">quay.io/containerdisks/fedora:latest</span>
          <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
        <span class="pi">-</span> <span class="na">cloudInitConfigDrive</span><span class="pi">:</span>
            <span class="na">userData</span><span class="pi">:</span> <span class="pi">|-</span>
              <span class="s">#cloud-config</span>
              <span class="s"># The default username is: fedora</span>
          <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinit</span>
      <span class="na">wait</span><span class="pi">:</span> <span class="s">yes</span>
</code></pre></div></div>

<p>Run the playbook by executing the following command:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ansible-playbook play-create.yml
</code></pre></div></div>

<p>Once the playbook completes successfully, the defined VM will be running in the <code class="language-plaintext highlighter-rouge">kubevirt-ansible</code> namespace, which can be confirmed by querying for <code class="language-plaintext highlighter-rouge">VirtualMachines</code> in this namespace:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl get VirtualMachine <span class="nt">-n</span> kubevirt-ansible
</code></pre></div></div>

<p>With the VM deployed, it is eligible for use in Ansible automation activities. Let’s illustrate how it can be queried and added to an Ansible inventory dynamically using the plugin provided by the <code class="language-plaintext highlighter-rouge">kubevirt.core</code> collection.</p>

<p>Create a file called <code class="language-plaintext highlighter-rouge">inventory.kubevirt.yml</code> containing the following content:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">plugin</span><span class="pi">:</span> <span class="s">kubevirt.core.kubevirt</span>
<span class="na">connections</span><span class="pi">:</span>
<span class="pi">-</span> <span class="na">namespaces</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">kubevirt-ansible</span>
  <span class="na">network_name</span><span class="pi">:</span> <span class="s">secondary-network</span>
  <span class="na">label_selector</span><span class="pi">:</span> <span class="s">app=test</span>
</code></pre></div></div>

<p>Use the <code class="language-plaintext highlighter-rouge">ansible-inventory</code> command to confirm the VM becomes added to the Ansible inventory:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ansible-inventory <span class="nt">-i</span> inventory.kubevirt.yml <span class="nt">--list</span>
</code></pre></div></div>

<p>Next, make use of the host by querying for all of the facts exposed by the VM using the setup module:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ansible <span class="nt">-i</span> inventory.kubevirt.yml <span class="nt">-u</span> fedora <span class="nt">--key-file</span> my-key all <span class="nt">-m</span> setup
</code></pre></div></div>

<p>Complete the lifecycle of the VM by destroying the previously created <code class="language-plaintext highlighter-rouge">VirtualMachine</code> and <code class="language-plaintext highlighter-rouge">Namespace</code>. Create a file called <code class="language-plaintext highlighter-rouge">play-delete.yml</code> containing the following playbook:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="pi">-</span> <span class="na">hosts</span><span class="pi">:</span> <span class="s">localhost</span>
  <span class="na">tasks</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">Delete VM</span>
    <span class="na">kubevirt.core.kubevirt_vm</span><span class="pi">:</span>
      <span class="na">name</span><span class="pi">:</span> <span class="s">testvm</span>
      <span class="na">namespace</span><span class="pi">:</span> <span class="s">kubevirt-ansible</span>
      <span class="na">state</span><span class="pi">:</span> <span class="s">absent</span>
      <span class="na">wait</span><span class="pi">:</span> <span class="s">yes</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">Delete namespace</span>
    <span class="na">kubernetes.core.k8s</span><span class="pi">:</span>
      <span class="na">name</span><span class="pi">:</span> <span class="s">kubevirt-ansible</span>
      <span class="na">api_version</span><span class="pi">:</span> <span class="s">v1</span>
      <span class="na">kind</span><span class="pi">:</span> <span class="s">Namespace</span>
      <span class="na">state</span><span class="pi">:</span> <span class="s">absent</span>
</code></pre></div></div>

<p>Run the playbook to remove the VM:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ansible-playbook play-delete.yml
</code></pre></div></div>

<p>More information including the full list of parameters and options can be found within the collection documentation:</p>

<p><a href="https://kubevirt.io/kubevirt.core">https://kubevirt.io/kubevirt.core</a></p>

<h2 id="what-next">What next?</h2>

<p>This has been a brief introduction to the concepts and usage of the newly released <code class="language-plaintext highlighter-rouge">kubevirt.core</code> collection. Nevertheless, we hope that it helped to showcase the integration now available between KubeVirt and Ansible, including how easy it is to manage KubeVirt assets. A next potential iteration could be to expose a VM via a Kubernetes <code class="language-plaintext highlighter-rouge">Service</code> using one of the methods described in <a href="https://kubevirt.io/user-guide/virtual_machines/service_objects/#service-objects">this article</a> instead of a secondary interface as was covered in this walkthrough. Not only does it leverage existing models outside the KubeVirt ecosystem, but it helps to enable a uniform method for exposing content.</p>

<p>Interested in learning more, providing feedback or contributing? Head over to the <code class="language-plaintext highlighter-rouge">kubevirt.core</code> GitHub repository to continue your journey and get involved.</p>

<p><a href="https://github.com/kubevirt/kubevirt.core">https://github.com/kubevirt/kubevirt.core</a></p>]]></content><author><name>Felix Matouschek, Andrew Block</name></author><category term="news" /><category term="Kubevirt" /><category term="kubernetes" /><category term="virtual machine" /><category term="VM" /><category term="Ansible" /><category term="ansible collection" /><category term="kubevirt.core" /><category term="iac" /><summary type="html"><![CDATA[This post explains how to manage KubeVirt VMs with the kubevirt.core Ansible collection.]]></summary></entry><entry><title type="html">NetworkPolicies for KubeVirt VMs secondary networks using OVN-Kubernetes</title><link href="https://kubevirt.io//2023/OVN-kubernetes-secondary-networks-policies.html" rel="alternate" type="text/html" title="NetworkPolicies for KubeVirt VMs secondary networks using OVN-Kubernetes" /><published>2023-07-24T00:00:00+00:00</published><updated>2023-07-24T00:00:00+00:00</updated><id>https://kubevirt.io//2023/OVN-kubernetes-secondary-networks-policies</id><content type="html" xml:base="https://kubevirt.io//2023/OVN-kubernetes-secondary-networks-policies.html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>Kubernetes <a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/">NetworkPolicies</a> are constructs to control traffic flow at the IP
address or port level (OSI layers 3 or 4).
They allow the user to specify how a pod (or group of pods) is allowed to
communicate with other entities on the network. In simpler words: the user can
specify ingress from or egress to other workloads, using L3 / L4 semantics.</p>

<p>Keeping in mind <code class="language-plaintext highlighter-rouge">NetworkPolicy</code> is a Kubernetes construct - which only cares
about a single network interface - they are only usable for the cluster’s
default network interface. This leaves a considerable gap for Virtual Machine
users, since they are heavily invested in secondary networks.</p>

<p>The <a href="https://github.com/k8snetworkplumbingwg">k8snetworkplumbingwg</a> has addressed this limitation by providing a
<code class="language-plaintext highlighter-rouge">MultiNetworkPolicy</code> CRD - it features the exact same API as <code class="language-plaintext highlighter-rouge">NetworkPolicy</code>
but can target <a href="https://github.com/k8snetworkplumbingwg/multus-cni/blob/master/docs/how-to-use.md#create-network-attachment-definition">network-attachment-definitions</a>.
<a href="https://github.com/ovn-org/ovn-kubernetes">OVN-Kubernetes</a> implements this API, and configures access control accordingly
for secondary networks in the cluster.</p>

<p>In this post we will see how we can govern access control for VMs using the
multi-network policy API. On our simple example, we’ll only allow into our VMs
for traffic ingressing from a particular CIDR range.</p>

<h2 id="current-limitations-of-multinetworkpolicies-for-vms">Current limitations of <code class="language-plaintext highlighter-rouge">MultiNetworkPolicies</code> for VMs</h2>
<p>Kubernetes <code class="language-plaintext highlighter-rouge">NetworkPolicy</code> has three types of policy peers:</p>
<ul>
  <li>namespace selectors: allows ingress-from, egress-to based on the peer’s namespace labels</li>
  <li>pod selectors: allows ingress-from, egress-to based on the peer’s labels</li>
  <li>ip block: allows ingress-from, egress-to based on the peer’s IP address</li>
</ul>

<p>While <code class="language-plaintext highlighter-rouge">MultiNetworkPolicy</code> allows these three types, when used with VMs we
recommend using <strong>only</strong> the <code class="language-plaintext highlighter-rouge">IPBlock</code> policy peer - both <code class="language-plaintext highlighter-rouge">namespace</code> and <code class="language-plaintext highlighter-rouge">pod</code>
selectors prevent the live-migration of Virtual Machines (these policy peers
require OVN-K managed IPAM, and currently the live-migration feature is only
available when IPAM is not enabled on the interfaces).</p>

<h2 id="demo">Demo</h2>
<p>To run this demo, we will prepare a Kubernetes cluster with the following
components installed:</p>
<ul>
  <li><a href="https://github.com/ovn-org/ovn-kubernetes">OVN-Kubernetes</a></li>
  <li><a href="https://github.com/k8snetworkplumbingwg/multus-cni">multus-cni</a></li>
  <li><a href="https://github.com/kubevirt/kubevirt">KubeVirt</a></li>
  <li><a href="https://github.com/k8snetworkplumbingwg/multi-networkpolicy">Multi-Network policy API</a></li>
</ul>

<p>The <a href="#setup-demo-environment">following section</a> will show you how to create a
<a href="https://kind.sigs.k8s.io/">KinD</a> cluster, with upstream latest OVN-Kubernetes,
upstream latest multus-cni, and the multi-network policy CRDs deployed.</p>

<h3 id="setup-demo-environment">Setup demo environment</h3>
<p>Refer to the OVN-Kubernetes repo
<a href="https://github.com/ovn-org/ovn-kubernetes/blob/master/docs/kind.md#ovn-kubernetes-kind-setup">KIND documentation</a>
for more details; the gist of it is you should clone the OVN-Kubernetes
repository, and run their kind helper script:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone git@github.com:ovn-org/ovn-kubernetes.git

<span class="nb">cd </span>ovn-kubernetes
<span class="nb">pushd </span>contrib <span class="p">;</span> ./kind.sh <span class="nt">--multi-network-enable</span> <span class="p">;</span> <span class="nb">popd</span>
</code></pre></div></div>

<p>This will get you a running kind cluster (one control plane, and two worker
nodes), configured to use OVN-Kubernetes as the default cluster network,
configuring the multi-homing OVN-Kubernetes feature gate, and deploying
<code class="language-plaintext highlighter-rouge">multus-cni</code> in the cluster.</p>

<h3 id="install-kubevirt-in-the-cluster">Install KubeVirt in the cluster</h3>
<p>Follow Kubevirt’s
<a href="https://kubevirt.io/user-guide/operations/installation/#installing-kubevirt-on-kubernetes">user guide</a>
to install the latest released version (currently, v1.0.0).</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">export </span><span class="nv">RELEASE</span><span class="o">=</span><span class="si">$(</span>curl https://storage.googleapis.com/kubevirt-prow/release/kubevirt/kubevirt/stable.txt<span class="si">)</span>
kubectl apply <span class="nt">-f</span> <span class="s2">"https://github.com/kubevirt/kubevirt/releases/download/</span><span class="k">${</span><span class="nv">RELEASE</span><span class="k">}</span><span class="s2">/kubevirt-operator.yaml"</span>
kubectl apply <span class="nt">-f</span> <span class="s2">"https://github.com/kubevirt/kubevirt/releases/download/</span><span class="k">${</span><span class="nv">RELEASE</span><span class="k">}</span><span class="s2">/kubevirt-cr.yaml"</span>
kubectl <span class="nt">-n</span> kubevirt <span class="nb">wait </span>kv kubevirt <span class="nt">--timeout</span><span class="o">=</span>360s <span class="nt">--for</span> <span class="nv">condition</span><span class="o">=</span>Available
</code></pre></div></div>

<p>Now we have a Kubernetes cluster with all the pieces to start the Demo.</p>

<h3 id="limiting-ingress-to-a-kubevirt-vm">Limiting ingress to a KubeVirt VM</h3>
<p>In this example, we will configure a <code class="language-plaintext highlighter-rouge">MultiNetworkPolicy</code> allowing ingress into
our VMs only from a particular CIDR range - let’s say <code class="language-plaintext highlighter-rouge">10.200.0.0/30</code>.</p>

<p>Provision the following NAD (to allow our VMs to live-migrate, we do not define
a <code class="language-plaintext highlighter-rouge">subnet</code>):</p>
<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">k8s.cni.cncf.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">NetworkAttachmentDefinition</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">flatl2net</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">config</span><span class="pi">:</span> <span class="pi">|2</span>
    <span class="s">{</span>
            <span class="s">"cniVersion": "0.4.0",</span>
            <span class="s">"name": "flatl2net",</span>
            <span class="s">"type": "ovn-k8s-cni-overlay",</span>
            <span class="s">"topology":"layer2",</span>
            <span class="s">"netAttachDefName": "default/flatl2net"</span>
    <span class="s">}</span>
</code></pre></div></div>

<p>Let’s now provision our six VMs, with the following name to IP address
(statically configured via cloud-init) association:</p>
<ul>
  <li>vm1: 10.200.0.1</li>
  <li>vm2: 10.200.0.2</li>
  <li>vm3: 10.200.0.3</li>
  <li>vm4: 10.200.0.4</li>
  <li>vm5: 10.200.0.5</li>
  <li>vm6: 10.200.0.6</li>
</ul>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">kubevirt.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachine</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">kubevirt.io/vm</span><span class="pi">:</span> <span class="s">vm1</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">vm1</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">running</span><span class="pi">:</span> <span class="no">true</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">access-control</span>
        <span class="na">kubevirt.io/domain</span><span class="pi">:</span> <span class="s">vm1</span>
        <span class="na">kubevirt.io/vm</span><span class="pi">:</span> <span class="s">vm1</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">domain</span><span class="pi">:</span>
        <span class="na">devices</span><span class="pi">:</span>
          <span class="na">disks</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">disk</span><span class="pi">:</span>
              <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
          <span class="pi">-</span> <span class="na">disk</span><span class="pi">:</span>
              <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
          <span class="na">interfaces</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">bridge</span><span class="pi">:</span> <span class="pi">{}</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">flatl2-overlay</span>
          <span class="na">rng</span><span class="pi">:</span> <span class="pi">{}</span>
        <span class="na">resources</span><span class="pi">:</span>
          <span class="na">requests</span><span class="pi">:</span>
            <span class="na">memory</span><span class="pi">:</span> <span class="s">1024Mi</span>
      <span class="na">networks</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">multus</span><span class="pi">:</span>
          <span class="na">networkName</span><span class="pi">:</span> <span class="s">flatl2net</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">flatl2-overlay</span>
      <span class="na">termination/GracePeriodSeconds</span><span class="pi">:</span> <span class="m">30</span>
      <span class="na">volumes</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">containerDisk</span><span class="pi">:</span>
          <span class="na">image</span><span class="pi">:</span> <span class="s">quay.io/kubevirt/fedora-with-test-tooling-container-disk:v1.0.0</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
      <span class="pi">-</span> <span class="na">cloudInitNoCloud</span><span class="pi">:</span>
          <span class="na">networkData</span><span class="pi">:</span> <span class="pi">|</span>
            <span class="s">ethernets:</span>
              <span class="s">eth0:</span>
                <span class="s">addresses:</span>
                <span class="s">- 10.200.0.1/24</span>
            <span class="s">version: 2</span>
          <span class="na">userData</span><span class="pi">:</span> <span class="pi">|-</span>
            <span class="s">#cloud-config</span>
            <span class="s">user: fedora</span>
            <span class="s">password: password</span>
            <span class="s">chpasswd: { expire: False }</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">kubevirt.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachine</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">kubevirt.io/vm</span><span class="pi">:</span> <span class="s">vm2</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">vm2</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">running</span><span class="pi">:</span> <span class="no">true</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">access-control</span>
        <span class="na">kubevirt.io/domain</span><span class="pi">:</span> <span class="s">vm2</span>
        <span class="na">kubevirt.io/vm</span><span class="pi">:</span> <span class="s">vm2</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">domain</span><span class="pi">:</span>
        <span class="na">devices</span><span class="pi">:</span>
          <span class="na">disks</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">disk</span><span class="pi">:</span>
              <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
          <span class="pi">-</span> <span class="na">disk</span><span class="pi">:</span>
              <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
          <span class="na">interfaces</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">bridge</span><span class="pi">:</span> <span class="pi">{}</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">flatl2-overlay</span>
          <span class="na">rng</span><span class="pi">:</span> <span class="pi">{}</span>
        <span class="na">resources</span><span class="pi">:</span>
          <span class="na">requests</span><span class="pi">:</span>
            <span class="na">memory</span><span class="pi">:</span> <span class="s">1024Mi</span>
      <span class="na">networks</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">multus</span><span class="pi">:</span>
          <span class="na">networkName</span><span class="pi">:</span> <span class="s">flatl2net</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">flatl2-overlay</span>
      <span class="na">termination/GracePeriodSeconds</span><span class="pi">:</span> <span class="m">30</span>
      <span class="na">volumes</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">containerDisk</span><span class="pi">:</span>
          <span class="na">image</span><span class="pi">:</span> <span class="s">quay.io/kubevirt/fedora-with-test-tooling-container-disk:v1.0.0</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
      <span class="pi">-</span> <span class="na">cloudInitNoCloud</span><span class="pi">:</span>
          <span class="na">networkData</span><span class="pi">:</span> <span class="pi">|</span>
            <span class="s">ethernets:</span>
              <span class="s">eth0:</span>
                <span class="s">addresses:</span>
                <span class="s">- 10.200.0.2/24</span>
            <span class="s">version: 2</span>
          <span class="na">userData</span><span class="pi">:</span> <span class="pi">|-</span>
            <span class="s">#cloud-config</span>
            <span class="s">user: fedora</span>
            <span class="s">password: password</span>
            <span class="s">chpasswd: { expire: False }</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">kubevirt.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachine</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">kubevirt.io/vm</span><span class="pi">:</span> <span class="s">vm3</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">vm3</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">running</span><span class="pi">:</span> <span class="no">true</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">access-control</span>
        <span class="na">kubevirt.io/domain</span><span class="pi">:</span> <span class="s">vm3</span>
        <span class="na">kubevirt.io/vm</span><span class="pi">:</span> <span class="s">vm3</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">domain</span><span class="pi">:</span>
        <span class="na">devices</span><span class="pi">:</span>
          <span class="na">disks</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">disk</span><span class="pi">:</span>
              <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
          <span class="pi">-</span> <span class="na">disk</span><span class="pi">:</span>
              <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
          <span class="na">interfaces</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">bridge</span><span class="pi">:</span> <span class="pi">{}</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">flatl2-overlay</span>
          <span class="na">rng</span><span class="pi">:</span> <span class="pi">{}</span>
        <span class="na">resources</span><span class="pi">:</span>
          <span class="na">requests</span><span class="pi">:</span>
            <span class="na">memory</span><span class="pi">:</span> <span class="s">1024Mi</span>
      <span class="na">networks</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">multus</span><span class="pi">:</span>
          <span class="na">networkName</span><span class="pi">:</span> <span class="s">flatl2net</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">flatl2-overlay</span>
      <span class="na">termination/GracePeriodSeconds</span><span class="pi">:</span> <span class="m">30</span>
      <span class="na">volumes</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">containerDisk</span><span class="pi">:</span>
          <span class="na">image</span><span class="pi">:</span> <span class="s">quay.io/kubevirt/fedora-with-test-tooling-container-disk:v1.0.0</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
      <span class="pi">-</span> <span class="na">cloudInitNoCloud</span><span class="pi">:</span>
          <span class="na">networkData</span><span class="pi">:</span> <span class="pi">|</span>
            <span class="s">ethernets:</span>
              <span class="s">eth0:</span>
                <span class="s">addresses:</span>
                <span class="s">- 10.200.0.3/24</span>
            <span class="s">version: 2</span>
          <span class="na">userData</span><span class="pi">:</span> <span class="pi">|-</span>
            <span class="s">#cloud-config</span>
            <span class="s">user: fedora</span>
            <span class="s">password: password</span>
            <span class="s">chpasswd: { expire: False }</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">kubevirt.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachine</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">kubevirt.io/vm</span><span class="pi">:</span> <span class="s">vm4</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">vm4</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">running</span><span class="pi">:</span> <span class="no">true</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">access-control</span>
        <span class="na">kubevirt.io/domain</span><span class="pi">:</span> <span class="s">vm4</span>
        <span class="na">kubevirt.io/vm</span><span class="pi">:</span> <span class="s">vm4</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">domain</span><span class="pi">:</span>
        <span class="na">devices</span><span class="pi">:</span>
          <span class="na">disks</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">disk</span><span class="pi">:</span>
              <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
          <span class="pi">-</span> <span class="na">disk</span><span class="pi">:</span>
              <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
          <span class="na">interfaces</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">bridge</span><span class="pi">:</span> <span class="pi">{}</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">flatl2-overlay</span>
          <span class="na">rng</span><span class="pi">:</span> <span class="pi">{}</span>
        <span class="na">resources</span><span class="pi">:</span>
          <span class="na">requests</span><span class="pi">:</span>
            <span class="na">memory</span><span class="pi">:</span> <span class="s">1024Mi</span>
      <span class="na">networks</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">multus</span><span class="pi">:</span>
          <span class="na">networkName</span><span class="pi">:</span> <span class="s">flatl2net</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">flatl2-overlay</span>
      <span class="na">termination/GracePeriodSeconds</span><span class="pi">:</span> <span class="m">30</span>
      <span class="na">volumes</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">containerDisk</span><span class="pi">:</span>
          <span class="na">image</span><span class="pi">:</span> <span class="s">quay.io/kubevirt/fedora-with-test-tooling-container-disk:v1.0.0</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
      <span class="pi">-</span> <span class="na">cloudInitNoCloud</span><span class="pi">:</span>
          <span class="na">networkData</span><span class="pi">:</span> <span class="pi">|</span>
            <span class="s">ethernets:</span>
              <span class="s">eth0:</span>
                <span class="s">addresses:</span>
                <span class="s">- 10.200.0.4/24</span>
            <span class="s">version: 2</span>
          <span class="na">userData</span><span class="pi">:</span> <span class="pi">|-</span>
            <span class="s">#cloud-config</span>
            <span class="s">user: fedora</span>
            <span class="s">password: password</span>
            <span class="s">chpasswd: { expire: False }</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">kubevirt.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachine</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">kubevirt.io/vm</span><span class="pi">:</span> <span class="s">vm5</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">vm5</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">running</span><span class="pi">:</span> <span class="no">true</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">access-control</span>
        <span class="na">kubevirt.io/domain</span><span class="pi">:</span> <span class="s">vm5</span>
        <span class="na">kubevirt.io/vm</span><span class="pi">:</span> <span class="s">vm5</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">domain</span><span class="pi">:</span>
        <span class="na">devices</span><span class="pi">:</span>
          <span class="na">disks</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">disk</span><span class="pi">:</span>
              <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
          <span class="pi">-</span> <span class="na">disk</span><span class="pi">:</span>
              <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
          <span class="na">interfaces</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">bridge</span><span class="pi">:</span> <span class="pi">{}</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">flatl2-overlay</span>
          <span class="na">rng</span><span class="pi">:</span> <span class="pi">{}</span>
        <span class="na">resources</span><span class="pi">:</span>
          <span class="na">requests</span><span class="pi">:</span>
            <span class="na">memory</span><span class="pi">:</span> <span class="s">1024Mi</span>
      <span class="na">networks</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">multus</span><span class="pi">:</span>
          <span class="na">networkName</span><span class="pi">:</span> <span class="s">flatl2net</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">flatl2-overlay</span>
      <span class="na">termination/GracePeriodSeconds</span><span class="pi">:</span> <span class="m">30</span>
      <span class="na">volumes</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">containerDisk</span><span class="pi">:</span>
          <span class="na">image</span><span class="pi">:</span> <span class="s">quay.io/kubevirt/fedora-with-test-tooling-container-disk:v1.0.0</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
      <span class="pi">-</span> <span class="na">cloudInitNoCloud</span><span class="pi">:</span>
          <span class="na">networkData</span><span class="pi">:</span> <span class="pi">|</span>
            <span class="s">ethernets:</span>
              <span class="s">eth0:</span>
                <span class="s">addresses:</span>
                <span class="s">- 10.200.0.5/24</span>
            <span class="s">version: 2</span>
          <span class="na">userData</span><span class="pi">:</span> <span class="pi">|-</span>
            <span class="s">#cloud-config</span>
            <span class="s">user: fedora</span>
            <span class="s">password: password</span>
            <span class="s">chpasswd: { expire: False }</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">kubevirt.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachine</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">kubevirt.io/vm</span><span class="pi">:</span> <span class="s">vm6</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">vm6</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">running</span><span class="pi">:</span> <span class="no">true</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">access-control</span>
        <span class="na">kubevirt.io/domain</span><span class="pi">:</span> <span class="s">vm6</span>
        <span class="na">kubevirt.io/vm</span><span class="pi">:</span> <span class="s">vm6</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">domain</span><span class="pi">:</span>
        <span class="na">devices</span><span class="pi">:</span>
          <span class="na">disks</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">disk</span><span class="pi">:</span>
              <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
          <span class="pi">-</span> <span class="na">disk</span><span class="pi">:</span>
              <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
          <span class="na">interfaces</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">bridge</span><span class="pi">:</span> <span class="pi">{}</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">flatl2-overlay</span>
          <span class="na">rng</span><span class="pi">:</span> <span class="pi">{}</span>
        <span class="na">resources</span><span class="pi">:</span>
          <span class="na">requests</span><span class="pi">:</span>
            <span class="na">memory</span><span class="pi">:</span> <span class="s">1024Mi</span>
      <span class="na">networks</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">multus</span><span class="pi">:</span>
          <span class="na">networkName</span><span class="pi">:</span> <span class="s">flatl2net</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">flatl2-overlay</span>
      <span class="na">termination/GracePeriodSeconds</span><span class="pi">:</span> <span class="m">30</span>
      <span class="na">volumes</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">containerDisk</span><span class="pi">:</span>
          <span class="na">image</span><span class="pi">:</span> <span class="s">quay.io/kubevirt/fedora-with-test-tooling-container-disk:v1.0.0</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
      <span class="pi">-</span> <span class="na">cloudInitNoCloud</span><span class="pi">:</span>
          <span class="na">networkData</span><span class="pi">:</span> <span class="pi">|</span>
            <span class="s">ethernets:</span>
              <span class="s">eth0:</span>
                <span class="s">addresses:</span>
                <span class="s">- 10.200.0.6/24</span>
            <span class="s">version: 2</span>
          <span class="na">userData</span><span class="pi">:</span> <span class="pi">|-</span>
            <span class="s">#cloud-config</span>
            <span class="s">user: fedora</span>
            <span class="s">password: password</span>
            <span class="s">chpasswd: { expire: False }</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
</code></pre></div></div>
<p><strong>NOTE:</strong> it is important to highlight all the Virtual Machines (and the
<code class="language-plaintext highlighter-rouge">network-attachment-definition</code>) are defined in the <code class="language-plaintext highlighter-rouge">default</code> namespace.</p>

<p>After this step, we should have the following deployment:</p>

<p><img src="/assets/2023-07-10-OVN-kubernetes-secondary-networks-policies/01-vms-provisioned.png" alt="image" /></p>

<p>Let’s check the VMs <code class="language-plaintext highlighter-rouge">vm1</code> and <code class="language-plaintext highlighter-rouge">vm4</code> can ping their peers in the same subnet.
For that we will
<a href="https://kubevirt.io/user-guide/virtual_machines/accessing_virtual_machines/#accessing-the-serial-console">connect to the VMs over their serial console</a>:</p>

<p>First, let’s check <code class="language-plaintext highlighter-rouge">vm1</code>:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>➜  virtctl console vm1
Successfully connected to vm1 console. The escape sequence is ^]

<span class="o">[</span>fedora@vm1 ~]<span class="nv">$ </span>ping 10.200.0.2 <span class="nt">-c</span> 4
PING 10.200.0.2 <span class="o">(</span>10.200.0.2<span class="o">)</span> 56<span class="o">(</span>84<span class="o">)</span> bytes of data.
64 bytes from 10.200.0.2: <span class="nv">icmp_seq</span><span class="o">=</span>1 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>5.16 ms
64 bytes from 10.200.0.2: <span class="nv">icmp_seq</span><span class="o">=</span>2 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>1.41 ms
64 bytes from 10.200.0.2: <span class="nv">icmp_seq</span><span class="o">=</span>3 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>34.2 ms
64 bytes from 10.200.0.2: <span class="nv">icmp_seq</span><span class="o">=</span>4 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>2.56 ms

<span class="nt">---</span> 10.200.0.2 ping statistics <span class="nt">---</span>
4 packets transmitted, 4 received, 0% packet loss, <span class="nb">time </span>3005ms
rtt min/avg/max/mdev <span class="o">=</span> 1.406/10.841/34.239/13.577 ms
<span class="o">[</span>fedora@vm1 ~]<span class="nv">$ </span>ping 10.200.0.6 <span class="nt">-c</span> 4
PING 10.200.0.6 <span class="o">(</span>10.200.0.6<span class="o">)</span> 56<span class="o">(</span>84<span class="o">)</span> bytes of data.
64 bytes from 10.200.0.6: <span class="nv">icmp_seq</span><span class="o">=</span>1 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>3.77 ms
64 bytes from 10.200.0.6: <span class="nv">icmp_seq</span><span class="o">=</span>2 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>1.46 ms
64 bytes from 10.200.0.6: <span class="nv">icmp_seq</span><span class="o">=</span>3 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>5.47 ms
64 bytes from 10.200.0.6: <span class="nv">icmp_seq</span><span class="o">=</span>4 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>1.74 ms

<span class="nt">---</span> 10.200.0.6 ping statistics <span class="nt">---</span>
4 packets transmitted, 4 received, 0% packet loss, <span class="nb">time </span>3007ms
rtt min/avg/max/mdev <span class="o">=</span> 1.459/3.109/5.469/1.627 ms
<span class="o">[</span>fedora@vm1 ~]<span class="nv">$ </span>
</code></pre></div></div>

<p>And from vm4:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>➜  ~ virtctl console vm4
Successfully connected to vm4 console. The escape sequence is ^]

<span class="o">[</span>fedora@vm4 ~]<span class="nv">$ </span>ping 10.200.0.1 <span class="nt">-c</span> 4
PING 10.200.0.1 <span class="o">(</span>10.200.0.1<span class="o">)</span> 56<span class="o">(</span>84<span class="o">)</span> bytes of data.
64 bytes from 10.200.0.1: <span class="nv">icmp_seq</span><span class="o">=</span>1 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>3.20 ms
64 bytes from 10.200.0.1: <span class="nv">icmp_seq</span><span class="o">=</span>2 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>1.62 ms
64 bytes from 10.200.0.1: <span class="nv">icmp_seq</span><span class="o">=</span>3 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>1.44 ms
64 bytes from 10.200.0.1: <span class="nv">icmp_seq</span><span class="o">=</span>4 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>0.951 ms

<span class="nt">---</span> 10.200.0.1 ping statistics <span class="nt">---</span>
4 packets transmitted, 4 received, 0% packet loss, <span class="nb">time </span>3006ms
rtt min/avg/max/mdev <span class="o">=</span> 0.951/1.803/3.201/0.843 ms
<span class="o">[</span>fedora@vm4 ~]<span class="nv">$ </span>ping 10.200.0.6 <span class="nt">-c</span> 4
PING 10.200.0.6 <span class="o">(</span>10.200.0.6<span class="o">)</span> 56<span class="o">(</span>84<span class="o">)</span> bytes of data.
64 bytes from 10.200.0.6: <span class="nv">icmp_seq</span><span class="o">=</span>1 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>1.85 ms
64 bytes from 10.200.0.6: <span class="nv">icmp_seq</span><span class="o">=</span>2 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>1.02 ms
64 bytes from 10.200.0.6: <span class="nv">icmp_seq</span><span class="o">=</span>3 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>1.27 ms
64 bytes from 10.200.0.6: <span class="nv">icmp_seq</span><span class="o">=</span>4 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>0.970 ms

<span class="nt">---</span> 10.200.0.6 ping statistics <span class="nt">---</span>
4 packets transmitted, 4 received, 0% packet loss, <span class="nb">time </span>3005ms
rtt min/avg/max/mdev <span class="o">=</span> 0.970/1.275/1.850/0.350 ms
</code></pre></div></div>

<p>We will now provision a <code class="language-plaintext highlighter-rouge">MultiNetworkPolicy</code> applying to all the VMs defined
above. To do this mapping correcly, the policy has to:</p>
<ul>
  <li>Be in the same namespace as the VM.</li>
  <li>Set <code class="language-plaintext highlighter-rouge">k8s.v1.cni.cncf.io/policy-for</code> annotation matching the secondary 
network used by the VM.</li>
  <li>Set <code class="language-plaintext highlighter-rouge">matchLabels</code> selector matching the labels set on VM’s
<code class="language-plaintext highlighter-rouge">spec.template.metadata</code>.</li>
</ul>

<p>This policy will allow ingress into these <code class="language-plaintext highlighter-rouge">access-control</code> labeled pods 
<strong>only if</strong> the traffic originates from within the <code class="language-plaintext highlighter-rouge">10.200.0.0/30</code> CIDR range
(IPs 10.200.0.1-3).</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">---</span>
apiVersion: k8s.cni.cncf.io/v1beta1
kind: MultiNetworkPolicy
metadata:
  name:  ingress-ipblock
  annotations:
    k8s.v1.cni.cncf.io/policy-for: default/flatl2net
spec:
  podSelector:
    matchLabels:
        name: access-control
  policyTypes:
  - Ingress
  ingress:
  - from:
    - ipBlock:
        cidr: 10.200.0.0/30
</code></pre></div></div>

<p>Taking into account our example, only
<code class="language-plaintext highlighter-rouge">vm1</code>, <code class="language-plaintext highlighter-rouge">vm2</code>, and <code class="language-plaintext highlighter-rouge">vm3</code> will be able to contact any of its peers, as pictured
by the following diagram:</p>

<p><img src="/assets/2023-07-10-OVN-kubernetes-secondary-networks-policies/02-no-access.png" alt="MultiNetworkPolicy is provisioned" /></p>

<p>Let’s try again the ping after provisioning the <code class="language-plaintext highlighter-rouge">MultiNetworkPolicy</code> object:</p>

<p>From <code class="language-plaintext highlighter-rouge">vm1</code> (inside the allowed ip block range):</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>fedora@vm1 ~]<span class="nv">$ </span>ping 10.200.0.2 <span class="nt">-c</span> 4
PING 10.200.0.2 <span class="o">(</span>10.200.0.2<span class="o">)</span> 56<span class="o">(</span>84<span class="o">)</span> bytes of data.
64 bytes from 10.200.0.2: <span class="nv">icmp_seq</span><span class="o">=</span>1 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>6.48 ms
64 bytes from 10.200.0.2: <span class="nv">icmp_seq</span><span class="o">=</span>2 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>4.40 ms
64 bytes from 10.200.0.2: <span class="nv">icmp_seq</span><span class="o">=</span>3 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>1.28 ms
64 bytes from 10.200.0.2: <span class="nv">icmp_seq</span><span class="o">=</span>4 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>1.51 ms

<span class="nt">---</span> 10.200.0.2 ping statistics <span class="nt">---</span>
4 packets transmitted, 4 received, 0% packet loss, <span class="nb">time </span>3006ms
rtt min/avg/max/mdev <span class="o">=</span> 1.283/3.418/6.483/2.154 ms
<span class="o">[</span>fedora@vm1 ~]<span class="nv">$ </span>ping 10.200.0.6 <span class="nt">-c</span> 4
PING 10.200.0.6 <span class="o">(</span>10.200.0.6<span class="o">)</span> 56<span class="o">(</span>84<span class="o">)</span> bytes of data.
64 bytes from 10.200.0.6: <span class="nv">icmp_seq</span><span class="o">=</span>1 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>3.81 ms
64 bytes from 10.200.0.6: <span class="nv">icmp_seq</span><span class="o">=</span>2 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>2.67 ms
64 bytes from 10.200.0.6: <span class="nv">icmp_seq</span><span class="o">=</span>3 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>1.68 ms
64 bytes from 10.200.0.6: <span class="nv">icmp_seq</span><span class="o">=</span>4 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>1.63 ms

<span class="nt">---</span> 10.200.0.6 ping statistics <span class="nt">---</span>
4 packets transmitted, 4 received, 0% packet loss, <span class="nb">time </span>3006ms
rtt min/avg/max/mdev <span class="o">=</span> 1.630/2.446/3.808/0.888 ms
</code></pre></div></div>

<p>From <code class="language-plaintext highlighter-rouge">vm4</code> (<strong>outside</strong> the allowed ip block range):</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>fedora@vm4 ~]<span class="nv">$ </span>ping 10.200.0.1 <span class="nt">-c</span> 4
PING 10.200.0.1 <span class="o">(</span>10.200.0.1<span class="o">)</span> 56<span class="o">(</span>84<span class="o">)</span> bytes of data.

<span class="nt">---</span> 10.200.0.1 ping statistics <span class="nt">---</span>
4 packets transmitted, 0 received, 100% packet loss, <span class="nb">time </span>3083ms

<span class="o">[</span>fedora@vm4 ~]<span class="nv">$ </span>ping 10.200.0.6 <span class="nt">-c</span> 4
PING 10.200.0.6 <span class="o">(</span>10.200.0.6<span class="o">)</span> 56<span class="o">(</span>84<span class="o">)</span> bytes of data.

<span class="nt">---</span> 10.200.0.6 ping statistics <span class="nt">---</span>
4 packets transmitted, 0 received, 100% packet loss, <span class="nb">time </span>3089ms
</code></pre></div></div>

<h2 id="conclusions">Conclusions</h2>
<p>In this post we’ve shown how <code class="language-plaintext highlighter-rouge">MultiNetworkPolicies</code> can be used to provide
access control to VMs with secondary network interfaces.</p>

<p>We have provided a comprehensive example on how a policy can be used to limit
ingress to our VMs only from desired sources, based on the client’s IP address.</p>]]></content><author><name>Miguel Duarte Barroso</name></author><category term="news" /><category term="Kubevirt" /><category term="kubernetes" /><category term="virtual machine" /><category term="VM" /><category term="SDN" /><category term="OVN" /><category term="NetworkPolicy" /><summary type="html"><![CDATA[This post explains how to configure NetworkPolicies for KubeVirt VMs secondary networks.]]></summary></entry><entry><title type="html">KubeVirt v1.0 has landed!</title><link href="https://kubevirt.io//2023/KubeVirt-v1-has-landed.html" rel="alternate" type="text/html" title="KubeVirt v1.0 has landed!" /><published>2023-07-11T00:00:00+00:00</published><updated>2023-07-11T00:00:00+00:00</updated><id>https://kubevirt.io//2023/KubeVirt-v1-has-landed</id><content type="html" xml:base="https://kubevirt.io//2023/KubeVirt-v1-has-landed.html"><![CDATA[<p>The KubeVirt community is proud to announce the release of <a href="https://github.com/kubevirt/kubevirt/releases/tag/v1.0.0">KubeVirt v1.0</a>! This release demonstrates the accomplishments of the community and user adoption over the years and represents an important milestone for everyone involved.</p>

<h2 id="a-brief-history">A brief history</h2>
<p>The KubeVirt project started in Red Hat at the end of 2016, with the question: Can virtual machines (VMs) run in containers and be deployed by Kubernetes?
It proved to be not only possible, but quickly emerged as a promising solution to the future of virtual machines in the container age.
KubeVirt joined the <a href="https://www.cncf.io/">CNCF</a> as a Sandbox project in September 2019, and an Incubating project in April 2022.
From a handful of people hacking away on a proof of concept, KubeVirt has grown into 45 active repositories, with the primary <a href="https://github.com/kubevirt/kubevirt">kubevirt/kubevirt</a> repo having 17k commits and 1k forks.</p>

<h2 id="what-does-v10-mean-to-the-community">What does v1.0 mean to the community?</h2>
<p>The v1.0 release signifies the incredible growth that the community has gone through in the past six years from an idea to a production-ready Virtual Machine Management solution. The next stage with v1.0 is the additional focus on maintaining APIs while continuing to grow the project. This has led KubeVirt to adopt community practices from Kubernetes in key parts of the project.</p>

<p>Leading up to this release we had a shift in release cadence: from monthly to 3 times a year, following the Kubernetes release model. This allows our developer community additional time to ensure stability and compatibility, our users more time to plan and comfortably upgrade, and also aligns our releases with Kubernetes to simplify maintenance and supportability.</p>

<p>The theme ‘aligning with Kubernetes’ is also felt through the other parts of the community, by following their governance processes; introducing SIGs to split test and review responsibilities, as well as a SIG release repo to handle everything related to a release; and regular <a href="https://calendar.google.com/calendar/u/0/embed?src=kubevirt@cncf.io">SIG meetings</a> that now include SIG scale and performance and SIG storage alongside our weekly Community meetings.</p>

<h2 id="whats-included-in-this-release">What’s included in this release?</h2>

<p>This release demonstrates the accomplishments of the community and user adoption over the past many months. The full list of feature and bug fixes can be found in our <a href="https://github.com/kubevirt/kubevirt/releases/tag/v1.0.0">release notes</a>, but we’ve also asked representatives from some of our SIGs for a summary.</p>

<h3 id="sig-scale">SIG-scale</h3>
<p>KubeVirt’s SIG-scale drives the performance and scalability initiatives in the community. Our focus for the v1.0 release was on sharing the performance results over the past 6 months. The benchmarks since December 2022 which cover the past two release - v0.59 (Mar 2023) and v1.0 (July 2023) are as follows:</p>

<p><a href="https://github.com/kubevirt/kubevirt/blob/release-1.0/docs/release-v1-perf-scale-benchmarks.md#performance-benchmarks-for-v1-release">Performance benchmarks for v1.0 release</a></p>

<p><a href="https://github.com/kubevirt/kubevirt/blob/release-1.0/docs/release-v1-perf-scale-benchmarks.md#scalability-benchmarks-for-v1-release">Scalability benchmarks for v1.0 release</a></p>

<p>Publishing these measurements provides the community and end-users visibility into the performance and scalability over multiple releases. In addition, these results help identify the effects of code changes so that community members can diagnose performance problems and regressions.</p>

<p>End-users can use the same tools and techniques SIG-scale uses to analyze performance and scalability in their own deployments. Since performance and scalability are mostly relative to the deployment stack, the same strategies should be used to further contextualize the community’s measurements.</p>

<h3 id="sig-storage">SIG-storage</h3>
<p>SIG-storage is focused on providing persistent storage to KubeVirt VMs and managing that storage throughout the lifecycle of the VM. This begins with provisioning and populating PVCs with bootable images but also includes features such as disk hotplug, snapshots, backup and restore, disaster recovery, and virtual machine export.</p>

<p>For v1.0, SIG-storage delivered the following features: providing a flexible VM export API, enabling persistent SCSI reservation, provisioning VMs from a retained snapshot, and setting out-of-the-box defaults for additional storage provisioners. Another major effort was to implement Volume Populator alternatives to the KubeVirt DataVolume API in order to better leverage platform capabilities. The SIG meets every 2 weeks and welcomes anyone to join us for interesting storage discussions.</p>

<h3 id="sig-compute">SIG-compute</h3>
<p>SIG-compute is focused on the core virtualization functionality of KubeVirt, but also encompasses features that don’t fit well into another SIG. Some examples of SIG-compute’s scope include the lifecycle of VMs, migration, as well as maintenance of the core API.</p>

<p>For v1.0, SIG-compute developed features for memory over-commit. This includes initial support for KSM and FreePageReporting. We added support for persistent vTPM, which makes it much easier to use BitLocker on Windows installs. Additionally, there’s now an initial implementation for CPU Hotplug (currently hidden behind a feature gate).</p>

<h3 id="sig-network">SIG-network</h3>
<p>SIG-network is committed to enhancing and maintaining all aspects of Virtual Machine network connectivity and management in KubeVirt.</p>

<p>For the v1.0 release, we have introduced hot plug and hot unplug (as alpha), which enables users to add and remove VM secondary network interfaces that use bridge binding on a running VM. Hot plug API stabilization and support for SR-IOV interfaces is under development for the next minor release.</p>

<h3 id="sig-infra">SIG-infra</h3>
<p>The effort to simplify the VirtualMachine UX is still ongoing and with the v1.0 release we were able to introduce the v1beta1 version of the instancetype.kubevirt.io API. In the future KubeVirt v1.1.0 release we are aiming to finally graduate the instancetype.kubevirt.io API to v1.</p>

<p>With the new version it is now possible to control the memory overcommit of virtual machines as a percentage within instance types. Resource requirements were added to preferences, which allows users to ensure that requirements of a workload are met. Also several new preference attributes have been added to cover more use cases.</p>

<p>Moreover, virtctl was extended to make use of the new instance type and preference features.</p>

<h2 id="what-next-for-kubevirt">What next for KubeVirt?</h2>
<p>From a development perspective, we will continue to introduce and improve features that make life easier for virtualization users in a manner that is as native to Kubernetes as possible. From a community perspective, we are improving our new contributor experience so that we can continue to grow and help new members learn and be a part of the cloud native ecosystem. In addition, with this milestone we can now shift our attention on becoming a CNCF Graduated project.</p>]]></content><author><name>KubeVirt Maintainers</name></author><category term="news" /><category term="KubeVirt" /><category term="v1.0" /><category term="release" /><category term="community" /><category term="cncf" /><category term="milestone" /><category term="party time" /><summary type="html"><![CDATA[We are very pleased to announce the release of KubeVirt v1.0!]]></summary></entry><entry><title type="html">Secondary networks connected to the physical underlay for KubeVirt VMs using OVN-Kubernetes</title><link href="https://kubevirt.io//2023/OVN-kubernetes-secondary-networks-localnet.html" rel="alternate" type="text/html" title="Secondary networks connected to the physical underlay for KubeVirt VMs using OVN-Kubernetes" /><published>2023-05-31T00:00:00+00:00</published><updated>2023-05-31T00:00:00+00:00</updated><id>https://kubevirt.io//2023/OVN-kubernetes-secondary-networks-localnet</id><content type="html" xml:base="https://kubevirt.io//2023/OVN-kubernetes-secondary-networks-localnet.html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>OVN (Open Virtual Network) is a series of daemons for the Open vSwitch that
translate virtual network configurations into OpenFlow. It provides virtual
networking capabilities for any type of workload on a virtualized platform
(virtual machines and containers) using the same API.</p>

<p>OVN provides a higher-layer of abstraction than Open vSwitch, working with
logical routers and logical switches, rather than flows.
More details can be found in the OVN architecture
<a href="https://man7.org/linux/man-pages/man7/ovn-architecture.7.html#DESCRIPTION">man page</a>.</p>

<p>In this post we will repeat the scenario of
<a href="https://kubevirt.io/2020/Multiple-Network-Attachments-with-bridge-CNI.html">its bridge CNI equivalent</a>,
using this SDN approach. This secondary network topology is akin to the one
described in the <a href="http://kubevirt.io/2023/OVN-kubernetes-secondary-networks.html">flatL2 topology</a>,
but allows connectivity to the physical underlay.</p>

<h2 id="demo">Demo</h2>
<p>To run this demo, we will prepare a Kubernetes cluster with the following
components installed:</p>
<ul>
  <li><a href="https://github.com/ovn-org/ovn-kubernetes">OVN-Kubernetes</a></li>
  <li><a href="https://github.com/k8snetworkplumbingwg/multus-cni">multus-cni</a></li>
  <li><a href="https://github.com/kubevirt/kubevirt">KubeVirt</a></li>
</ul>

<p>The <a href="#environment-setup">following section</a> will show you how to create a
<a href="https://kind.sigs.k8s.io/">KinD</a> cluster, with upstream latest OVN-Kubernetes,
and upstream latest multus-cni deployed.</p>

<h3 id="setup-demo-environment">Setup demo environment</h3>
<p>Refer to the OVN-Kubernetes repo
<a href="https://github.com/ovn-org/ovn-kubernetes/blob/master/docs/kind.md#ovn-kubernetes-kind-setup">KIND documentation</a>
for more details; the gist of it is you should clone the OVN-Kubernetes
repository, and run their kind helper script:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone git@github.com:ovn-org/ovn-kubernetes.git

<span class="nb">cd </span>ovn-kubernetes
<span class="nb">pushd </span>contrib <span class="p">;</span> ./kind.sh <span class="nt">--multi-network-enable</span> <span class="p">;</span> <span class="nb">popd</span>
</code></pre></div></div>

<p>This will get you a running kind cluster, configured to use OVN-Kubernetes as
the default cluster network, configuring the multi-homing OVN-Kubernetes feature
gate, and deploying
<a href="https://github.com/k8snetworkplumbingwg/multus-cni">multus-cni</a> in the cluster.</p>

<h3 id="install-kubevirt-in-the-cluster">Install KubeVirt in the cluster</h3>
<p>Follow Kubevirt’s
<a href="https://kubevirt.io/user-guide/operations/installation/#installing-kubevirt-on-kubernetes">user guide</a>
to install the latest released version (currently, v0.59.0).</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">export </span><span class="nv">RELEASE</span><span class="o">=</span><span class="si">$(</span>curl https://storage.googleapis.com/kubevirt-prow/release/kubevirt/kubevirt/stable.txt<span class="si">)</span>
kubectl apply <span class="nt">-f</span> <span class="s2">"https://github.com/kubevirt/kubevirt/releases/download/</span><span class="k">${</span><span class="nv">RELEASE</span><span class="k">}</span><span class="s2">/kubevirt-operator.yaml"</span>
kubectl apply <span class="nt">-f</span> <span class="s2">"https://github.com/kubevirt/kubevirt/releases/download/</span><span class="k">${</span><span class="nv">RELEASE</span><span class="k">}</span><span class="s2">/kubevirt-cr.yaml"</span>
kubectl <span class="nt">-n</span> kubevirt <span class="nb">wait </span>kv kubevirt <span class="nt">--timeout</span><span class="o">=</span>360s <span class="nt">--for</span> <span class="nv">condition</span><span class="o">=</span>Available
</code></pre></div></div>

<p>Now we have a Kubernetes cluster with all the pieces to start the Demo.</p>

<h3 id="single-broadcast-domain">Single broadcast domain</h3>
<p>In this scenario we will see how traffic from a single localnet network can be
connected to a physical network in the host using a dedicated bridge.</p>

<p>This scenario does not use any VLAN encapsulation, thus is simpler, since the
network admin does not need to provision any VLANs in advance.</p>

<h4 id="configuring-the-underlay">Configuring the underlay</h4>
<p>When you’ve started the KinD cluster with the <code class="language-plaintext highlighter-rouge">--multi-network-enable</code> flag an
additional OCI network was created, and attached to each of the KinD nodes.</p>

<p>But still, further steps may be required, depending on the desired L2
configuration.</p>

<p>Let’s first create a dedicated OVS bridge, and attach the aforementioned
virtualized network to it:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for </span>node <span class="k">in</span> <span class="si">$(</span>kubectl <span class="nt">-n</span> ovn-kubernetes get pods <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>ovs-node <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.items[*].metadata.name}"</span><span class="si">)</span>
<span class="k">do
	</span>kubectl <span class="nt">-n</span> ovn-kubernetes <span class="nb">exec</span> <span class="nt">-ti</span> <span class="nv">$node</span> <span class="nt">--</span> ovs-vsctl <span class="nt">--may-exist</span> add-br ovsbr1
	kubectl <span class="nt">-n</span> ovn-kubernetes <span class="nb">exec</span> <span class="nt">-ti</span> <span class="nv">$node</span> <span class="nt">--</span> ovs-vsctl <span class="nt">--may-exist</span> add-port ovsbr1 eth1
	kubectl <span class="nt">-n</span> ovn-kubernetes <span class="nb">exec</span> <span class="nt">-ti</span> <span class="nv">$node</span> <span class="nt">--</span> ovs-vsctl <span class="nb">set </span>open <span class="nb">.</span> external_ids:ovn-bridge-mappings<span class="o">=</span>physnet:breth0,localnet-network:ovsbr1
<span class="k">done</span>
</code></pre></div></div>

<p>The first two commands are self-evident: you create an OVS bridge, and attach
a port to it; the last one is not. In it, we’re using the
<a href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.0/html/networking_guide/bridge-mappings">OVN bridge mapping</a>
API to configure which OVS bridge must be used for each physical network.
It creates a patch port between the OVN integration bridge - <code class="language-plaintext highlighter-rouge">br-int</code> - and the
OVS bridge you tell it to, and traffic will be forwarded to/from it with the
help of a
<a href="https://man7.org/linux/man-pages/man5/ovn-nb.5.html#Logical_Switch_Port_TABLE">localnet port</a>.</p>

<p><strong>NOTE:</strong> The provided mapping <strong>must</strong> match the <code class="language-plaintext highlighter-rouge">name</code> within the
<code class="language-plaintext highlighter-rouge">net-attach-def</code>.Spec.Config JSON, otherwise, the patch ports will not be
created.</p>

<p>You will also have to configure an IP address on the bridge for the
extra-network the kind script created. For that, you first need to identify the
bridge’s name. In the example below we’re providing a command for the podman
runtime:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>podman network inspect underlay <span class="nt">--format</span> <span class="s1">''</span>
podman3

ip addr add 10.128.0.1/24 dev podman3
</code></pre></div></div>

<p>Let’s also use an IP in the same subnet as the network subnet (defined in the
NAD). This IP address must be excluded from the IPAM pool (also on the NAD),
otherwise the OVN-Kubernetes IPAM may assign it to a workload.</p>

<h4 id="defining-the-ovn-kubernetes-networks">Defining the OVN-Kubernetes networks</h4>
<p>Once the underlay is configured, we can now provision the attachment configuration:</p>
<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">k8s.cni.cncf.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">NetworkAttachmentDefinition</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">localnet-network</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">config</span><span class="pi">:</span> <span class="pi">|2</span>
    <span class="s">{</span>
            <span class="s">"cniVersion": "0.3.1",</span>
            <span class="s">"name": "localnet-network",</span>
            <span class="s">"type": "ovn-k8s-cni-overlay",</span>
            <span class="s">"topology": "localnet",</span>
            <span class="s">"subnets": "10.128.0.0/24",</span>
            <span class="s">"excludeSubnets": "10.128.0.1/32",</span>
            <span class="s">"netAttachDefName": "default/localnet-network"</span>
    <span class="s">}</span>
</code></pre></div></div>

<p>It is required to list the gateway IP in the <code class="language-plaintext highlighter-rouge">excludedSubnets</code> attribute, thus
preventing OVN-Kubernetes from assigning that IP address to the workloads.</p>

<h4 id="spin-up-the-vms">Spin up the VMs</h4>
<p>These four VMs (two VMs connected to each tenant network) can be used for the
single broadcast domain scenario (no VLANs).</p>
<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">kubevirt.io/v1alpha3</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachine</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">vm-server</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">running</span><span class="pi">:</span> <span class="no">true</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">nodeSelector</span><span class="pi">:</span>
        <span class="na">kubernetes.io/hostname</span><span class="pi">:</span> <span class="s">ovn-worker</span>
      <span class="na">domain</span><span class="pi">:</span>
        <span class="na">devices</span><span class="pi">:</span>
          <span class="na">disks</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
              <span class="na">disk</span><span class="pi">:</span>
                <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
              <span class="na">disk</span><span class="pi">:</span>
                <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
          <span class="na">interfaces</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">localnet</span>
            <span class="na">bridge</span><span class="pi">:</span> <span class="pi">{}</span>
        <span class="na">machine</span><span class="pi">:</span>
          <span class="na">type</span><span class="pi">:</span> <span class="s2">"</span><span class="s">"</span>
        <span class="na">resources</span><span class="pi">:</span>
          <span class="na">requests</span><span class="pi">:</span>
            <span class="na">memory</span><span class="pi">:</span> <span class="s">1024M</span>
      <span class="na">networks</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">localnet</span>
        <span class="na">multus</span><span class="pi">:</span>
          <span class="na">networkName</span><span class="pi">:</span> <span class="s">localnet-network</span>
      <span class="na">terminationGracePeriodSeconds</span><span class="pi">:</span> <span class="m">0</span>
      <span class="na">volumes</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
          <span class="na">containerDisk</span><span class="pi">:</span>
            <span class="na">image</span><span class="pi">:</span> <span class="s">quay.io/kubevirt/fedora-with-test-tooling-container-disk:devel</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
          <span class="na">cloudInitNoCloud</span><span class="pi">:</span>
            <span class="na">networkData</span><span class="pi">:</span> <span class="pi">|</span>
              <span class="s">version: 2</span>
              <span class="s">ethernets:</span>
                <span class="s">eth0:</span>
                  <span class="s">dhcp4: true</span>
            <span class="na">userData</span><span class="pi">:</span> <span class="pi">|-</span>
              <span class="s">#cloud-config</span>
              <span class="s">password: fedora</span>
              <span class="s">chpasswd: { expire: False }</span>
<span class="s">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">kubevirt.io/v1alpha3</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachine</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">vm-client</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">running</span><span class="pi">:</span> <span class="no">true</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">nodeSelector</span><span class="pi">:</span>
        <span class="na">kubernetes.io/hostname</span><span class="pi">:</span> <span class="s">ovn-worker2</span>
      <span class="na">domain</span><span class="pi">:</span>
        <span class="na">devices</span><span class="pi">:</span>
          <span class="na">disks</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
              <span class="na">disk</span><span class="pi">:</span>
                <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
              <span class="na">disk</span><span class="pi">:</span>
                <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
          <span class="na">interfaces</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">localnet</span>
            <span class="na">bridge</span><span class="pi">:</span> <span class="pi">{}</span>
        <span class="na">machine</span><span class="pi">:</span>
          <span class="na">type</span><span class="pi">:</span> <span class="s2">"</span><span class="s">"</span>
        <span class="na">resources</span><span class="pi">:</span>
          <span class="na">requests</span><span class="pi">:</span>
            <span class="na">memory</span><span class="pi">:</span> <span class="s">1024M</span>
      <span class="na">networks</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">localnet</span>
        <span class="na">multus</span><span class="pi">:</span>
          <span class="na">networkName</span><span class="pi">:</span> <span class="s">localnet-network</span>
      <span class="na">terminationGracePeriodSeconds</span><span class="pi">:</span> <span class="m">0</span>
      <span class="na">volumes</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
          <span class="na">containerDisk</span><span class="pi">:</span>
            <span class="na">image</span><span class="pi">:</span> <span class="s">quay.io/kubevirt/fedora-with-test-tooling-container-disk:devel</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
          <span class="na">cloudInitNoCloud</span><span class="pi">:</span>
            <span class="na">networkData</span><span class="pi">:</span> <span class="pi">|</span>
              <span class="s">version: 2</span>
              <span class="s">ethernets:</span>
                <span class="s">eth0:</span>
                  <span class="s">dhcp4: true</span>
            <span class="na">userData</span><span class="pi">:</span> <span class="pi">|-</span>
              <span class="s">#cloud-config</span>
              <span class="s">password: fedora</span>
              <span class="s">chpasswd: { expire: False }</span>
</code></pre></div></div>

<h4 id="test-east--west-communication">Test East / West communication</h4>
<p>You can check east/west connectivity between both <strong>red</strong> VMs via ICMP:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl get vmi vm-server <span class="nt">-ojsonpath</span><span class="o">=</span><span class="s2">"{ @.status.interfaces }"</span> | jq
<span class="o">[</span>
  <span class="o">{</span>
    <span class="s2">"infoSource"</span>: <span class="s2">"domain, guest-agent"</span>,
    <span class="s2">"interfaceName"</span>: <span class="s2">"eth0"</span>,
    <span class="s2">"ipAddress"</span>: <span class="s2">"10.128.0.2"</span>,
    <span class="s2">"ipAddresses"</span>: <span class="o">[</span>
      <span class="s2">"10.128.0.2"</span>,
      <span class="s2">"fe80::e83d:16ff:fe76:c1bd"</span>
    <span class="o">]</span>,
    <span class="s2">"mac"</span>: <span class="s2">"ea:3d:16:76:c1:bd"</span>,
    <span class="s2">"name"</span>: <span class="s2">"localnet"</span>,
    <span class="s2">"queueCount"</span>: 1
  <span class="o">}</span>
<span class="o">]</span>

<span class="nv">$ </span>virtctl console vm-client
Successfully connected to vm-client console. The escape sequence is ^]

<span class="o">[</span>fedora@vm-client ~]<span class="nv">$ </span>ping 192.168.123.20
PING 192.168.123.20 <span class="o">(</span>192.168.123.20<span class="o">)</span> 56<span class="o">(</span>84<span class="o">)</span> bytes of data.
64 bytes from 192.168.123.20: <span class="nv">icmp_seq</span><span class="o">=</span>1 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>0.534 ms
64 bytes from 192.168.123.20: <span class="nv">icmp_seq</span><span class="o">=</span>2 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>0.246 ms
64 bytes from 192.168.123.20: <span class="nv">icmp_seq</span><span class="o">=</span>3 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>0.178 ms
64 bytes from 192.168.123.20: <span class="nv">icmp_seq</span><span class="o">=</span>4 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>0.236 ms

<span class="nt">---</span> 192.168.123.20 ping statistics <span class="nt">---</span>
4 packets transmitted, 4 received, 0% packet loss, <span class="nb">time </span>3028ms
rtt min/avg/max/mdev <span class="o">=</span> 0.178/0.298/0.534/0.138 ms
</code></pre></div></div>

<h4 id="check-underlay-services">Check underlay services</h4>
<p>We can now start HTTP servers listening to the IPs attached on
the gateway:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python3 <span class="nt">-m</span> http.server <span class="nt">--bind</span> 10.128.0.1 9000
</code></pre></div></div>

<p>And finally curl this from your client:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>fedora@vm-client ~]<span class="nv">$ </span>curl <span class="nt">-v</span> 10.128.0.1:9000
<span class="k">*</span>   Trying 10.128.0.1:9000...
<span class="k">*</span> Connected to 10.128.0.1 <span class="o">(</span>10.128.0.1<span class="o">)</span> port 9000 <span class="o">(</span><span class="c">#0)</span>
<span class="o">&gt;</span> GET / HTTP/1.1
<span class="o">&gt;</span> Host: 10.128.0.1:9000
<span class="o">&gt;</span> User-Agent: curl/7.69.1
<span class="o">&gt;</span> Accept: <span class="k">*</span>/<span class="k">*</span>
<span class="o">&gt;</span> 
<span class="k">*</span> Mark bundle as not supporting multiuse
<span class="k">*</span> HTTP 1.0, assume close after body
&lt; HTTP/1.0 200 OK
&lt; Server: SimpleHTTP/0.6 Python/3.11.3
&lt; Date: Thu, 01 Jun 2023 16:05:09 GMT
&lt; Content-type: text/html<span class="p">;</span> <span class="nv">charset</span><span class="o">=</span>utf-8
&lt; Content-Length: 2923
...
</code></pre></div></div>

<h3 id="multiple-physical-networks-pointing-to-the-same-ovs-bridge">Multiple physical networks pointing to the same OVS bridge</h3>
<p>This example will feature 2 physical networks, each with a different VLAN,
both pointing at the same OVS bridge.</p>

<h4 id="configuring-the-underlay-1">Configuring the underlay</h4>
<p>Again, the first thing to do is create a dedicated OVS bridge, and attach the
aforementioned virtualized network to it, while defining it as a trunk port
for two broadcast domains, with tags 10 and 20.</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for </span>node <span class="k">in</span> <span class="si">$(</span>kubectl <span class="nt">-n</span> ovn-kubernetes get pods <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>ovs-node <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.items[*].metadata.name}"</span><span class="si">)</span>
<span class="k">do
	</span>kubectl <span class="nt">-n</span> ovn-kubernetes <span class="nb">exec</span> <span class="nt">-ti</span> <span class="nv">$node</span> <span class="nt">--</span> ovs-vsctl <span class="nt">--may-exist</span> add-br ovsbr1
	kubectl <span class="nt">-n</span> ovn-kubernetes <span class="nb">exec</span> <span class="nt">-ti</span> <span class="nv">$node</span> <span class="nt">--</span> ovs-vsctl <span class="nt">--may-exist</span> add-port ovsbr1 eth1 <span class="nv">trunks</span><span class="o">=</span>10,20 <span class="nv">vlan_mode</span><span class="o">=</span>trunk
	kubectl <span class="nt">-n</span> ovn-kubernetes <span class="nb">exec</span> <span class="nt">-ti</span> <span class="nv">$node</span> <span class="nt">--</span> ovs-vsctl <span class="nb">set </span>open <span class="nb">.</span> external_ids:ovn-bridge-mappings<span class="o">=</span>physnet:breth0,tenantblue:ovsbr1,tenantred:ovsbr1
<span class="k">done</span>
</code></pre></div></div>

<p>We must now configure the physical network; since the packets are leaving the
OVS bridge tagged with either the 10 or 20 VLAN, we must configure the physical
network where the virtualized nodes run to handle the tagged traffic.</p>

<p>For that we will create two VLANed interfaces, each with a different subnet; we
will need to know the name of the bridge the kind script created to implement
the extra network it required. Those VLAN interfaces also need to be configured
with an IP address:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">OCI_BIN</span><span class="o">=</span>podman | docker <span class="c"># choose your cup of tea.</span>
<span class="nv">$OCI_BIN</span> network inspect underlay <span class="nt">--format</span> <span class="s1">'{ .NetworkInterface }}'</span>
podman3

<span class="c"># create the VLANs</span>
ip <span class="nb">link </span>add <span class="nb">link </span>podman3 name podman3.10 <span class="nb">type </span>vlan <span class="nb">id </span>10
ip addr add 192.168.123.1/24 dev podman3.10
ip <span class="nb">link set </span>dev podman3.10 up

ip <span class="nb">link </span>add <span class="nb">link </span>podman3 name podman3.20 <span class="nb">type </span>vlan <span class="nb">id </span>20
ip addr add 192.168.124.1/24 dev podman3.20
ip <span class="nb">link set </span>dev podman3.20 up
</code></pre></div></div>

<p><strong>NOTE:</strong> both the <code class="language-plaintext highlighter-rouge">tenantblue</code> and <code class="language-plaintext highlighter-rouge">tenantred</code> networks forward their traffic
to the <code class="language-plaintext highlighter-rouge">ovsbr1</code> OVS bridge.</p>

<h4 id="defining-the-ovn-kubernetes-networks-1">Defining the OVN-Kubernetes networks</h4>
<p>Let us now provision the attachment configuration for the two physical networks.
Notice they do not have a subnet defined, which means our workloads must
configure static IPs via cloud-init.</p>
<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">k8s.cni.cncf.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">NetworkAttachmentDefinition</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">tenantred</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">config</span><span class="pi">:</span> <span class="pi">|2</span>
    <span class="s">{</span>
            <span class="s">"cniVersion": "0.3.1",</span>
            <span class="s">"name": "tenantred",</span>
            <span class="s">"type": "ovn-k8s-cni-overlay",</span>
            <span class="s">"topology": "localnet",</span>
            <span class="s">"vlanID": 10,</span>
            <span class="s">"netAttachDefName": "default/tenantred"</span>
    <span class="s">}</span>
<span class="s">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">k8s.cni.cncf.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">NetworkAttachmentDefinition</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">tenantblue</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">config</span><span class="pi">:</span> <span class="pi">|2</span>
    <span class="s">{</span>
            <span class="s">"cniVersion": "0.3.1",</span>
            <span class="s">"name": "tenantblue",</span>
            <span class="s">"type": "ovn-k8s-cni-overlay",</span>
            <span class="s">"topology": "localnet",</span>
            <span class="s">"vlanID": 20,</span>
            <span class="s">"netAttachDefName": "default/tenantblue"</span>
    <span class="s">}</span>
</code></pre></div></div>

<p><strong>NOTE:</strong> each of the <code class="language-plaintext highlighter-rouge">tenantblue</code> and <code class="language-plaintext highlighter-rouge">tenantred</code> networks tags their traffic
with a different VLAN, which must be listed on the port <code class="language-plaintext highlighter-rouge">trunks</code> configuration.</p>

<h4 id="spin-up-the-vms-1">Spin up the VMs</h4>
<p>These two VMs can be used for the OVS bridge sharing scenario (two physical
networks share the same OVS bridge, each on a different VLAN).</p>
<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">kubevirt.io/v1alpha3</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachine</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">vm-red-1</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">running</span><span class="pi">:</span> <span class="no">true</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">nodeSelector</span><span class="pi">:</span>
        <span class="na">kubernetes.io/hostname</span><span class="pi">:</span> <span class="s">ovn-worker</span>
      <span class="na">domain</span><span class="pi">:</span>
        <span class="na">devices</span><span class="pi">:</span>
          <span class="na">disks</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
              <span class="na">disk</span><span class="pi">:</span>
                <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
              <span class="na">disk</span><span class="pi">:</span>
                <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
          <span class="na">interfaces</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">physnet-red</span>
            <span class="na">bridge</span><span class="pi">:</span> <span class="pi">{}</span>
        <span class="na">machine</span><span class="pi">:</span>
          <span class="na">type</span><span class="pi">:</span> <span class="s2">"</span><span class="s">"</span>
        <span class="na">resources</span><span class="pi">:</span>
          <span class="na">requests</span><span class="pi">:</span>
            <span class="na">memory</span><span class="pi">:</span> <span class="s">1024M</span>
      <span class="na">networks</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">physnet-red</span>
        <span class="na">multus</span><span class="pi">:</span>
          <span class="na">networkName</span><span class="pi">:</span> <span class="s">tenantred</span>
      <span class="na">terminationGracePeriodSeconds</span><span class="pi">:</span> <span class="m">0</span>
      <span class="na">volumes</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
          <span class="na">containerDisk</span><span class="pi">:</span>
            <span class="na">image</span><span class="pi">:</span> <span class="s">quay.io/kubevirt/fedora-with-test-tooling-container-disk:devel</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
          <span class="na">cloudInitNoCloud</span><span class="pi">:</span>
            <span class="na">networkData</span><span class="pi">:</span> <span class="pi">|</span>
              <span class="s">version: 2</span>
              <span class="s">ethernets:</span>
                <span class="s">eth0:</span>
                  <span class="s">addresses: [ 192.168.123.10/24 ]</span>
            <span class="na">userData</span><span class="pi">:</span> <span class="pi">|-</span>
              <span class="s">#cloud-config</span>
              <span class="s">password: fedora</span>
              <span class="s">chpasswd: { expire: False }</span>
<span class="s">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">kubevirt.io/v1alpha3</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachine</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">vm-red-2</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">running</span><span class="pi">:</span> <span class="no">true</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">nodeSelector</span><span class="pi">:</span>
        <span class="na">kubernetes.io/hostname</span><span class="pi">:</span> <span class="s">ovn-worker</span>
      <span class="na">domain</span><span class="pi">:</span>
        <span class="na">devices</span><span class="pi">:</span>
          <span class="na">disks</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
              <span class="na">disk</span><span class="pi">:</span>
                <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
              <span class="na">disk</span><span class="pi">:</span>
                <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
          <span class="na">interfaces</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">flatl2-overlay</span>
            <span class="na">bridge</span><span class="pi">:</span> <span class="pi">{}</span>
        <span class="na">machine</span><span class="pi">:</span>
          <span class="na">type</span><span class="pi">:</span> <span class="s2">"</span><span class="s">"</span>
        <span class="na">resources</span><span class="pi">:</span>
          <span class="na">requests</span><span class="pi">:</span>
            <span class="na">memory</span><span class="pi">:</span> <span class="s">1024M</span>
      <span class="na">networks</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">flatl2-overlay</span>
        <span class="na">multus</span><span class="pi">:</span>
          <span class="na">networkName</span><span class="pi">:</span> <span class="s">tenantred</span>
      <span class="na">terminationGracePeriodSeconds</span><span class="pi">:</span> <span class="m">0</span>
      <span class="na">volumes</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
          <span class="na">containerDisk</span><span class="pi">:</span>
            <span class="na">image</span><span class="pi">:</span> <span class="s">quay.io/kubevirt/fedora-with-test-tooling-container-disk:devel</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
          <span class="na">cloudInitNoCloud</span><span class="pi">:</span>
            <span class="na">networkData</span><span class="pi">:</span> <span class="pi">|</span>
              <span class="s">version: 2</span>
              <span class="s">ethernets:</span>
                <span class="s">eth0:</span>
                  <span class="s">addresses: [ 192.168.123.20/24 ]</span>
            <span class="na">userData</span><span class="pi">:</span> <span class="pi">|-</span>
              <span class="s">#cloud-config</span>
              <span class="s">password: fedora</span>
              <span class="s">chpasswd: { expire: False }</span>
<span class="s">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">kubevirt.io/v1alpha3</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachine</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">vm-blue-1</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">running</span><span class="pi">:</span> <span class="no">true</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">nodeSelector</span><span class="pi">:</span>
        <span class="na">kubernetes.io/hostname</span><span class="pi">:</span> <span class="s">ovn-worker</span>
      <span class="na">domain</span><span class="pi">:</span>
        <span class="na">devices</span><span class="pi">:</span>
          <span class="na">disks</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
              <span class="na">disk</span><span class="pi">:</span>
                <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
              <span class="na">disk</span><span class="pi">:</span>
                <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
          <span class="na">interfaces</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">physnet-blue</span>
            <span class="na">bridge</span><span class="pi">:</span> <span class="pi">{}</span>
        <span class="na">machine</span><span class="pi">:</span>
          <span class="na">type</span><span class="pi">:</span> <span class="s2">"</span><span class="s">"</span>
        <span class="na">resources</span><span class="pi">:</span>
          <span class="na">requests</span><span class="pi">:</span>
            <span class="na">memory</span><span class="pi">:</span> <span class="s">1024M</span>
      <span class="na">networks</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">physnet-blue</span>
        <span class="na">multus</span><span class="pi">:</span>
          <span class="na">networkName</span><span class="pi">:</span> <span class="s">tenantblue</span>
      <span class="na">terminationGracePeriodSeconds</span><span class="pi">:</span> <span class="m">0</span>
      <span class="na">volumes</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
          <span class="na">containerDisk</span><span class="pi">:</span>
            <span class="na">image</span><span class="pi">:</span> <span class="s">quay.io/kubevirt/fedora-with-test-tooling-container-disk:devel</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
          <span class="na">cloudInitNoCloud</span><span class="pi">:</span>
            <span class="na">networkData</span><span class="pi">:</span> <span class="pi">|</span>
              <span class="s">version: 2</span>
              <span class="s">ethernets:</span>
                <span class="s">eth0:</span>
                  <span class="s">addresses: [ 192.168.124.10/24 ]</span>
            <span class="na">userData</span><span class="pi">:</span> <span class="pi">|-</span>
              <span class="s">#cloud-config</span>
              <span class="s">password: fedora</span>
              <span class="s">chpasswd: { expire: False }</span>
<span class="s">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">kubevirt.io/v1alpha3</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachine</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">vm-blue-2</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">running</span><span class="pi">:</span> <span class="no">true</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">nodeSelector</span><span class="pi">:</span>
        <span class="na">kubernetes.io/hostname</span><span class="pi">:</span> <span class="s">ovn-worker</span>
      <span class="na">domain</span><span class="pi">:</span>
        <span class="na">devices</span><span class="pi">:</span>
          <span class="na">disks</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
              <span class="na">disk</span><span class="pi">:</span>
                <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
              <span class="na">disk</span><span class="pi">:</span>
                <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
          <span class="na">interfaces</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">physnet-blue</span>
            <span class="na">bridge</span><span class="pi">:</span> <span class="pi">{}</span>
        <span class="na">machine</span><span class="pi">:</span>
          <span class="na">type</span><span class="pi">:</span> <span class="s2">"</span><span class="s">"</span>
        <span class="na">resources</span><span class="pi">:</span>
          <span class="na">requests</span><span class="pi">:</span>
            <span class="na">memory</span><span class="pi">:</span> <span class="s">1024M</span>
      <span class="na">networks</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">physnet-blue</span>
        <span class="na">multus</span><span class="pi">:</span>
          <span class="na">networkName</span><span class="pi">:</span> <span class="s">tenantblue</span>
      <span class="na">terminationGracePeriodSeconds</span><span class="pi">:</span> <span class="m">0</span>
      <span class="na">volumes</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
          <span class="na">containerDisk</span><span class="pi">:</span>
            <span class="na">image</span><span class="pi">:</span> <span class="s">quay.io/kubevirt/fedora-with-test-tooling-container-disk:devel</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
          <span class="na">cloudInitNoCloud</span><span class="pi">:</span>
            <span class="na">networkData</span><span class="pi">:</span> <span class="pi">|</span>
              <span class="s">version: 2</span>
              <span class="s">ethernets:</span>
                <span class="s">eth0:</span>
                  <span class="s">addresses: [ 192.168.124.20/24 ]</span>
            <span class="na">userData</span><span class="pi">:</span> <span class="pi">|-</span>
              <span class="s">#cloud-config</span>
              <span class="s">password: fedora</span>
              <span class="s">chpasswd: { expire: False }</span>
</code></pre></div></div>

<h4 id="test-east--west-communication-1">Test East / West communication</h4>
<p>You can check east/west connectivity between both <strong>red</strong> VMs via ICMP:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl get vmi vm-red-2 <span class="nt">-ojsonpath</span><span class="o">=</span><span class="s2">"{ @.status.interfaces }"</span> | jq
<span class="o">[</span>
  <span class="o">{</span>
    <span class="s2">"infoSource"</span>: <span class="s2">"domain, guest-agent"</span>,
    <span class="s2">"interfaceName"</span>: <span class="s2">"eth0"</span>,
    <span class="s2">"ipAddress"</span>: <span class="s2">"192.168.123.20"</span>,
    <span class="s2">"ipAddresses"</span>: <span class="o">[</span>
      <span class="s2">"192.168.123.20"</span>,
      <span class="s2">"fe80::e83d:16ff:fe76:c1bd"</span>
    <span class="o">]</span>,
    <span class="s2">"mac"</span>: <span class="s2">"ea:3d:16:76:c1:bd"</span>,
    <span class="s2">"name"</span>: <span class="s2">"flatl2-overlay"</span>,
    <span class="s2">"queueCount"</span>: 1
  <span class="o">}</span>
<span class="o">]</span>

<span class="nv">$ </span>virtctl console vm-red-1
Successfully connected to vm-red-1 console. The escape sequence is ^]

<span class="o">[</span>fedora@vm-red-1 ~]<span class="nv">$ </span>ping 192.168.123.20
PING 192.168.123.20 <span class="o">(</span>192.168.123.20<span class="o">)</span> 56<span class="o">(</span>84<span class="o">)</span> bytes of data.
64 bytes from 192.168.123.20: <span class="nv">icmp_seq</span><span class="o">=</span>1 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>0.534 ms
64 bytes from 192.168.123.20: <span class="nv">icmp_seq</span><span class="o">=</span>2 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>0.246 ms
64 bytes from 192.168.123.20: <span class="nv">icmp_seq</span><span class="o">=</span>3 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>0.178 ms
64 bytes from 192.168.123.20: <span class="nv">icmp_seq</span><span class="o">=</span>4 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>0.236 ms

<span class="nt">---</span> 192.168.123.20 ping statistics <span class="nt">---</span>
4 packets transmitted, 4 received, 0% packet loss, <span class="nb">time </span>3028ms
rtt min/avg/max/mdev <span class="o">=</span> 0.178/0.298/0.534/0.138 ms
</code></pre></div></div>

<p>The same behavior can be seen on the VMs attached to the <strong>blue</strong> network:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl get vmi vm-blue-2 <span class="nt">-ojsonpath</span><span class="o">=</span><span class="s2">"{ @.status.interfaces }"</span> | jq
<span class="o">[</span>
  <span class="o">{</span>
    <span class="s2">"infoSource"</span>: <span class="s2">"domain, guest-agent"</span>,
    <span class="s2">"interfaceName"</span>: <span class="s2">"eth0"</span>,
    <span class="s2">"ipAddress"</span>: <span class="s2">"192.168.124.20"</span>,
    <span class="s2">"ipAddresses"</span>: <span class="o">[</span>
      <span class="s2">"192.168.124.20"</span>,
      <span class="s2">"fe80::6cae:e4ff:fefc:bd02"</span>
    <span class="o">]</span>,
    <span class="s2">"mac"</span>: <span class="s2">"6e:ae:e4:fc:bd:02"</span>,
    <span class="s2">"name"</span>: <span class="s2">"physnet-blue"</span>,
    <span class="s2">"queueCount"</span>: 1
  <span class="o">}</span>
<span class="o">]</span>

<span class="nv">$ </span>virtctl console vm-blue-1
Successfully connected to vm-blue-1 console. The escape sequence is ^]

<span class="o">[</span>fedora@vm-blue-1 ~]<span class="nv">$ </span>ping 
<span class="o">[</span>fedora@vm-blue-1 ~]<span class="nv">$ </span>ping 192.168.124.20
PING 192.168.124.20 <span class="o">(</span>192.168.124.20<span class="o">)</span> 56<span class="o">(</span>84<span class="o">)</span> bytes of data.
64 bytes from 192.168.124.20: <span class="nv">icmp_seq</span><span class="o">=</span>1 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>0.531 ms
64 bytes from 192.168.124.20: <span class="nv">icmp_seq</span><span class="o">=</span>2 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>0.255 ms
64 bytes from 192.168.124.20: <span class="nv">icmp_seq</span><span class="o">=</span>3 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>0.688 ms
64 bytes from 192.168.124.20: <span class="nv">icmp_seq</span><span class="o">=</span>4 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>0.648 ms

<span class="nt">---</span> 192.168.124.20 ping statistics <span class="nt">---</span>
4 packets transmitted, 4 received, 0% packet loss, <span class="nb">time </span>3047ms
rtt min/avg/max/mdev <span class="o">=</span> 0.255/0.530/0.688/0.169 ms
</code></pre></div></div>

<h3 id="accessing-the-underlay-services">Accessing the underlay services</h3>
<p>We can now start HTTP servers listening to the IPs attached on the VLAN
interfaces:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python3 <span class="nt">-m</span> http.server <span class="nt">--bind</span> 192.168.123.1 9000 &amp;
python3 <span class="nt">-m</span> http.server <span class="nt">--bind</span> 192.168.124.1 9000 &amp;
</code></pre></div></div>

<p>And finally curl this from your client (blue network):</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>fedora@vm-blue-1 ~]<span class="nv">$ </span>curl <span class="nt">-v</span> 192.168.124.1:9000
<span class="k">*</span>   Trying 192.168.124.1:9000...
<span class="k">*</span> Connected to 192.168.124.1 <span class="o">(</span>192.168.124.1<span class="o">)</span> port 9000 <span class="o">(</span><span class="c">#0)</span>
<span class="o">&gt;</span> GET / HTTP/1.1
<span class="o">&gt;</span> Host: 192.168.124.1:9000
<span class="o">&gt;</span> User-Agent: curl/7.69.1
<span class="o">&gt;</span> Accept: <span class="k">*</span>/<span class="k">*</span>
<span class="o">&gt;</span> 
<span class="k">*</span> Mark bundle as not supporting multiuse
<span class="k">*</span> HTTP 1.0, assume close after body
&lt; HTTP/1.0 200 OK
&lt; Server: SimpleHTTP/0.6 Python/3.11.3
&lt; Date: Thu, 01 Jun 2023 16:05:09 GMT
&lt; Content-type: text/html<span class="p">;</span> <span class="nv">charset</span><span class="o">=</span>utf-8
&lt; Content-Length: 2923
...
</code></pre></div></div>

<p>And from the client connected to the red network:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>fedora@vm-red-1 ~]<span class="nv">$ </span>curl <span class="nt">-v</span> 192.168.123.1:9000
<span class="k">*</span>   Trying 192.168.123.1:9000...
<span class="k">*</span> Connected to 192.168.123.1 <span class="o">(</span>192.168.123.1<span class="o">)</span> port 9000 <span class="o">(</span><span class="c">#0)</span>
<span class="o">&gt;</span> GET / HTTP/1.1
<span class="o">&gt;</span> Host: 192.168.123.1:9000
<span class="o">&gt;</span> User-Agent: curl/7.69.1
<span class="o">&gt;</span> Accept: <span class="k">*</span>/<span class="k">*</span>
<span class="o">&gt;</span> 
<span class="k">*</span> Mark bundle as not supporting multiuse
<span class="k">*</span> HTTP 1.0, assume close after body
&lt; HTTP/1.0 200 OK
&lt; Server: SimpleHTTP/0.6 Python/3.11.3
&lt; Date: Thu, 01 Jun 2023 16:06:02 GMT
&lt; Content-type: text/html<span class="p">;</span> <span class="nv">charset</span><span class="o">=</span>utf-8
&lt; Content-Length: 2923
&lt; 
...
</code></pre></div></div>

<h2 id="conclusions">Conclusions</h2>
<p>In this post we have seen how to use OVN-Kubernetes to create secondary
networks connected to the physical underlay, allowing both east/west
communication between VMs, and access to services running outside the
Kubernetes cluster.</p>]]></content><author><name>Miguel Duarte Barroso</name></author><category term="news" /><category term="Kubevirt" /><category term="kubernetes" /><category term="virtual machine" /><category term="VM" /><category term="SDN" /><category term="OVN" /><summary type="html"><![CDATA[This post explains how to configure secondary networks connected to the physical underlay for KubeVirt virtual machines.]]></summary></entry><entry><title type="html">Secondary networks for KubeVirt VMs using OVN-Kubernetes</title><link href="https://kubevirt.io//2023/OVN-kubernetes-secondary-networks.html" rel="alternate" type="text/html" title="Secondary networks for KubeVirt VMs using OVN-Kubernetes" /><published>2023-03-06T00:00:00+00:00</published><updated>2023-03-06T00:00:00+00:00</updated><id>https://kubevirt.io//2023/OVN-kubernetes-secondary-networks</id><content type="html" xml:base="https://kubevirt.io//2023/OVN-kubernetes-secondary-networks.html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>OVN (Open Virtual Network) is a series of daemons for the Open vSwitch that
translate virtual network configurations into OpenFlow. It provides virtual
networking capabilities for any type of workload on a virtualized platform
(virtual machines and containers) using the same API.</p>

<p>OVN provides a higher-layer of abstraction than Open vSwitch, working with
logical routers and logical switches, rather than flows.
More details can be found in the OVN architecture
<a href="https://man7.org/linux/man-pages/man7/ovn-architecture.7.html#DESCRIPTION">man page</a>.</p>

<p>In this post we will repeat the scenario of
<a href="https://kubevirt.io/2020/Multiple-Network-Attachments-with-bridge-CNI.html">its bridge CNI equivalent</a>,
using this SDN approach, which uses virtual networking infrastructure: thus, it
is <strong>not</strong> required to provision VLANs or other physical network resources.</p>

<h2 id="demo">Demo</h2>
<p>To run this demo, you will need a Kubernetes cluster with the following
components installed:</p>
<ul>
  <li>OVN-Kubernetes</li>
  <li>multus-cni</li>
  <li>KubeVirt</li>
</ul>

<p>The <a href="#environment-setup">following section</a> will show you how to create a
<a href="https://kind.sigs.k8s.io/">KinD</a> cluster, with upstream latest OVN-Kubernetes,
and upstream latest multus-cni deployed. Please <strong>skip</strong> this section if your
cluster already features these components (e.g. Openshift).</p>

<h3 id="setup-demo-environment">Setup demo environment</h3>
<p>Refer to the OVN-Kubernetes repo
<a href="https://github.com/ovn-org/ovn-kubernetes/blob/master/docs/kind.md#ovn-kubernetes-kind-setup">KIND documentation</a>
for more details; the gist of it is you should clone the OVN-Kubernetes
repository, and run their kind helper script:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone git@github.com:ovn-org/ovn-kubernetes.git

<span class="nb">cd </span>ovn-kubernetes
<span class="nb">pushd </span>contrib <span class="p">;</span> ./kind.sh <span class="nt">--multi-network-enable</span> <span class="p">;</span> <span class="nb">popd</span>
</code></pre></div></div>

<p>This will get you a running kind cluster, configured to use OVN-Kubernetes as
the default cluster network, configuring the multi-homing OVN-Kubernetes feature
gate, and deploying
<a href="https://github.com/k8snetworkplumbingwg/multus-cni">multus-cni</a> in the cluster.</p>

<h4 id="install-kubevirt-in-the-cluster">Install KubeVirt in the cluster</h4>
<p>Follow Kubevirt’s
<a href="https://kubevirt.io/user-guide/operations/installation/#installing-kubevirt-on-kubernetes">user guide</a>
to install the latest released version (currently, v0.59.0). Please skip this
section if you already have a running cluster with KubeVirt installed in it.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">export </span><span class="nv">RELEASE</span><span class="o">=</span><span class="si">$(</span>curl https://storage.googleapis.com/kubevirt-prow/release/kubevirt/kubevirt/stable.txt<span class="si">)</span>
kubectl apply <span class="nt">-f</span> <span class="s2">"https://github.com/kubevirt/kubevirt/releases/download/</span><span class="k">${</span><span class="nv">RELEASE</span><span class="k">}</span><span class="s2">/kubevirt-operator.yaml"</span>
kubectl apply <span class="nt">-f</span> <span class="s2">"https://github.com/kubevirt/kubevirt/releases/download/</span><span class="k">${</span><span class="nv">RELEASE</span><span class="k">}</span><span class="s2">/kubevirt-cr.yaml"</span>
kubectl <span class="nt">-n</span> kubevirt <span class="nb">wait </span>kv kubevirt <span class="nt">--timeout</span><span class="o">=</span>360s <span class="nt">--for</span> <span class="nv">condition</span><span class="o">=</span>Available
</code></pre></div></div>

<p>Now we have a Kubernetes cluster with all the pieces to start the Demo.</p>

<h3 id="define-the-overlay-network">Define the overlay network</h3>
<p>Provision the following yaml to define the overlay which will configure the
secondary attachment for the KubeVirt VMs. Please refer to the OVN-Kubernetes
user
<a href="https://github.com/ovn-org/ovn-kubernetes/blob/master/docs/multi-homing.md#switched---layer-2---topology">documentation</a>
for details into each of the knobs.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | kubectl apply -f -
apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
metadata:
  name: l2-network
  namespace: default
spec:
  config: |2
    {
            "cniVersion": "0.3.1",
            "name": "l2-network",
            "type": "ovn-k8s-cni-overlay",
            "topology":"layer2",
            "netAttachDefName": "default/l2-network"
    }
</span><span class="no">EOF
</span></code></pre></div></div>

<p>The above example will configure a cluster-wide overlay <strong>without</strong> a subnet
defined. This means the users will have to define static IPs for their VMs.</p>

<p>It is also worth to point out the value of the <code class="language-plaintext highlighter-rouge">netAttachDefName</code> attribute
must match the <code class="language-plaintext highlighter-rouge">&lt;namespace&gt;/&lt;name&gt;</code> of the surrounding
<code class="language-plaintext highlighter-rouge">NetworkAttachmentDefinition</code> object.</p>

<h3 id="spin-up-the-vms">Spin up the VMs</h3>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | kubectl apply -f -
---
apiVersion: kubevirt.io/v1alpha3
kind: VirtualMachine
metadata:
  name: vm-server
spec:
  running: true
  template:
    spec:
      domain:
        devices:
          disks:
            - name: containerdisk
              disk:
                bus: virtio
            - name: cloudinitdisk
              disk:
                bus: virtio
          interfaces:
          - name: default
            masquerade: {}
          - name: flatl2-overlay
            bridge: {}
        machine:
          type: ""
        resources:
          requests:
            memory: 1024M
      networks:
      - name: default
        pod: {}
      - name: flatl2-overlay
        multus:
          networkName: l2-network
      terminationGracePeriodSeconds: 0
      volumes:
        - name: containerdisk
          containerDisk:
            image: quay.io/kubevirt/fedora-with-test-tooling-container-disk:devel
        - name: cloudinitdisk
          cloudInitNoCloud:
            networkData: |
              version: 2
              ethernets:
                eth1:
                  addresses: [ 192.0.2.20/24 ]
            userData: |-
              #cloud-config
              password: fedora
              chpasswd: { expire: False }
---
apiVersion: kubevirt.io/v1alpha3
kind: VirtualMachine
metadata:
  name: vm-client
spec:
  running: true
  template:
    spec:
      domain:
        devices:
          disks:
            - name: containerdisk
              disk:
                bus: virtio
            - name: cloudinitdisk
              disk:
                bus: virtio
          interfaces:
          - name: default
            masquerade: {}
          - name: flatl2-overlay
            bridge: {}
        machine:
          type: ""
        resources:
          requests:
            memory: 1024M
      networks:
      - name: default
        pod: {}
      - name: flatl2-overlay
        multus:
          networkName: l2-network
      terminationGracePeriodSeconds: 0
      volumes:
        - name: containerdisk
          containerDisk:
            image: quay.io/kubevirt/fedora-with-test-tooling-container-disk:devel
        - name: cloudinitdisk
          cloudInitNoCloud:
            networkData: |
              version: 2
              ethernets:
                eth1:
                  addresses: [ 192.0.2.10/24 ]
            userData: |-
              #cloud-config
              password: fedora
              chpasswd: { expire: False }
</span><span class="no">EOF
</span></code></pre></div></div>

<p>Provision these two Virtual Machines, and wait for them to boot up.</p>

<h3 id="test-connectivity">Test connectivity</h3>
<p>To verify connectivity over our layer 2 overlay, we need first to ensure the IP
address of the server VM; let’s query the VMI status for that:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl get vmi vm-server <span class="nt">-ojsonpath</span><span class="o">=</span><span class="s2">"{ @.status.interfaces }"</span> | jq
<span class="o">[</span>
  <span class="o">{</span>
    <span class="s2">"infoSource"</span>: <span class="s2">"domain, guest-agent"</span>,
    <span class="s2">"interfaceName"</span>: <span class="s2">"eth0"</span>,
    <span class="s2">"ipAddress"</span>: <span class="s2">"10.244.2.8"</span>,
    <span class="s2">"ipAddresses"</span>: <span class="o">[</span>
      <span class="s2">"10.244.2.8"</span>
    <span class="o">]</span>,
    <span class="s2">"mac"</span>: <span class="s2">"52:54:00:23:1c:c2"</span>,
    <span class="s2">"name"</span>: <span class="s2">"default"</span>,
    <span class="s2">"queueCount"</span>: 1
  <span class="o">}</span>,
  <span class="o">{</span>
    <span class="s2">"infoSource"</span>: <span class="s2">"domain, guest-agent"</span>,
    <span class="s2">"interfaceName"</span>: <span class="s2">"eth1"</span>,
    <span class="s2">"ipAddress"</span>: <span class="s2">"192.0.2.20"</span>,
    <span class="s2">"ipAddresses"</span>: <span class="o">[</span>
      <span class="s2">"192.0.2.20"</span>,
      <span class="s2">"fe80::7cab:88ff:fe5b:39f"</span>
    <span class="o">]</span>,
    <span class="s2">"mac"</span>: <span class="s2">"7e:ab:88:5b:03:9f"</span>,
    <span class="s2">"name"</span>: <span class="s2">"flatl2-overlay"</span>,
    <span class="s2">"queueCount"</span>: 1
  <span class="o">}</span>
<span class="o">]</span>
</code></pre></div></div>

<p>You can afterwards connect to them via console and ping <code class="language-plaintext highlighter-rouge">vm-server</code>:</p>

<div class="premonition note"><div class="fa fa-check-square"></div><div class="content"><p class="header">Note</p><p>The user and password for this VMs is fedora; check the VM template spec cloudinit userData</p>


</div></div>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>virtctl console vm-client
ip a <span class="c"># confirm the IP address is the one set via cloud-init</span>
<span class="o">[</span>fedora@vm-client ~]<span class="nv">$ </span>ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    <span class="nb">link</span>/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1400 qdisc fq_codel state UP group default qlen 1000
    <span class="nb">link</span>/ether 52:54:00:29:de:53 brd ff:ff:ff:ff:ff:ff
    altname enp1s0
    inet 10.0.2.2/24 brd 10.0.2.255 scope global dynamic noprefixroute eth0
       valid_lft 86313584sec preferred_lft 86313584sec
    inet6 fe80::5054:ff:fe29:de53/64 scope <span class="nb">link
       </span>valid_lft forever preferred_lft forever
3: eth1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1400 qdisc fq_codel state UP group default qlen 1000
    <span class="nb">link</span>/ether 36:f9:29:65:66:55 brd ff:ff:ff:ff:ff:ff
    altname enp2s0
    inet 192.0.2.10/24 brd 192.0.2.255 scope global noprefixroute eth1
       valid_lft forever preferred_lft forever
    inet6 fe80::34f9:29ff:fe65:6655/64 scope <span class="nb">link
       </span>valid_lft forever preferred_lft forever

<span class="o">[</span>fedora@vm-client ~]<span class="nv">$ </span>ping <span class="nt">-c4</span> 192.0.2.20 <span class="c"># ping the vm-server static IP</span>
PING 192.0.2.20 <span class="o">(</span>192.0.2.20<span class="o">)</span> 56<span class="o">(</span>84<span class="o">)</span> bytes of data.
64 bytes from 192.0.2.20: <span class="nv">icmp_seq</span><span class="o">=</span>1 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>1.05 ms
64 bytes from 192.0.2.20: <span class="nv">icmp_seq</span><span class="o">=</span>2 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>1.05 ms
64 bytes from 192.0.2.20: <span class="nv">icmp_seq</span><span class="o">=</span>3 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>0.995 ms
64 bytes from 192.0.2.20: <span class="nv">icmp_seq</span><span class="o">=</span>4 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>0.902 ms

<span class="nt">---</span> 192.0.2.20 ping statistics <span class="nt">---</span>
4 packets transmitted, 4 received, 0% packet loss, <span class="nb">time </span>3006ms
rtt min/avg/max/mdev <span class="o">=</span> 0.902/0.997/1.046/0.058 ms
</code></pre></div></div>
<h2 id="conclusion">Conclusion</h2>
<p>In this post we have seen how to use OVN-Kubernetes to create an overlay to
connect VMs in different nodes using secondary networks, without having to
configure any physical networking infrastructure.</p>]]></content><author><name>Miguel Duarte Barroso</name></author><category term="news" /><category term="Kubevirt" /><category term="kubernetes" /><category term="virtual machine" /><category term="VM" /><category term="SDN" /><category term="OVN" /><summary type="html"><![CDATA[This post explains how to configure cluster-wide overlays as secondary networks for KubeVirt virtual machines.]]></summary></entry><entry><title type="html">KubeVirt Summit 2023!</title><link href="https://kubevirt.io//2023/KubeVirt-Summit-2023.html" rel="alternate" type="text/html" title="KubeVirt Summit 2023!" /><published>2023-03-03T00:00:00+00:00</published><updated>2023-03-03T00:00:00+00:00</updated><id>https://kubevirt.io//2023/KubeVirt-Summit-2023</id><content type="html" xml:base="https://kubevirt.io//2023/KubeVirt-Summit-2023.html"><![CDATA[<p>The third online <a href="/summit/">KubeVirt Summit</a> starts March 29, 2023!</p>

<h2 id="when">When</h2>

<p>The event will take place online over two half-days:</p>

<ul>
  <li>Dates: March 29 and 30, 2023.</li>
  <li>Time: 14:00 – 19:00 UTC (9:00–14:00 EST, 15:00–20:00 CET)</li>
</ul>

<h2 id="register">Register</h2>

<p><a href="/summit/">KubeVirt Summit</a> is hosted on Community.CNCF.io. This is a free event but you need to register in order to attend.</p>

<p><a href="https://community.cncf.io/events/details/cncf-kubevirt-community-presents-kubevirt-summit-2023/">Register for KubeVirt Summit 2023</a></p>

<p>If this is your first time attending, you will need to create an account with CNCF.io.</p>

<h2 id="schedule">Schedule</h2>

<p>The schedule is available on the <a href="https://community.cncf.io/events/details/cncf-kubevirt-community-presents-kubevirt-summit-2023/">CNCF Community Events page</a> where you register, as well as on the <a href="/summit/">KubeVirt Summit page</a>.</p>

<h2 id="keep-up-to-date">Keep up to date</h2>

<p>Connect with the KubeVirt Community through our <a href="/community">community page</a>.</p>

<p>See you there!</p>]]></content><author><name>Andrew Burden</name></author><category term="news" /><category term="kubevirt" /><category term="event" /><category term="community" /><summary type="html"><![CDATA[Join us for the KubeVirt community's third annual dedicated online event]]></summary></entry><entry><title type="html">Simplifying KubeVirt’s `VirtualMachine` UX with Instancetypes and Preferences</title><link href="https://kubevirt.io//2022/KubeVirt-Introduction-of-instancetypes.html" rel="alternate" type="text/html" title="Simplifying KubeVirt’s `VirtualMachine` UX with Instancetypes and Preferences" /><published>2022-08-12T00:00:00+00:00</published><updated>2022-08-12T00:00:00+00:00</updated><id>https://kubevirt.io//2022/KubeVirt-Introduction-of-instancetypes</id><content type="html" xml:base="https://kubevirt.io//2022/KubeVirt-Introduction-of-instancetypes.html"><![CDATA[<p>KubeVirt’s <a href="https://kubevirt.io/api-reference/main/definitions.html#_v1_virtualmachine"><code class="language-plaintext highlighter-rouge">VirtualMachine</code></a> API contains many advanced options for tuning a virtual machine’s resources and performance that go beyond what typical users need to be aware of. Users have until now been unable to simply define the storage/network they want assigned to their VM and then declare in broad terms what quality of resources and kind of performance they need for their VM. Instead, the user has to be keenly aware how to request specific compute resources alongside all of the performance tunings available on the <a href="https://kubevirt.io/api-reference/main/definitions.html#_v1_virtualmachine"><code class="language-plaintext highlighter-rouge">VirtualMachine</code></a> API and how those tunings impact their guest’s operating system in order to get a desired result.</p>

<p>A common pattern for IaaS is to have abstractions separating the resource sizing and performance of a workload from the user-defined values related to launching their custom application. This pattern is evident across all the major cloud providers (also known as hyperscalers) as well as open source IaaS projects like OpenStack. AWS has <a href="https://aws.amazon.com/ec2/instance-types/">instance types</a>, GCP has <a href="https://cloud.google.com/compute/docs/machine-types#custom_machine_types">machine types</a>, Azure has <a href="https://docs.microsoft.com/en-us/azure/virtual-machines/sizes">instance VM sizes</a>, and OpenStack has <a href="https://docs.openstack.org/nova/latest/user/flavors.html">flavors</a>.</p>

<p>Let’s take AWS for example to help visualize what this abstraction enables. Launching an EC2 instance only requires a few top level arguments; the disk image, instance type, keypair, security group, and subnet:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>aws ec2 run-instances <span class="nt">--image-id</span> ami-xxxxxxxx <span class="se">\</span>
                        <span class="nt">--count</span> 1 <span class="se">\</span>
                        <span class="nt">--instance-type</span> c4.xlarge <span class="se">\</span>
                        <span class="nt">--key-name</span> MyKeyPair <span class="se">\</span>
                        <span class="nt">--security-group-ids</span> sg-903004f8 <span class="se">\</span>
                        <span class="nt">--subnet-id</span> subnet-6e7f829e
</code></pre></div></div>

<p>When creating the EC2 instance the user doesn’t define the amount of resources, what processor to use, how to optimize the performance of the instance, or what hardware to schedule the instance on. Instead, all of that information is wrapped up in that single <code class="language-plaintext highlighter-rouge">--instance-type c4.xlarge</code> CLI argument. <code class="language-plaintext highlighter-rouge">c4</code> denotes a specific performance profile version, in this case from the <code class="language-plaintext highlighter-rouge">Compute Optimized</code> family and <code class="language-plaintext highlighter-rouge">xlarge</code> denotes a specific amount of compute resources provided by the instance type, in this case 4 vCPUs, 7.5 GiB of RAM, 750 Mbps EBS bandwidth, etc.</p>

<p>While hyperscalers can provide predefined types with performance profiles and compute resources already assigned IaaS and virtualization projects such as OpenStack and KubeVirt can only provide the raw abstractions for operators, admins, and even vendors to then create instances of these abstractions specific to each deployment.</p>

<h2 id="instancetype-api">Instancetype API</h2>

<p>The recently renamed instancetype API and associated <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/"><code class="language-plaintext highlighter-rouge">CRDs</code></a> aim to address this by providing KubeVirt users with a set of APIs and abstractions that allow them to make fewer choices when creating a <a href="https://kubevirt.io/api-reference/main/definitions.html#_v1_virtualmachine"><code class="language-plaintext highlighter-rouge">VirtualMachine</code></a> while still ending up with a working, performant guest at runtime.</p>

<h2 id="virtualmachineinstancetype">VirtualMachineInstancetype</h2>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">instancetype.kubevirt.io/v1alpha1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachineInstancetype</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">example-instancetype</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">cpu</span><span class="pi">:</span>
    <span class="na">guest</span><span class="pi">:</span> <span class="m">1</span>
  <span class="na">memory</span><span class="pi">:</span>
    <span class="na">guest</span><span class="pi">:</span> <span class="s">128Mi</span>
</code></pre></div></div>

<p>KubeVirt now provides two instancetype based <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/"><code class="language-plaintext highlighter-rouge">CRDs</code></a>, a cluster wide <a href="http://kubevirt.io/api-reference/main/definitions.html#_v1alpha1_virtualmachineclusterinstancetype"><code class="language-plaintext highlighter-rouge">VirtualMachineClusterInstancetype</code></a> and a namespaced <a href="http://kubevirt.io/api-reference/main/definitions.html#_v1alpha1_virtualmachineinstancetype"><code class="language-plaintext highlighter-rouge">VirtualMachineInstancetype</code></a>. These <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/"><code class="language-plaintext highlighter-rouge">CRDs</code></a> encapsulate the following resource related characteristics of a <a href="http://kubevirt.io/api-reference/main/definitions.html#_v1alpha1_virtualmachine"><code class="language-plaintext highlighter-rouge">VirtualMachine</code></a> through a shared <a href="http://kubevirt.io/api-reference/main/definitions.html#_v1alpha1_virtualmachineinstancetypespec"><code class="language-plaintext highlighter-rouge">VirtualMachineInstancetypeSpec</code></a>:</p>

<ul>
  <li><a href="http://kubevirt.io/api-reference/main/definitions.html#_v1alpha1_cpuinstancetype">CPU</a> : Required number of vCPUs presented to the guest</li>
  <li><a href="http://kubevirt.io/api-reference/main/definitions.html#_v1alpha1_memoryinstancetype">Memory</a> : Required amount of memory presented to the guest</li>
  <li><a href="http://kubevirt.io/api-reference/main/definitions.html#_v1_gpu">GPUs</a> : Optional list of vGPUs to passthrough</li>
  <li><a href="http://kubevirt.io/api-reference/main/definitions.html#_v1_hostdevice">HostDevices</a>: Optional list of HostDevices to passthrough</li>
  <li><a href="`string`">IOThreadsPolicy</a> : Optional IOThreadsPolicy to be used</li>
  <li><a href="http://kubevirt.io/api-reference/main/definitions.html#_v1_launchsecurity">LaunchSecurity</a>: Optional LaunchSecurity to be used</li>
</ul>

<p>Anything provided within an instancetype cannot be overridden within a <a href="http://kubevirt.io/api-reference/main/definitions.html#_v1alpha1_virtualmachine"><code class="language-plaintext highlighter-rouge">VirtualMachine</code></a>. For example, <code class="language-plaintext highlighter-rouge">CPU</code> and <code class="language-plaintext highlighter-rouge">Memory</code> are both required attributes of an instancetype. If a user makes any requests for <code class="language-plaintext highlighter-rouge">CPU</code> or <code class="language-plaintext highlighter-rouge">Memory</code> resources within their <a href="http://kubevirt.io/api-reference/main/definitions.html#_v1alpha1_virtualmachine"><code class="language-plaintext highlighter-rouge">VirtualMachine</code></a>, the instancetype will conflict and the request will be rejected.</p>

<h2 id="virtualmachinepreference">VirtualMachinePreference</h2>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">instancetype.kubevirt.io/v1alpha1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachinePreference</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">example-preference</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">devices</span><span class="pi">:</span>
    <span class="na">preferredDiskBus</span><span class="pi">:</span> <span class="s">virtio</span>
    <span class="na">preferredInterfaceModel</span><span class="pi">:</span> <span class="s">virtio</span>
</code></pre></div></div>

<p>KubeVirt also provides two further preference based <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/"><code class="language-plaintext highlighter-rouge">CRDs</code></a>, again a cluster-wide <a href="https://kubevirt.io/api-reference/main/definitions.html#_v1alpha1_virtualmachineclusterpreference"><code class="language-plaintext highlighter-rouge">VirtualMachineClusterPreference</code></a> and namespaced <a href="https://kubevirt.io/api-reference/main/definitions.html#_v1alpha1_virtualmachinepreference"><code class="language-plaintext highlighter-rouge">VirtualMachinePreference</code></a>. These <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/"><code class="language-plaintext highlighter-rouge">CRDs</code></a> encapsulate the preferred value of any remaining attributes of a <a href="http://kubevirt.io/api-reference/main/definitions.html#_v1alpha1_virtualmachine"><code class="language-plaintext highlighter-rouge">VirtualMachine</code></a> required to run a given workload, again this is through a shared <a href="http://kubevirt.io/api-reference/main/definitions.html#_v1alpha1_virtualmachinepreferencespec"><code class="language-plaintext highlighter-rouge">VirtualMachinePreferenceSpec</code></a>.</p>

<p>Unlike instancetypes, preferences only represent the preferred values and as such can be overridden by values in the <a href="http://kubevirt.io/api-reference/main/definitions.html#_v1alpha1_virtualmachine"><code class="language-plaintext highlighter-rouge">VirtualMachine</code></a> provided by the user.</p>

<h2 id="virtualmachineinstancetypepreferencematcher">VirtualMachine{Instancetype,Preference}Matcher</h2>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">kubevirt.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachine</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">example-vm</span>
<span class="na">spec</span><span class="pi">:</span>
<span class="pi">[</span><span class="nv">..</span><span class="pi">]</span>
  <span class="na">instancetype</span><span class="pi">:</span>
    <span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachineInstancetype</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">example-instancetype</span>
  <span class="na">preference</span><span class="pi">:</span>
    <span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachinePreference</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">example-preference</span>
<span class="pi">[</span><span class="nv">..</span><span class="pi">]</span>
</code></pre></div></div>

<p>The previous instancetype and preference CRDs are matched to a given <a href="http://kubevirt.io/api-reference/main/definitions.html#_v1alpha1_virtualmachine"><code class="language-plaintext highlighter-rouge">VirtualMachine</code></a> through the use of a matcher. Each matcher consists of the following:</p>

<ul>
  <li>Name (string): Name of the resource being referenced</li>
  <li>Kind (string):  Optional, defaults to the cluster wide CRD kinds of <code class="language-plaintext highlighter-rouge">VirtualMachineClusterInstancetype</code> or <code class="language-plaintext highlighter-rouge">VirtualMachineClusterPreference</code></li>
  <li>RevisionName (string) : Optional, name of a <a href="https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/controller-revision-v1/">ControllerRevision</a> containing a copy of the <a href="http://kubevirt.io/api-reference/main/definitions.html#_v1alpha1_virtualmachineinstancetypespec"><code class="language-plaintext highlighter-rouge">VirtualMachineInstancetypeSpec</code></a> or <a href="http://kubevirt.io/api-reference/main/definitions.html#_v1alpha1_virtualmachinepreferencespec"><code class="language-plaintext highlighter-rouge">VirtualMachinePreferenceSpec</code></a> taken when the <a href="http://kubevirt.io/api-reference/main/definitions.html#_v1alpha1_virtualmachine"><code class="language-plaintext highlighter-rouge">VirtualMachine</code></a> is first started.</li>
</ul>

<h2 id="virtualmachineinstancepreset-deprecation">VirtualMachineInstancePreset Deprecation</h2>

<p>The new instancetype API and CRDs conflict somewhat with the existing <a href="https://kubevirt.io/api-reference/main/definitions.html#_v1_virtualmachineinstancepreset"><code class="language-plaintext highlighter-rouge">VirtualMachineInstancePreset</code></a> CRD. The approach taken by the CRD has also been removed in core k8s so, as advertised on the <a href="https://groups.google.com/g/kubevirt-dev/c/eM7JaDV_EU8">mailing list</a>, I have started the <a href="https://github.com/kubevirt/kubevirt/pull/8069">process of deprecating</a> <a href="https://kubevirt.io/api-reference/main/definitions.html#_v1_virtualmachineinstancepreset"><code class="language-plaintext highlighter-rouge">VirtualMachineInstancePreset</code></a> in favor of the Instancetype CRDs listed above.</p>

<h2 id="examples">Examples</h2>

<p>The following example is taken from the <a href="https://kubevirt.io/user-guide/virtual_machines/instancetypes/">KubeVirt User Guide</a>:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">$ cat &lt;&lt; EOF | kubectl apply -f -</span> 
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">instancetype.kubevirt.io/v1alpha1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachineInstancetype</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">cmedium</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">cpu</span><span class="pi">:</span>
    <span class="na">guest</span><span class="pi">:</span> <span class="m">1</span>
  <span class="na">memory</span><span class="pi">:</span>
    <span class="na">guest</span><span class="pi">:</span> <span class="s">1Gi</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">instancetype.kubevirt.io/v1alpha1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachinePreference</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">fedora</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">devices</span><span class="pi">:</span>
    <span class="na">preferredDiskBus</span><span class="pi">:</span> <span class="s">virtio</span>
    <span class="na">preferredInterfaceModel</span><span class="pi">:</span> <span class="s">virtio</span>
    <span class="na">preferredRng</span><span class="pi">:</span> <span class="pi">{}</span>
  <span class="na">features</span><span class="pi">:</span>
    <span class="na">preferredAcpi</span><span class="pi">:</span> <span class="pi">{}</span>
    <span class="na">preferredSmm</span><span class="pi">:</span> <span class="pi">{}</span>
  <span class="na">firmware</span><span class="pi">:</span>
    <span class="na">preferredUseEfi</span><span class="pi">:</span> <span class="no">true</span>
    <span class="na">preferredUseSecureBoot</span><span class="pi">:</span> <span class="no">true</span>    
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">kubevirt.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachine</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">creationTimestamp</span><span class="pi">:</span> <span class="no">null</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">fedora</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">instancetype</span><span class="pi">:</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">cmedium</span>
    <span class="na">kind</span><span class="pi">:</span> <span class="s">virtualMachineInstancetype</span>
  <span class="na">preference</span><span class="pi">:</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">fedora</span>
    <span class="na">kind</span><span class="pi">:</span> <span class="s">virtualMachinePreference</span>
  <span class="na">runStrategy</span><span class="pi">:</span> <span class="s">Always</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">creationTimestamp</span><span class="pi">:</span> <span class="no">null</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">domain</span><span class="pi">:</span>
        <span class="na">devices</span><span class="pi">:</span> <span class="pi">{}</span>
      <span class="na">volumes</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">containerDisk</span><span class="pi">:</span>
          <span class="na">image</span><span class="pi">:</span> <span class="s">quay.io/containerdisks/fedora:latest</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
      <span class="pi">-</span> <span class="na">cloudInitNoCloud</span><span class="pi">:</span>
          <span class="na">userData</span><span class="pi">:</span> <span class="pi">|-</span>
            <span class="s">#cloud-config</span>
            <span class="s">users:</span>
              <span class="s">- name: admin</span>
                <span class="s">sudo: ALL=(ALL) NOPASSWD:ALL</span>
                <span class="s">ssh_authorized_keys:</span>
                  <span class="s">- ssh-rsa AAAA...</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinit</span>
<span class="s">EOF</span>
</code></pre></div></div>

<p>We can compare the original <code class="language-plaintext highlighter-rouge">VirtualMachine</code> spec with that of the running <code class="language-plaintext highlighter-rouge">VirtualMachineInstance</code> to confirm our instancetype and preferences have been applied using the following <code class="language-plaintext highlighter-rouge">diff</code> command:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>diff <span class="nt">--color</span> <span class="nt">-u</span> &lt;<span class="o">(</span> kubectl get vms/fedora <span class="nt">-o</span> json | jq .spec.template.spec<span class="o">)</span> &lt;<span class="o">(</span> kubectl get vmis/fedora <span class="nt">-o</span> json | jq .spec<span class="o">)</span>
<span class="o">[</span>..]
 <span class="o">{</span>
   <span class="s2">"domain"</span>: <span class="o">{</span>
-    <span class="s2">"devices"</span>: <span class="o">{}</span>,
+    <span class="s2">"cpu"</span>: <span class="o">{</span>
+      <span class="s2">"cores"</span>: 1,
+      <span class="s2">"model"</span>: <span class="s2">"host-model"</span>,
+      <span class="s2">"sockets"</span>: 1,
+      <span class="s2">"threads"</span>: 1
+    <span class="o">}</span>,
+    <span class="s2">"devices"</span>: <span class="o">{</span>
+      <span class="s2">"disks"</span>: <span class="o">[</span>
+        <span class="o">{</span>
+          <span class="s2">"disk"</span>: <span class="o">{</span>
+            <span class="s2">"bus"</span>: <span class="s2">"virtio"</span>
+          <span class="o">}</span>,
+          <span class="s2">"name"</span>: <span class="s2">"containerdisk"</span>
+        <span class="o">}</span>,
+        <span class="o">{</span>
+          <span class="s2">"disk"</span>: <span class="o">{</span>
+            <span class="s2">"bus"</span>: <span class="s2">"virtio"</span>
+          <span class="o">}</span>,
+          <span class="s2">"name"</span>: <span class="s2">"cloudinit"</span>
+        <span class="o">}</span>
+      <span class="o">]</span>,
+      <span class="s2">"interfaces"</span>: <span class="o">[</span>
+        <span class="o">{</span>
+          <span class="s2">"bridge"</span>: <span class="o">{}</span>,
+          <span class="s2">"model"</span>: <span class="s2">"virtio"</span>,
+          <span class="s2">"name"</span>: <span class="s2">"default"</span>
+        <span class="o">}</span>
+      <span class="o">]</span>,
+      <span class="s2">"rng"</span>: <span class="o">{}</span>
+    <span class="o">}</span>,
+    <span class="s2">"features"</span>: <span class="o">{</span>
+      <span class="s2">"acpi"</span>: <span class="o">{</span>
+        <span class="s2">"enabled"</span>: <span class="nb">true</span>
+      <span class="o">}</span>,
+      <span class="s2">"smm"</span>: <span class="o">{</span>
+        <span class="s2">"enabled"</span>: <span class="nb">true</span>
+      <span class="o">}</span>
+    <span class="o">}</span>,
+    <span class="s2">"firmware"</span>: <span class="o">{</span>
+      <span class="s2">"bootloader"</span>: <span class="o">{</span>
+        <span class="s2">"efi"</span>: <span class="o">{</span>
+          <span class="s2">"secureBoot"</span>: <span class="nb">true</span>
+        <span class="o">}</span>
+      <span class="o">}</span>,
+      <span class="s2">"uuid"</span>: <span class="s2">"98f07cdd-96da-5880-b6c7-1a5700b73dc4"</span>
+    <span class="o">}</span>,
     <span class="s2">"machine"</span>: <span class="o">{</span>
       <span class="s2">"type"</span>: <span class="s2">"q35"</span>
     <span class="o">}</span>,
-    <span class="s2">"resources"</span>: <span class="o">{}</span>
+    <span class="s2">"memory"</span>: <span class="o">{</span>
+      <span class="s2">"guest"</span>: <span class="s2">"1Gi"</span>
+    <span class="o">}</span>,
+    <span class="s2">"resources"</span>: <span class="o">{</span>
+      <span class="s2">"requests"</span>: <span class="o">{</span>
+        <span class="s2">"memory"</span>: <span class="s2">"1Gi"</span>
+      <span class="o">}</span>
+    <span class="o">}</span>
   <span class="o">}</span>,
+  <span class="s2">"networks"</span>: <span class="o">[</span>
+    <span class="o">{</span>
+      <span class="s2">"name"</span>: <span class="s2">"default"</span>,
+      <span class="s2">"pod"</span>: <span class="o">{}</span>
+    <span class="o">}</span>
+  <span class="o">]</span>,
   <span class="s2">"volumes"</span>: <span class="o">[</span>
     <span class="o">{</span>
       <span class="s2">"containerDisk"</span>: <span class="o">{</span>
-        <span class="s2">"image"</span>: <span class="s2">"quay.io/containerdisks/fedora:latest"</span>
+        <span class="s2">"image"</span>: <span class="s2">"quay.io/containerdisks/fedora:latest"</span>,
+        <span class="s2">"imagePullPolicy"</span>: <span class="s2">"Always"</span>
       <span class="o">}</span>,
       <span class="s2">"name"</span>: <span class="s2">"containerdisk"</span>
     <span class="o">}</span>,
</code></pre></div></div>

<h2 id="future-work">Future work</h2>

<p>There’s still plenty of work required before the API and CRDs can move from their current <code class="language-plaintext highlighter-rouge">alpha</code> version to <code class="language-plaintext highlighter-rouge">beta</code>. We have a specific <a href="https://github.com/kubevirt/kubevirt/issues/8235"><code class="language-plaintext highlighter-rouge">kubevirt/kubevirt</code> issue tracking our progress to <code class="language-plaintext highlighter-rouge">beta</code></a>. As set out there and in the <a href="https://github.com/kubevirt/community/blob/main/docs/api-graduation-guidelines.md">KubeVirt community API Graduation Phase Expecations</a>, part of this work is to seek feedback from the wider community so please do feel free to chime in there with any and all feedback on the API and CRDs.</p>

<p>You can also track our work on this API through the <a href="https://github.com/kubevirt/kubevirt/labels/area%2Finstancetype"><code class="language-plaintext highlighter-rouge">area/instancetype</code> tag</a> or my <a href="https://blog.yarwood.me.uk/tags/instancetypes/">personal blog</a> where I will be posting <a href="https://blog.yarwood.me.uk/2022/07/21/kubevirt_instancetype_update_2/">regular updates</a> and <a href="https://blog.yarwood.me.uk/2022/08/03/kubevirt_instancetype_demo_2/">demos</a> for instancetypes.</p>]]></content><author><name>Lee Yarwood</name></author><category term="news" /><category term="kubevirt" /><category term="kubernetes" /><category term="virtual machine" /><category term="VM" /><category term="instancetypes" /><category term="preferences" /><category term="VirtualMachine" /><category term="VirtualMachineInstancetype" /><category term="VirtualMachinePreference" /><summary type="html"><![CDATA[An introduction to Instancetypes and preferences in KubeVirt]]></summary></entry><entry><title type="html">KubeVirt: installing Microsoft Windows 11 from an ISO</title><link href="https://kubevirt.io//2022/KubeVirt-installing_Microsoft_Windows_11_from_an_iso.html" rel="alternate" type="text/html" title="KubeVirt: installing Microsoft Windows 11 from an ISO" /><published>2022-08-02T00:00:00+00:00</published><updated>2022-08-02T00:00:00+00:00</updated><id>https://kubevirt.io//2022/KubeVirt-installing_Microsoft_Windows_11_from_an_iso</id><content type="html" xml:base="https://kubevirt.io//2022/KubeVirt-installing_Microsoft_Windows_11_from_an_iso.html"><![CDATA[<p>This blog post describes a simple way to deploy a Windows 11 VM with KubeVirt, using an installation ISO as a starting point.<br />
Although only tested with Windows 11, the steps described here should also work to deploy other recent versions of Windows.</p>

<h2 id="pre-requisites">Pre-requisites</h2>

<ul>
  <li>You’ll need a Kubernetes cluster with worker node(s) that have at least 6GB of available memory</li>
  <li><a href="https://kubevirt.io/user-guide">KubeVirt</a> and <a href="https://github.com/kubevirt/containerized-data-importer/blob/main/README.md">CDI</a> both deployed on the cluster</li>
  <li>A storage backend, such as <a href="https://ceph.com/">Rook Ceph</a></li>
  <li>A Windows iso. One can be found at <a href="https://www.microsoft.com/software-download/windows11">https://www.microsoft.com/software-download/windows11</a></li>
</ul>

<p>A suitable test cluster can easily be deployed thanks to KubeVirtCI by running the following commands from the <a href="https://github.com/kubevirt/kubevirt">KubeVirt source repository</a>:</p>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">export </span><span class="nv">KUBEVIRT_MEMORY_SIZE</span><span class="o">=</span>8192M
<span class="nv">$ </span><span class="nb">export </span><span class="nv">KUBEVIRT_STORAGE</span><span class="o">=</span>rook-ceph-default
<span class="nv">$ </span>make cluster-up <span class="o">&amp;&amp;</span> make cluster-sync
</code></pre></div></div>

<h2 id="preparation">Preparation</h2>

<p>Before the virtual machine can be created, we need to setup storage volumes for the ISO and the drive, and write the appropriate VM(I) yaml.</p>

<ol>
  <li>
    <p>Uploading the ISO to a PVC</p>

    <p>KubeVirt provides a simple tool that is able to do that for us: <code class="language-plaintext highlighter-rouge">virtctl</code>.<br />
Here’s the command to upload the ISO, just replace <code class="language-plaintext highlighter-rouge">/storage/win11.iso</code> with the path to your Windows 11 ISO:
<code class="language-plaintext highlighter-rouge">virtctl image-upload pvc win11cd-pvc --size 6Gi --image-path=/storage/win11.iso --insecure</code></p>
  </li>
  <li>
    <p>Creating a persistent volume to use as the Windows drive</p>

    <p>This will depend on the storage configuration of your cluster.
The following yaml, to apply to the cluster using <code class="language-plaintext highlighter-rouge">kubectl create</code>, should work just fine on a KubeVirtCI cluster:</p>

    <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">PersistentVolume</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">task-pv-volume</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">type</span><span class="pi">:</span> <span class="s">local</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">storageClassName</span><span class="pi">:</span> <span class="s">hostpath</span>
  <span class="na">capacity</span><span class="pi">:</span>
    <span class="na">storage</span><span class="pi">:</span> <span class="s">15Gi</span>
  <span class="na">accessModes</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">ReadWriteOnce</span>
  <span class="na">hostPath</span><span class="pi">:</span>
    <span class="na">path</span><span class="pi">:</span> <span class="s2">"</span><span class="s">/tmp/hostImages/win11"</span>
</code></pre></div>    </div>
  </li>
</ol>

<div class="premonition note"><div class="fa fa-check-square"></div><div class="content"><p class="header">Note</p><p>Microsoft actually <a href="https://docs.microsoft.com/en-us/windows/whats-new/windows-11-requirements">recommends</a> at least 64GB of storage.
But, unlike some other requirements, the installer will accept smaller disks.
This is convenient when testing with KubeVirtCI, as nodes only have about 20GB of free space.
However, please bear in mind that such a small drive should only be used for testing purposes, and might lead to instabilities.</p>


</div></div>
<ol>
  <li>
    <p>Creating a persistent volume claim (PVC) for the drive</p>

    <p>Once again, your milage may vary, but the following PVC yaml works fine on KubeVirtCI:</p>

    <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">PersistentVolumeClaim</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">disk-windows</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">accessModes</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">ReadWriteOnce</span>
  <span class="na">resources</span><span class="pi">:</span>
    <span class="na">requests</span><span class="pi">:</span>
      <span class="na">storage</span><span class="pi">:</span> <span class="s">15Gi</span>
  <span class="na">storageClassName</span><span class="pi">:</span> <span class="s">hostpath</span>
</code></pre></div>    </div>

    <p>The name of PVC, <code class="language-plaintext highlighter-rouge">disk-windows</code> here, will be used in the yaml of the VM(I) as the main volume.</p>
  </li>
  <li>
    <p>Creating the VM(I) yaml file</p>

    <p>KubeVirt already includes an example <a href="https://github.com/kubevirt/kubevirt/blob/main/examples/vmi-windows.yaml">Windows VMI yaml file</a>, which we’ll use as a starting point here for convenience.<br />
Using a VMI yaml is more than enough for testing purposes, however for more serious applications you might want to consider changing it into a VM.</p>

    <p>First, in the yaml above, bump the memory up to 4Gi, which is a hard requirement of Windows 11. (Windows 10 is happy with 2Gi).</p>

    <p>Then, let’s add the ISO created above.
Add is as a cdrom in the disks section:</p>
    <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="pi">-</span> <span class="na">cdrom</span><span class="pi">:</span>
    <span class="na">bus</span><span class="pi">:</span> <span class="s">sata</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">winiso</span>
</code></pre></div>    </div>
    <p>And the corresponding volume at the bottom:</p>
    <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">winiso</span>
    <span class="na">persistentVolumeClaim</span><span class="pi">:</span>
      <span class="na">claimName</span><span class="pi">:</span> <span class="s">win11cd-pvc</span>
</code></pre></div>    </div>
    <p>Note that the names should match, and that the <code class="language-plaintext highlighter-rouge">claimName</code> is what we used in the <code class="language-plaintext highlighter-rouge">virtctl</code> command above.</p>

    <p>Here is what the VMI looks like after those changes:</p>
    <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">kubevirt.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachineInstance</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">special</span><span class="pi">:</span> <span class="s">vmi-windows</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">vmi-windows</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">domain</span><span class="pi">:</span>
    <span class="na">clock</span><span class="pi">:</span>
      <span class="na">timer</span><span class="pi">:</span>
        <span class="na">hpet</span><span class="pi">:</span>
          <span class="na">present</span><span class="pi">:</span> <span class="no">false</span>
        <span class="na">hyperv</span><span class="pi">:</span> <span class="pi">{}</span>
        <span class="na">pit</span><span class="pi">:</span>
          <span class="na">tickPolicy</span><span class="pi">:</span> <span class="s">delay</span>
        <span class="na">rtc</span><span class="pi">:</span>
          <span class="na">tickPolicy</span><span class="pi">:</span> <span class="s">catchup</span>
      <span class="na">utc</span><span class="pi">:</span> <span class="pi">{}</span>
    <span class="na">cpu</span><span class="pi">:</span>
      <span class="na">cores</span><span class="pi">:</span> <span class="m">2</span>
    <span class="na">devices</span><span class="pi">:</span>
      <span class="na">disks</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">disk</span><span class="pi">:</span>
          <span class="na">bus</span><span class="pi">:</span> <span class="s">sata</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">pvcdisk</span>
      <span class="pi">-</span> <span class="na">cdrom</span><span class="pi">:</span>
          <span class="na">bus</span><span class="pi">:</span> <span class="s">sata</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">winiso</span>
      <span class="na">interfaces</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">masquerade</span><span class="pi">:</span> <span class="pi">{}</span>
        <span class="na">model</span><span class="pi">:</span> <span class="s">e1000</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">default</span>
      <span class="na">tpm</span><span class="pi">:</span> <span class="pi">{}</span>
    <span class="na">features</span><span class="pi">:</span>
      <span class="na">acpi</span><span class="pi">:</span> <span class="pi">{}</span>
      <span class="na">apic</span><span class="pi">:</span> <span class="pi">{}</span>
      <span class="na">hyperv</span><span class="pi">:</span>
        <span class="na">relaxed</span><span class="pi">:</span> <span class="pi">{}</span>
        <span class="na">spinlocks</span><span class="pi">:</span>
          <span class="na">spinlocks</span><span class="pi">:</span> <span class="m">8191</span>
        <span class="na">vapic</span><span class="pi">:</span> <span class="pi">{}</span>
      <span class="na">smm</span><span class="pi">:</span> <span class="pi">{}</span>
    <span class="na">firmware</span><span class="pi">:</span>
      <span class="na">bootloader</span><span class="pi">:</span>
        <span class="na">efi</span><span class="pi">:</span>
          <span class="na">secureBoot</span><span class="pi">:</span> <span class="no">true</span>
      <span class="na">uuid</span><span class="pi">:</span> <span class="s">5d307ca9-b3ef-428c-8861-06e72d69f223</span>
    <span class="na">resources</span><span class="pi">:</span>
      <span class="na">requests</span><span class="pi">:</span>
        <span class="na">memory</span><span class="pi">:</span> <span class="s">4Gi</span>
  <span class="na">networks</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">default</span>
    <span class="na">pod</span><span class="pi">:</span> <span class="pi">{}</span>
  <span class="na">terminationGracePeriodSeconds</span><span class="pi">:</span> <span class="m">0</span>
  <span class="na">volumes</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">pvcdisk</span>
    <span class="na">persistentVolumeClaim</span><span class="pi">:</span>
      <span class="na">claimName</span><span class="pi">:</span> <span class="s">disk-windows</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">winiso</span>
    <span class="na">persistentVolumeClaim</span><span class="pi">:</span>
      <span class="na">claimName</span><span class="pi">:</span> <span class="s">win11cd-pvc</span>
</code></pre></div>    </div>
  </li>
</ol>

<div class="premonition note"><div class="fa fa-check-square"></div><div class="content"><p class="header">Note</p><p>When customizing this VMI definition or creating your own, please keep in mind that the TPM device and the UEFI firmware with SecureBoot are both hard requirements of Windows 11.
Not having them will cause the Windows 11 installation to fail early. Please also note that the SMM CPU feature is required for UEFI + SecureBoot.
However, they can all be omitted in the case of a Windows 10 VM(I).
Finally, we do not currently support TPM persistence, so any secret stored in the emulated TPM will be lost next time you boot the VMI.
For example, do not enable BitLocker, as it will fail to find the encryption key next boot and you will have to manually enter the (55 characters!) recovery key each boot.</p>


</div></div>
<h2 id="windows-installation">Windows installation</h2>

<p>You should now be able to create the VMI and start the Windows installation process.<br />
Just use kubectl to start the VMI created above: <code class="language-plaintext highlighter-rouge">kubectl create -f vmi-windows.yaml</code>.<br />
Shortly after, open a VNC session to it using <code class="language-plaintext highlighter-rouge">virtctl vnc vmi-windows</code> (keep trying until the VMI is running and the VNC session pops up).<br />
You should now see the boot screen, and shortly after a prompt to “Press any key to boot from CD or DVD…”. You have a few seconds to do so or the VM will fail to boot.
Then just follow the steps to install Windows.</p>

<h2 id="virtio-drivers-installation-optional">VirtIO drivers installation (optional)</h2>

<p>Once Windows is installed, it’s a good ideas to install the <a href="http://www.linux-kvm.org/page/Virtio">VirtIO</a> drivers inside the VM, as they can drastically improve performance.
The latest version can be downloaded <a href="https://fedorapeople.org/groups/virt/virtio-win/direct-downloads/latest-virtio/">here</a>.
<code class="language-plaintext highlighter-rouge">virtio-win-gt-x64.msi</code> is the simplest package to install, as you just have to run it as Administrator.</p>

<p>Alternatively, KubeVirt has a containerdisk image that can be mounted inside the VM.<br />
To use it, just add a simple cdrom disk to the VMI, like:</p>
<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="pi">-</span> <span class="na">cdrom</span><span class="pi">:</span>
    <span class="na">bus</span><span class="pi">:</span> <span class="s">sata</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">virtio</span>
</code></pre></div></div>
<p>and the volume:</p>
<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="pi">-</span> <span class="na">containerDisk</span><span class="pi">:</span>
      <span class="na">image</span><span class="pi">:</span> <span class="s">kubevirt/virtio-container-disk</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">virtio</span>
</code></pre></div></div>
<p>When using KubeVirtCI, a local copy of the image is also available at <code class="language-plaintext highlighter-rouge">registry:5000/kubevirt/virtio-container-disk:devel</code>.</p>

<h2 id="further-performance-improvements">Further performance improvements</h2>

<p>Windows is quite resource-hungry, and you might find that the VM created above is too slow, even with the VirtIO drivers installed.<br />
Here are a few steps you can take to improve things:</p>
<ul>
  <li>Increasing the RAM is always a good idea, if you have enough available of course.</li>
  <li>Increasing the number of CPUs, and/or using CPUManager to assign dedicated CPU to the VM should also help a lot.</li>
  <li>Once the VirtIO drivers are installed, the main drive can also be switched from <code class="language-plaintext highlighter-rouge">sata</code> to <code class="language-plaintext highlighter-rouge">virtio</code>, and the attached CDROMs can be removed.</li>
</ul>]]></content><author><name>Jed Lejosne</name></author><category term="news" /><category term="kubevirt" /><category term="kubernetes" /><category term="virtual machine" /><category term="Microsoft Windows kubernetes" /><category term="Microsoft Windows container" /><category term="Windows" /><summary type="html"><![CDATA[This blog post describes how to create a Microsoft Windows 11 virtual machine with KubeVirt]]></summary></entry><entry><title type="html">KubeVirt at KubeCon EU 2022</title><link href="https://kubevirt.io//2022/KubeVirt-at-KubeCon-EU-2022.html" rel="alternate" type="text/html" title="KubeVirt at KubeCon EU 2022" /><published>2022-06-28T00:00:00+00:00</published><updated>2022-06-28T00:00:00+00:00</updated><id>https://kubevirt.io//2022/KubeVirt-at-KubeCon-EU-2022</id><content type="html" xml:base="https://kubevirt.io//2022/KubeVirt-at-KubeCon-EU-2022.html"><![CDATA[<p>KubeCon EU was in Valencia, Spain this year from May 16-20. For many of the 7000+ physical attendees, it was their first in-person conference in several years. With luck, it was the first of many more to come, as KubeCon is a rare opportunity to learn about, from, and with a rich variety of adopters, communities, and vendors that make up the open source and cloud native ecosystem.</p>

<p>The KubeVirt community presented two sessions, both on Wednesday May 18th:</p>

<ol>
  <li>A Virtual Open Office Hours session, and</li>
  <li>A Maintainer Track session: ‘It’s All for the Users. More Durable, Secure, and Pluggable. KubeVirt v0.53’</li>
</ol>

<h2 id="virtual-open-office-hours">Virtual Open Office Hours</h2>

<p>This was a 45-minute project virtual session, hosted by the CNCF. This was on the Bevy platform (which will be familiar to KubeVirt Summit attendees from the past two years) and we had five lovely people from the KubeVirt community ready with a variety of demos and presentations and to answer questions from attendees:+
Alice Frosi, Itamar Holder, Miguel Duarte de Mora Barroso, Luboslav Pivarc, and Bartosz Rybacki</p>

<p>This was an opportunity for KubeCon attendees (virtual and physical) to ask questions and discuss any topics, and our presenters covered the following: Introduction to KubeVirt, live migration, Istio integration, and CDI hotplug/resize.
Despite some initial technical issues and improvised changes, this session went really well. We had about ~25 consistent attendees, and we received a good range of Q&amp;A and interaction with the attendees on all topics presented. It was a very solid 45 minutes.
Unfortunately, due to a miscommunication, there is no recording of this session.</p>

<p>A huge thanks to the presenters for their time and collaboration in preparing for this.</p>

<h2 id="maintainer-track">Maintainer Track</h2>

<p>Later that day, on the Maintainer Track, Alice also gave an in-depth breakdown of a whole slew of new KubeVirt features and showed a demo with the KubeVirt Cluster API: deploying Kubernetes on top of Kubernetes.
You can watch <a href="https://youtu.be/L9H0pz5PpKo">the CNCF recording here</a>, and download the <a href="https://kccnceu2022.sched.com/event/ytu1">demo video and slides</a> that are available from the schedule.</p>

<p>There was a healthy amount of questions, both during Q&amp;A and after the talk. The participants were particularly interested to know how to prepare and customize VM disks with KubeVirt, how to run Windows VM, especially combined with GPUs, and how to expose the Kubernetes API service of a deployed cluster to the KubeVirt cluster API provider outside of the KubeVirt VM. There were additional questions on the status of TPM support and VM migration when the hosting node goes down.</p>

<h2 id="thank-you">Thank you!</h2>

<p>Big thanks again to our presenters: Alice Frosi, Itamar Holder, Miguel Duarte de Mora Barroso, Luboslav Pivarc, and Bartosz Rybacki.
And everyone who attended the sessions, listened, and asked great questions.</p>

<h2 id="want-to-see-more-from-kubecon-eu-2022">Want to see more from KubeCon EU 2022?</h2>

<p>If you’re interested in seeing more photos and recordings from the event:</p>

<ul>
  <li><a href="https://www.flickr.com/photos/143247548@N03/albums/72177720298987342">CNCF’s Photo album (Flickr) of the event</a>.</li>
  <li><a href="https://www.youtube.com/c/cloudnativefdn">The CNCF video recordings of the sessions on Youtube</a>.</li>
  <li>And <a href="https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/program/schedule/">the event schedule</a> to help you find sessions.</li>
</ul>]]></content><author><name>Andrew Burden</name></author><category term="news" /><category term="kubevirt" /><category term="event" /><category term="community" /><category term="KubeCon" /><summary type="html"><![CDATA[A short report on the two sessions KubeVirt presented at KubeCon EU 2022]]></summary></entry><entry><title type="html">Load-balancer for virtual machines on bare metal Kubernetes clusters</title><link href="https://kubevirt.io//2022/Virtual-Machines-with-MetalLB.html" rel="alternate" type="text/html" title="Load-balancer for virtual machines on bare metal Kubernetes clusters" /><published>2022-04-03T00:00:00+00:00</published><updated>2022-04-03T00:00:00+00:00</updated><id>https://kubevirt.io//2022/Virtual-Machines-with-MetalLB</id><content type="html" xml:base="https://kubevirt.io//2022/Virtual-Machines-with-MetalLB.html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>Over the last year, Kubevirt and MetalLB have shown to be powerful duo in order to support fault-tolerant access to an application on virtual machines through an external IP address. 
As a Cluster administrator using an on-prem cluster without a network load-balancer, now it’s possible to use MetalLB operator to provide load-balancer capabilities (with Services of type <code class="language-plaintext highlighter-rouge">LoadBalancer</code>) to virtual machines.</p>

<h2 id="metallb">MetalLB</h2>

<p><a href="https://metallb.universe.tf/">MetalLB</a> allows you to create Kubernetes services of type <code class="language-plaintext highlighter-rouge">LoadBalancer</code>, and provides network load-balancer implementation in on-prem clusters that don’t run on a cloud provider.
MetalLB is responsible for assigning/unassigning an external IP Address to your service, using IPs from pre-configured pools. In order for the external IPs to be announced externally, MetalLB works in 2 modes, Layer 2 and BGP:</p>

<ul>
  <li>
    <p>Layer 2 mode (ARP/NDP):</p>

    <p>This mode - which actually does not implement real load-balancing behavior - provides a failover mechanism where a single node owns the <code class="language-plaintext highlighter-rouge">LoadBalancer</code> service, until it fails, triggering another node to be chosen as the service owner. This configuration mode makes the IPs reachable from the local network.<br />
In this method, the MetalLB speaker pod announces the IPs in ARP (for IPv4) and NDP (for IPv6) protocols over the host network. From a network perspective, the node owning the service appears to have multiple IP addresses assigned to a network interface. After traffic is routed to the node, the service proxy sends the traffic to the application pods.</p>
  </li>
  <li>
    <p>BGP mode:</p>

    <p>This mode provides real load-balancing behavior, by establishing BGP peering sessions with the network routers - which advertise the external IPs of the <code class="language-plaintext highlighter-rouge">LoadBalancer</code> service, distributing the load over the nodes.</p>
  </li>
</ul>

<p>To read more on MetalLB concepts, implementation and limitations, please read <a href="https://metallb.universe.tf/concepts/">its documentation</a>.</p>

<h2 id="demo-virtual-machine-with-external-ip-and-metallb-load-balancer">Demo: Virtual machine with external IP and MetalLB load-balancer</h2>

<p>With the following recipe we will end up with a nginx server running on a virtual machine, accessible outside the cluster using MetalLB load-balancer with Layer 2 mode.</p>

<h3 id="demo-environment-setup">Demo environment setup</h3>

<p>We are going to use <a href="https://kind.sigs.k8s.io">kind</a> provider as an ephemeral Kubernetes cluster.</p>

<p>Prerequirements:</p>
<ul>
  <li>First install kind on your machine following its <a href="https://kind.sigs.k8s.io/docs/user/quick-start/#installation">installation guide</a>.</li>
  <li>To use kind, you will also need to <a href="https://docs.docker.com/install/">install docker</a>.</li>
</ul>

<h4 id="external-ips-on-macos-and-windows">External IPs on macOS and Windows</h4>

<p>This demo runs Docker on Linux, which allows sending traffic directly to the load-balancer’s external IP if the IP space is within the docker IP space.
On macOS and Windows however, docker does not expose the docker network to the host, rendering the external IP unreachable from other kind nodes. In order to workaround this, one could expose pods and services using extra port mappings as shown in the extra port mappings section of kind’s <a href="https://kind.sigs.k8s.io/docs/user/configuration#extra-port-mappings">Configuration Guide</a>.</p>

<h3 id="deploying-cluster">Deploying cluster</h3>

<p>To start a kind cluster:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kind create cluster
</code></pre></div></div>

<p>In order to interact with the specific cluster created:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl cluster-info <span class="nt">--context</span> kind-kind
</code></pre></div></div>

<h3 id="installing-components">Installing components</h3>

<h4 id="installing-metallb-on-the-cluster">Installing MetalLB on the cluster</h4>

<p>There are <a href="https://metallb.universe.tf/installation/">many ways</a> to install MetalLB. For the sake of this example, we will install MetalLB via manifests. To do this, follow this <a href="https://metallb.universe.tf/installation/#installation-by-manifest">guide</a>. 
Confirm successful installation by waiting for MetalLB pods to have a status of Running:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl get pods <span class="nt">-n</span> metallb-system <span class="nt">--watch</span>
</code></pre></div></div>

<h4 id="installing-kubevirt-on-the-cluster">Installing Kubevirt on the cluster</h4>

<p>Following Kubevirt <a href="https://kubevirt.io/user-guide/operations/installation/#installing-kubevirt-on-kubernetes">user guide</a> to install released version v0.51.0</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">export </span><span class="nv">RELEASE</span><span class="o">=</span>v0.51.0
kubectl apply <span class="nt">-f</span> <span class="s2">"https://github.com/kubevirt/kubevirt/releases/download/</span><span class="k">${</span><span class="nv">RELEASE</span><span class="k">}</span><span class="s2">/kubevirt-operator.yaml"</span>
kubectl apply <span class="nt">-f</span> <span class="s2">"https://github.com/kubevirt/kubevirt/releases/download/</span><span class="k">${</span><span class="nv">RELEASE</span><span class="k">}</span><span class="s2">/kubevirt-cr.yaml"</span>
kubectl <span class="nt">-n</span> kubevirt <span class="nb">wait </span>kv kubevirt <span class="nt">--timeout</span><span class="o">=</span>360s <span class="nt">--for</span> <span class="nv">condition</span><span class="o">=</span>Available
</code></pre></div></div>

<p>Now we have a Kubernetes cluster with all the pieces to start the Demo.</p>

<h3 id="network-resources-configuration">Network resources configuration</h3>

<h4 id="setting-address-pool-to-be-used-by-the-loadbalancer">Setting Address Pool to be used by the LoadBalancer</h4>

<p>In order to complete the Layer 2 mode configuration, we need to set a range of IP addresses for the LoadBalancer to use.
On Linux we can use the docker kind network (macOS and Windows users see <a href="#external-ips-on-macos-and-windows">External IPs Prerequirement</a>), so by using this command:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker network inspect <span class="nt">-f</span> <span class="s1">''</span> kind
</code></pre></div></div>

<p>You should get the subclass you can set the IP range from. The output should contain a cidr such as 172.18.0.0/16.
Using this result we will create the following Layer 2 address pool with 172.18.1.1-172.18.1.16 range:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">cat &lt;&lt;EOF | kubectl apply -f -</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">ConfigMap</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">metallb-system</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">config</span>
<span class="na">data</span><span class="pi">:</span>
  <span class="na">config</span><span class="pi">:</span> <span class="pi">|</span>
    <span class="s">address-pools:</span>
    <span class="s">- name: addresspool-sample1</span>
      <span class="s">protocol: layer2</span>
      <span class="s">addresses:</span>
      <span class="s">- 172.18.1.1-172.18.1.16</span>
<span class="s">EOF</span>
</code></pre></div></div>

<h3 id="network-utilization">Network utilization</h3>

<h4 id="spin-up-a-virtual-machine-running-nginx">Spin up a Virtual Machine running Nginx</h4>

<p>Now it’s time to start-up a virtual machine running nginx using the following yaml.
The virtual machine has a <code class="language-plaintext highlighter-rouge">metallb-service=nginx</code> we created to use when creating the service.</p>
<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">cat &lt;&lt;EOF | kubectl apply -f -</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">kubevirt.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachine</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">fedora-nginx</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">default</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">metallb-service</span><span class="pi">:</span> <span class="s">nginx</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">running</span><span class="pi">:</span> <span class="no">true</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">metallb-service</span><span class="pi">:</span> <span class="s">nginx</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">domain</span><span class="pi">:</span>
        <span class="na">devices</span><span class="pi">:</span>
          <span class="na">disks</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">disk</span><span class="pi">:</span>
                <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
              <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
            <span class="pi">-</span> <span class="na">disk</span><span class="pi">:</span>
                <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
              <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
          <span class="na">interfaces</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">masquerade</span><span class="pi">:</span> <span class="pi">{}</span>
              <span class="na">name</span><span class="pi">:</span> <span class="s">default</span>
        <span class="na">resources</span><span class="pi">:</span>
          <span class="na">requests</span><span class="pi">:</span>
            <span class="na">memory</span><span class="pi">:</span> <span class="s">1024M</span>
      <span class="na">networks</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">default</span>
          <span class="na">pod</span><span class="pi">:</span> <span class="pi">{}</span>
      <span class="na">terminationGracePeriodSeconds</span><span class="pi">:</span> <span class="m">0</span>
      <span class="na">volumes</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">containerDisk</span><span class="pi">:</span>
            <span class="na">image</span><span class="pi">:</span> <span class="s">kubevirt/fedora-cloud-container-disk-demo</span>
          <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
        <span class="pi">-</span> <span class="na">cloudInitNoCloud</span><span class="pi">:</span>
            <span class="na">userData</span><span class="pi">:</span> <span class="pi">|-</span>
              <span class="s">#cloud-config</span>
              <span class="s">password: fedora</span>
              <span class="s">chpasswd: { expire: False }</span>
              <span class="s">packages:</span>
                <span class="s">- nginx</span>
              <span class="s">runcmd:</span>
                <span class="s">- [ "systemctl", "enable", "--now", "nginx" ]</span>
          <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
<span class="s">EOF</span>
</code></pre></div></div>

<h4 id="expose-the-virtual-machine-with-a-typed-loadbalancer-service">Expose the virtual machine with a typed <code class="language-plaintext highlighter-rouge">LoadBalancer</code> service</h4>

<p>When creating the <code class="language-plaintext highlighter-rouge">LoadBalancer</code> typed service, we need to remember annotating the address-pool we want to use 
<code class="language-plaintext highlighter-rouge">addresspool-sample1</code> and also add the selector <code class="language-plaintext highlighter-rouge">metallb-service: nginx</code>:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">cat &lt;&lt;EOF | kubectl apply -f -</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Service</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">metallb-nginx-svc</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">default</span>
  <span class="na">annotations</span><span class="pi">:</span>
    <span class="na">metallb.universe.tf/address-pool</span><span class="pi">:</span> <span class="s">addresspool-sample1</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">externalTrafficPolicy</span><span class="pi">:</span> <span class="s">Local</span>
  <span class="na">ipFamilies</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">IPv4</span>
  <span class="na">ports</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">tcp-5678</span>
      <span class="na">protocol</span><span class="pi">:</span> <span class="s">TCP</span>
      <span class="na">port</span><span class="pi">:</span> <span class="m">5678</span>
      <span class="na">targetPort</span><span class="pi">:</span> <span class="m">80</span>
  <span class="na">type</span><span class="pi">:</span> <span class="s">LoadBalancer</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">metallb-service</span><span class="pi">:</span> <span class="s">nginx</span>
<span class="s">EOF</span>
</code></pre></div></div>

<p>Notice that the service got assigned with an external IP from the range assigned by the address pool:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl get service <span class="nt">-n</span> default metallb-nginx-svc
</code></pre></div></div>

<p>Example output:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>NAME                TYPE           CLUSTER-IP      EXTERNAL-IP   PORT<span class="o">(</span>S<span class="o">)</span>          AGE
metallb-nginx-svc   LoadBalancer   10.96.254.136   172.18.1.1    5678:32438/TCP   53s
</code></pre></div></div>

<h4 id="access-the-virtual-machine-from-outside-the-cluster">Access the virtual machine from outside the cluster</h4>

<p>Finally, we can check that the nginx server is accessible from outside the cluster:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl <span class="nt">-s</span> <span class="nt">-o</span> /dev/null 172.18.1.1:5678 <span class="o">&amp;&amp;</span> <span class="nb">echo</span> <span class="s2">"URL exists"</span>
</code></pre></div></div>

<p>Example output:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>URL exists
</code></pre></div></div>
<p>Note that it may take a short while for the URL to work after setting the service.</p>

<h2 id="doing-this-on-your-own-cluster">Doing this on your own cluster</h2>

<p>Moving outside the demo example, one who would like use MetalLB on their real life cluster, should also take other considerations in mind:</p>
<ul>
  <li>User privileges: you should have <code class="language-plaintext highlighter-rouge">cluster-admin</code> privileges on the cluster - in order to install MetalLB.</li>
  <li>IP Ranges for MetalLB: getting IP Address pools allocation for MetalLB depends on your cluster environment:
    <ul>
      <li>If you’re running a bare-metal cluster in a shared host environment, you need to first reserve this IP Address pool from your hosting provider.</li>
      <li>Alternatively, if you’re running on a private cluster, you can use one of the private IP Address spaces (a.k.a RFC1918 addresses). Such addresses are free, and work fine as long as you’re only providing cluster services to your LAN.</li>
    </ul>
  </li>
</ul>

<h2 id="conclusion">Conclusion</h2>

<p>In this blog post we used MetalLB to expose a service using an external IP assigned to a virtual machine. 
This illustrates how virtual machine traffic can be load-balanced via a service.</p>]]></content><author><name>Ram Lavi</name></author><category term="news" /><category term="Kubevirt" /><category term="kubernetes" /><category term="virtual machine" /><category term="VM" /><category term="load-balancer" /><category term="MetalLB" /><summary type="html"><![CDATA[This post illustrates setting up a virtual machine with MetalLB LoadBalancer service.]]></summary></entry></feed>