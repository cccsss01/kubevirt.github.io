<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="https://kubevirt.io//feed.xml" rel="self" type="application/atom+xml" /><link href="https://kubevirt.io//" rel="alternate" type="text/html" /><updated>2023-06-27T14:36:52+00:00</updated><id>https://kubevirt.io//feed.xml</id><title type="html">KubeVirt.io</title><subtitle>Virtual Machine Management on Kubernetes</subtitle><entry><title type="html">Secondary networks connected to the physical underlay for KubeVirt VMs using OVN-Kubernetes</title><link href="https://kubevirt.io//2023/OVN-kubernetes-secondary-networks-localnet.html" rel="alternate" type="text/html" title="Secondary networks connected to the physical underlay for KubeVirt VMs using OVN-Kubernetes" /><published>2023-05-31T00:00:00+00:00</published><updated>2023-05-31T00:00:00+00:00</updated><id>https://kubevirt.io//2023/OVN-kubernetes-secondary-networks-localnet</id><content type="html" xml:base="https://kubevirt.io//2023/OVN-kubernetes-secondary-networks-localnet.html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>OVN (Open Virtual Network) is a series of daemons for the Open vSwitch that
translate virtual network configurations into OpenFlow. It provides virtual
networking capabilities for any type of workload on a virtualized platform
(virtual machines and containers) using the same API.</p>

<p>OVN provides a higher-layer of abstraction than Open vSwitch, working with
logical routers and logical switches, rather than flows.
More details can be found in the OVN architecture
<a href="https://man7.org/linux/man-pages/man7/ovn-architecture.7.html#DESCRIPTION">man page</a>.</p>

<p>In this post we will repeat the scenario of
<a href="https://kubevirt.io/2020/Multiple-Network-Attachments-with-bridge-CNI.html">its bridge CNI equivalent</a>,
using this SDN approach. This secondary network topology is akin to the one
described in the <a href="http://kubevirt.io/2023/OVN-kubernetes-secondary-networks.html">flatL2 topology</a>,
but allows connectivity to the physical underlay.</p>

<h2 id="demo">Demo</h2>
<p>To run this demo, we will prepare a Kubernetes cluster with the following
components installed:</p>
<ul>
  <li><a href="https://github.com/ovn-org/ovn-kubernetes">OVN-Kubernetes</a></li>
  <li><a href="https://github.com/k8snetworkplumbingwg/multus-cni">multus-cni</a></li>
  <li><a href="https://github.com/kubevirt/kubevirt">KubeVirt</a></li>
</ul>

<p>The <a href="#environment-setup">following section</a> will show you how to create a
<a href="https://kind.sigs.k8s.io/">KinD</a> cluster, with upstream latest OVN-Kubernetes,
and upstream latest multus-cni deployed.</p>

<h3 id="setup-demo-environment">Setup demo environment</h3>
<p>Refer to the OVN-Kubernetes repo
<a href="https://github.com/ovn-org/ovn-kubernetes/blob/master/docs/kind.md#ovn-kubernetes-kind-setup">KIND documentation</a>
for more details; the gist of it is you should clone the OVN-Kubernetes
repository, and run their kind helper script:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone git@github.com:ovn-org/ovn-kubernetes.git

<span class="nb">cd </span>ovn-kubernetes
<span class="nb">pushd </span>contrib <span class="p">;</span> ./kind.sh <span class="nt">--multi-network-enable</span> <span class="p">;</span> <span class="nb">popd</span>
</code></pre></div></div>

<p>This will get you a running kind cluster, configured to use OVN-Kubernetes as
the default cluster network, configuring the multi-homing OVN-Kubernetes feature
gate, and deploying
<a href="https://github.com/k8snetworkplumbingwg/multus-cni">multus-cni</a> in the cluster.</p>

<h3 id="install-kubevirt-in-the-cluster">Install KubeVirt in the cluster</h3>
<p>Follow Kubevirt’s
<a href="https://kubevirt.io/user-guide/operations/installation/#installing-kubevirt-on-kubernetes">user guide</a>
to install the latest released version (currently, v0.59.0).</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">export </span><span class="nv">RELEASE</span><span class="o">=</span><span class="si">$(</span>curl https://storage.googleapis.com/kubevirt-prow/release/kubevirt/kubevirt/stable.txt<span class="si">)</span>
kubectl apply <span class="nt">-f</span> <span class="s2">"https://github.com/kubevirt/kubevirt/releases/download/</span><span class="k">${</span><span class="nv">RELEASE</span><span class="k">}</span><span class="s2">/kubevirt-operator.yaml"</span>
kubectl apply <span class="nt">-f</span> <span class="s2">"https://github.com/kubevirt/kubevirt/releases/download/</span><span class="k">${</span><span class="nv">RELEASE</span><span class="k">}</span><span class="s2">/kubevirt-cr.yaml"</span>
kubectl <span class="nt">-n</span> kubevirt <span class="nb">wait </span>kv kubevirt <span class="nt">--timeout</span><span class="o">=</span>360s <span class="nt">--for</span> <span class="nv">condition</span><span class="o">=</span>Available
</code></pre></div></div>

<p>Now we have a Kubernetes cluster with all the pieces to start the Demo.</p>

<h3 id="single-broadcast-domain">Single broadcast domain</h3>
<p>In this scenario we will see how traffic from a single localnet network can be
connected to a physical network in the host using a dedicated bridge.</p>

<p>This scenario does not use any VLAN encapsulation, thus is simpler, since the
network admin does not need to provision any VLANs in advance.</p>

<h4 id="configuring-the-underlay">Configuring the underlay</h4>
<p>When you’ve started the KinD cluster with the <code class="language-plaintext highlighter-rouge">--multi-network-enable</code> flag an
additional OCI network was created, and attached to each of the KinD nodes.</p>

<p>But still, further steps may be required, depending on the desired L2
configuration.</p>

<p>Let’s first create a dedicated OVS bridge, and attach the aforementioned
virtualized network to it:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for </span>node <span class="k">in</span> <span class="si">$(</span>kubectl <span class="nt">-n</span> ovn-kubernetes get pods <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>ovs-node <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.items[*].metadata.name}"</span><span class="si">)</span>
<span class="k">do
	</span>kubectl <span class="nt">-n</span> ovn-kubernetes <span class="nb">exec</span> <span class="nt">-ti</span> <span class="nv">$node</span> <span class="nt">--</span> ovs-vsctl <span class="nt">--may-exist</span> add-br ovsbr1
	kubectl <span class="nt">-n</span> ovn-kubernetes <span class="nb">exec</span> <span class="nt">-ti</span> <span class="nv">$node</span> <span class="nt">--</span> ovs-vsctl <span class="nt">--may-exist</span> add-port ovsbr1 eth1
	kubectl <span class="nt">-n</span> ovn-kubernetes <span class="nb">exec</span> <span class="nt">-ti</span> <span class="nv">$node</span> <span class="nt">--</span> ovs-vsctl <span class="nb">set </span>open <span class="nb">.</span> external_ids:ovn-bridge-mappings<span class="o">=</span>physnet:breth0,localnet.network_br-localnet:ovsbr1
<span class="k">done</span>
</code></pre></div></div>

<p>The first two commands are self-evident: you create an OVS bridge, and attach
a port to it; the last one is not. In it, we’re using the
<a href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.0/html/networking_guide/bridge-mappings">OVN bridge mapping</a>
API to configure which OVS bridge must be used for each physical network.
It creates a patch port between the OVN integration bridge - <code class="language-plaintext highlighter-rouge">br-int</code> - and the
OVS bridge you tell it to, and traffic will be forwarded to/from it with the
help of a
<a href="https://man7.org/linux/man-pages/man5/ovn-nb.5.html#Logical_Switch_Port_TABLE">localnet port</a>.</p>

<p>You will also have to configure an IP address on the bridge for the
extra-network the kind script created. For that, you first need to identify the
bridge’s name:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">OCI_BIN</span><span class="o">=</span>podman | docker <span class="c"># choose your cup of tea.</span>
<span class="nv">$OCI_BIN</span> network inspect underlay <span class="nt">--format</span> <span class="s1">'{ .NetworkInterface }}'</span>
podman3

ip addr add 10.128.0.1/24 dev podman3
</code></pre></div></div>

<p>Let’s also use an IP in the same subnet as the network subnet (defined in the
NAD). This IP address must be excluded from the IPAM pool (also on the NAD),
otherwise the OVN-Kubernetes IPAM may assign it to a workload.</p>

<h4 id="defining-the-ovn-kubernetes-networks">Defining the OVN-Kubernetes networks</h4>
<p>Once the underlay is configured, we can now provision the attachment configuration:</p>
<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">k8s.cni.cncf.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">NetworkAttachmentDefinition</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">localnet-network</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">config</span><span class="pi">:</span> <span class="pi">|2</span>
    <span class="s">{</span>
            <span class="s">"cniVersion": "0.3.1",</span>
            <span class="s">"name": "localnet-network",</span>
            <span class="s">"type": "ovn-k8s-cni-overlay",</span>
            <span class="s">"topology": "localnet",</span>
            <span class="s">"subnets": "10.128.0.0/24",</span>
            <span class="s">"excludeSubnets": "10.128.0.1/32",</span>
            <span class="s">"netAttachDefName": "default/localnet-network"</span>
    <span class="s">}</span>
</code></pre></div></div>

<p>It is required to list the gateway IP in the <code class="language-plaintext highlighter-rouge">excludedSubnets</code> attribute, thus
preventing OVN-Kubernetes from assigning that IP address to the workloads.</p>

<h4 id="spin-up-the-vms">Spin up the VMs</h4>
<p>These four VMs (two VMs connected to each tenant network) can be used for the
single broadcast domain scenario (no VLANs).</p>
<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">kubevirt.io/v1alpha3</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachine</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">vm-server</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">running</span><span class="pi">:</span> <span class="no">true</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">nodeSelector</span><span class="pi">:</span>
        <span class="na">kubernetes.io/hostname</span><span class="pi">:</span> <span class="s">ovn-worker</span>
      <span class="na">domain</span><span class="pi">:</span>
        <span class="na">devices</span><span class="pi">:</span>
          <span class="na">disks</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
              <span class="na">disk</span><span class="pi">:</span>
                <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
              <span class="na">disk</span><span class="pi">:</span>
                <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
          <span class="na">interfaces</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">localnet</span>
            <span class="na">bridge</span><span class="pi">:</span> <span class="pi">{}</span>
        <span class="na">machine</span><span class="pi">:</span>
          <span class="na">type</span><span class="pi">:</span> <span class="s2">"</span><span class="s">"</span>
        <span class="na">resources</span><span class="pi">:</span>
          <span class="na">requests</span><span class="pi">:</span>
            <span class="na">memory</span><span class="pi">:</span> <span class="s">1024M</span>
      <span class="na">networks</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">localnet</span>
        <span class="na">multus</span><span class="pi">:</span>
          <span class="na">networkName</span><span class="pi">:</span> <span class="s">localnet-network</span>
      <span class="na">terminationGracePeriodSeconds</span><span class="pi">:</span> <span class="m">0</span>
      <span class="na">volumes</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
          <span class="na">containerDisk</span><span class="pi">:</span>
            <span class="na">image</span><span class="pi">:</span> <span class="s">quay.io/kubevirt/fedora-with-test-tooling-container-disk:devel</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
          <span class="na">cloudInitNoCloud</span><span class="pi">:</span>
            <span class="na">networkData</span><span class="pi">:</span> <span class="pi">|</span>
              <span class="s">version: 2</span>
              <span class="s">ethernets:</span>
                <span class="s">eth0:</span>
                  <span class="s">dhcp4: true</span>
            <span class="na">userData</span><span class="pi">:</span> <span class="pi">|-</span>
              <span class="s">#cloud-config</span>
              <span class="s">password: fedora</span>
              <span class="s">chpasswd: { expire: False }</span>
<span class="s">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">kubevirt.io/v1alpha3</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachine</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">vm-client</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">running</span><span class="pi">:</span> <span class="no">true</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">nodeSelector</span><span class="pi">:</span>
        <span class="na">kubernetes.io/hostname</span><span class="pi">:</span> <span class="s">ovn-worker2</span>
      <span class="na">domain</span><span class="pi">:</span>
        <span class="na">devices</span><span class="pi">:</span>
          <span class="na">disks</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
              <span class="na">disk</span><span class="pi">:</span>
                <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
              <span class="na">disk</span><span class="pi">:</span>
                <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
          <span class="na">interfaces</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">localnet</span>
            <span class="na">bridge</span><span class="pi">:</span> <span class="pi">{}</span>
        <span class="na">machine</span><span class="pi">:</span>
          <span class="na">type</span><span class="pi">:</span> <span class="s2">"</span><span class="s">"</span>
        <span class="na">resources</span><span class="pi">:</span>
          <span class="na">requests</span><span class="pi">:</span>
            <span class="na">memory</span><span class="pi">:</span> <span class="s">1024M</span>
      <span class="na">networks</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">localnet</span>
        <span class="na">multus</span><span class="pi">:</span>
          <span class="na">networkName</span><span class="pi">:</span> <span class="s">localnet-network</span>
      <span class="na">terminationGracePeriodSeconds</span><span class="pi">:</span> <span class="m">0</span>
      <span class="na">volumes</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
          <span class="na">containerDisk</span><span class="pi">:</span>
            <span class="na">image</span><span class="pi">:</span> <span class="s">quay.io/kubevirt/fedora-with-test-tooling-container-disk:devel</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
          <span class="na">cloudInitNoCloud</span><span class="pi">:</span>
            <span class="na">networkData</span><span class="pi">:</span> <span class="pi">|</span>
              <span class="s">version: 2</span>
              <span class="s">ethernets:</span>
                <span class="s">eth0:</span>
                  <span class="s">dhcp4: true</span>
            <span class="na">userData</span><span class="pi">:</span> <span class="pi">|-</span>
              <span class="s">#cloud-config</span>
              <span class="s">password: fedora</span>
              <span class="s">chpasswd: { expire: False }</span>
</code></pre></div></div>

<h4 id="test-east--west-communication">Test East / West communication</h4>
<p>You can check east/west connectivity between both <strong>red</strong> VMs via ICMP:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl get vmi vm-server <span class="nt">-ojsonpath</span><span class="o">=</span><span class="s2">"{ @.status.interfaces }"</span> | jq
<span class="o">[</span>
  <span class="o">{</span>
    <span class="s2">"infoSource"</span>: <span class="s2">"domain, guest-agent"</span>,
    <span class="s2">"interfaceName"</span>: <span class="s2">"eth0"</span>,
    <span class="s2">"ipAddress"</span>: <span class="s2">"10.128.0.2"</span>,
    <span class="s2">"ipAddresses"</span>: <span class="o">[</span>
      <span class="s2">"10.128.0.2"</span>,
      <span class="s2">"fe80::e83d:16ff:fe76:c1bd"</span>
    <span class="o">]</span>,
    <span class="s2">"mac"</span>: <span class="s2">"ea:3d:16:76:c1:bd"</span>,
    <span class="s2">"name"</span>: <span class="s2">"localnet"</span>,
    <span class="s2">"queueCount"</span>: 1
  <span class="o">}</span>
<span class="o">]</span>

<span class="nv">$ </span>virtctl console vm-client
Successfully connected to vm-client console. The escape sequence is ^]

<span class="o">[</span>fedora@vm-client ~]<span class="nv">$ </span>ping 192.168.123.20
PING 192.168.123.20 <span class="o">(</span>192.168.123.20<span class="o">)</span> 56<span class="o">(</span>84<span class="o">)</span> bytes of data.
64 bytes from 192.168.123.20: <span class="nv">icmp_seq</span><span class="o">=</span>1 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>0.534 ms
64 bytes from 192.168.123.20: <span class="nv">icmp_seq</span><span class="o">=</span>2 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>0.246 ms
64 bytes from 192.168.123.20: <span class="nv">icmp_seq</span><span class="o">=</span>3 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>0.178 ms
64 bytes from 192.168.123.20: <span class="nv">icmp_seq</span><span class="o">=</span>4 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>0.236 ms

<span class="nt">---</span> 192.168.123.20 ping statistics <span class="nt">---</span>
4 packets transmitted, 4 received, 0% packet loss, <span class="nb">time </span>3028ms
rtt min/avg/max/mdev <span class="o">=</span> 0.178/0.298/0.534/0.138 ms
</code></pre></div></div>

<h4 id="check-underlay-services">Check underlay services</h4>
<p>We can now start HTTP servers listening to the IPs attached on
the gateway:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python3 <span class="nt">-m</span> http.server <span class="nt">--bind</span> 10.128.0.1 9000
</code></pre></div></div>

<p>And finally curl this from your client:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>fedora@vm-client ~]<span class="nv">$ </span>curl <span class="nt">-v</span> 10.128.0.1:9000
<span class="k">*</span>   Trying 10.128.0.1:9000...
<span class="k">*</span> Connected to 10.128.0.1 <span class="o">(</span>10.128.0.1<span class="o">)</span> port 9000 <span class="o">(</span><span class="c">#0)</span>
<span class="o">&gt;</span> GET / HTTP/1.1
<span class="o">&gt;</span> Host: 10.128.0.1:9000
<span class="o">&gt;</span> User-Agent: curl/7.69.1
<span class="o">&gt;</span> Accept: <span class="k">*</span>/<span class="k">*</span>
<span class="o">&gt;</span> 
<span class="k">*</span> Mark bundle as not supporting multiuse
<span class="k">*</span> HTTP 1.0, assume close after body
&lt; HTTP/1.0 200 OK
&lt; Server: SimpleHTTP/0.6 Python/3.11.3
&lt; Date: Thu, 01 Jun 2023 16:05:09 GMT
&lt; Content-type: text/html<span class="p">;</span> <span class="nv">charset</span><span class="o">=</span>utf-8
&lt; Content-Length: 2923
...
</code></pre></div></div>

<h3 id="multiple-physical-networks-pointing-to-the-same-ovs-bridge">Multiple physical networks pointing to the same OVS bridge</h3>
<p>This example will feature 2 physical networks, each with a different VLAN,
both pointing at the same OVS bridge.</p>

<h4 id="configuring-the-underlay-1">Configuring the underlay</h4>
<p>Again, the first thing to do is create a dedicated OVS bridge, and attach the
aforementioned virtualized network to it, while defining it as a trunk port
for two broadcast domains, with tags 10 and 20.</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for </span>node <span class="k">in</span> <span class="si">$(</span>kubectl <span class="nt">-n</span> ovn-kubernetes get pods <span class="nt">-l</span> <span class="nv">app</span><span class="o">=</span>ovs-node <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.items[*].metadata.name}"</span><span class="si">)</span>
<span class="k">do
	</span>kubectl <span class="nt">-n</span> ovn-kubernetes <span class="nb">exec</span> <span class="nt">-ti</span> <span class="nv">$node</span> <span class="nt">--</span> ovs-vsctl <span class="nt">--may-exist</span> add-br ovsbr1
	kubectl <span class="nt">-n</span> ovn-kubernetes <span class="nb">exec</span> <span class="nt">-ti</span> <span class="nv">$node</span> <span class="nt">--</span> ovs-vsctl <span class="nt">--may-exist</span> add-port ovsbr1 eth1 <span class="nv">trunks</span><span class="o">=</span>10,20 <span class="nv">vlan_mode</span><span class="o">=</span>trunk
	kubectl <span class="nt">-n</span> ovn-kubernetes <span class="nb">exec</span> <span class="nt">-ti</span> <span class="nv">$node</span> <span class="nt">--</span> ovs-vsctl <span class="nb">set </span>open <span class="nb">.</span> external_ids:ovn-bridge-mappings<span class="o">=</span>physnet:breth0,tenantblue_br-localnet:ovsbr1,tenantred_br-localnet:ovsbr1
<span class="k">done</span>
</code></pre></div></div>

<p>We must now configure the physical network; since the packets are leaving the
OVS bridge tagged with either the 10 or 20 VLAN, we must configure the physical
network where the virtualized nodes run to handle the tagged traffic.</p>

<p>For that we will create two VLANed interfaces, each with a different subnet; we
will need to know the name of the bridge the kind script created to implement
the extra network it required. Those VLAN interfaces also need to be configured
with an IP address:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">OCI_BIN</span><span class="o">=</span>podman | docker <span class="c"># choose your cup of tea.</span>
<span class="nv">$OCI_BIN</span> network inspect underlay <span class="nt">--format</span> <span class="s1">'{ .NetworkInterface }}'</span>
podman3

<span class="c"># create the VLANs</span>
ip <span class="nb">link </span>add <span class="nb">link </span>podman3 name podman3.10 <span class="nb">type </span>vlan <span class="nb">id </span>10
ip addr add 192.168.123.1/24 dev podman3.10
ip <span class="nb">link set </span>dev podman3.10 up

ip <span class="nb">link </span>add <span class="nb">link </span>podman3 name podman3.20 <span class="nb">type </span>vlan <span class="nb">id </span>20
ip addr add 192.168.124.1/24 dev podman3.20
ip <span class="nb">link set </span>dev podman3.20 up
</code></pre></div></div>

<p><strong>NOTE:</strong> both the <code class="language-plaintext highlighter-rouge">tenantblue</code> and <code class="language-plaintext highlighter-rouge">tenantred</code> networks forward their traffic
to the <code class="language-plaintext highlighter-rouge">ovsbr1</code> OVS bridge.</p>

<h4 id="defining-the-ovn-kubernetes-networks-1">Defining the OVN-Kubernetes networks</h4>
<p>Let us now provision the attachment configuration for the two physical networks.
Notice they do not have a subnet defined, which means our workloads must
configure static IPs via cloud-init.</p>
<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">k8s.cni.cncf.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">NetworkAttachmentDefinition</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">tenantred</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">config</span><span class="pi">:</span> <span class="pi">|2</span>
    <span class="s">{</span>
            <span class="s">"cniVersion": "0.3.1",</span>
            <span class="s">"name": "tenantred",</span>
            <span class="s">"type": "ovn-k8s-cni-overlay",</span>
            <span class="s">"topology": "localnet",</span>
            <span class="s">"vlanID": 10,</span>
            <span class="s">"netAttachDefName": "default/tenantred"</span>
    <span class="s">}</span>
<span class="s">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">k8s.cni.cncf.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">NetworkAttachmentDefinition</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">tenantblue</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">config</span><span class="pi">:</span> <span class="pi">|2</span>
    <span class="s">{</span>
            <span class="s">"cniVersion": "0.3.1",</span>
            <span class="s">"name": "tenantblue",</span>
            <span class="s">"type": "ovn-k8s-cni-overlay",</span>
            <span class="s">"topology": "localnet",</span>
            <span class="s">"vlanID": 20,</span>
            <span class="s">"netAttachDefName": "default/tenantblue"</span>
    <span class="s">}</span>
</code></pre></div></div>

<p><strong>NOTE:</strong> each of the <code class="language-plaintext highlighter-rouge">tenantblue</code> and <code class="language-plaintext highlighter-rouge">tenantred</code> networks tags their traffic
with a different VLAN, which must be listed on the port <code class="language-plaintext highlighter-rouge">trunks</code> configuration.</p>

<h4 id="spin-up-the-vms-1">Spin up the VMs</h4>
<p>These two VMs can be used for the OVS bridge sharing scenario (two physical
networks share the same OVS bridge, each on a different VLAN).</p>
<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">kubevirt.io/v1alpha3</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachine</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">vm-red-1</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">running</span><span class="pi">:</span> <span class="no">true</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">nodeSelector</span><span class="pi">:</span>
        <span class="na">kubernetes.io/hostname</span><span class="pi">:</span> <span class="s">ovn-worker</span>
      <span class="na">domain</span><span class="pi">:</span>
        <span class="na">devices</span><span class="pi">:</span>
          <span class="na">disks</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
              <span class="na">disk</span><span class="pi">:</span>
                <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
              <span class="na">disk</span><span class="pi">:</span>
                <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
          <span class="na">interfaces</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">physnet-red</span>
            <span class="na">bridge</span><span class="pi">:</span> <span class="pi">{}</span>
        <span class="na">machine</span><span class="pi">:</span>
          <span class="na">type</span><span class="pi">:</span> <span class="s2">"</span><span class="s">"</span>
        <span class="na">resources</span><span class="pi">:</span>
          <span class="na">requests</span><span class="pi">:</span>
            <span class="na">memory</span><span class="pi">:</span> <span class="s">1024M</span>
      <span class="na">networks</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">physnet-red</span>
        <span class="na">multus</span><span class="pi">:</span>
          <span class="na">networkName</span><span class="pi">:</span> <span class="s">tenantred</span>
      <span class="na">terminationGracePeriodSeconds</span><span class="pi">:</span> <span class="m">0</span>
      <span class="na">volumes</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
          <span class="na">containerDisk</span><span class="pi">:</span>
            <span class="na">image</span><span class="pi">:</span> <span class="s">quay.io/kubevirt/fedora-with-test-tooling-container-disk:devel</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
          <span class="na">cloudInitNoCloud</span><span class="pi">:</span>
            <span class="na">networkData</span><span class="pi">:</span> <span class="pi">|</span>
              <span class="s">version: 2</span>
              <span class="s">ethernets:</span>
                <span class="s">eth0:</span>
                  <span class="s">addresses: [ 192.168.123.10/24 ]</span>
            <span class="na">userData</span><span class="pi">:</span> <span class="pi">|-</span>
              <span class="s">#cloud-config</span>
              <span class="s">password: fedora</span>
              <span class="s">chpasswd: { expire: False }</span>
<span class="s">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">kubevirt.io/v1alpha3</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachine</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">vm-red-2</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">running</span><span class="pi">:</span> <span class="no">true</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">nodeSelector</span><span class="pi">:</span>
        <span class="na">kubernetes.io/hostname</span><span class="pi">:</span> <span class="s">ovn-worker</span>
      <span class="na">domain</span><span class="pi">:</span>
        <span class="na">devices</span><span class="pi">:</span>
          <span class="na">disks</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
              <span class="na">disk</span><span class="pi">:</span>
                <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
              <span class="na">disk</span><span class="pi">:</span>
                <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
          <span class="na">interfaces</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">flatl2-overlay</span>
            <span class="na">bridge</span><span class="pi">:</span> <span class="pi">{}</span>
        <span class="na">machine</span><span class="pi">:</span>
          <span class="na">type</span><span class="pi">:</span> <span class="s2">"</span><span class="s">"</span>
        <span class="na">resources</span><span class="pi">:</span>
          <span class="na">requests</span><span class="pi">:</span>
            <span class="na">memory</span><span class="pi">:</span> <span class="s">1024M</span>
      <span class="na">networks</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">flatl2-overlay</span>
        <span class="na">multus</span><span class="pi">:</span>
          <span class="na">networkName</span><span class="pi">:</span> <span class="s">tenantred</span>
      <span class="na">terminationGracePeriodSeconds</span><span class="pi">:</span> <span class="m">0</span>
      <span class="na">volumes</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
          <span class="na">containerDisk</span><span class="pi">:</span>
            <span class="na">image</span><span class="pi">:</span> <span class="s">quay.io/kubevirt/fedora-with-test-tooling-container-disk:devel</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
          <span class="na">cloudInitNoCloud</span><span class="pi">:</span>
            <span class="na">networkData</span><span class="pi">:</span> <span class="pi">|</span>
              <span class="s">version: 2</span>
              <span class="s">ethernets:</span>
                <span class="s">eth0:</span>
                  <span class="s">addresses: [ 192.168.123.20/24 ]</span>
            <span class="na">userData</span><span class="pi">:</span> <span class="pi">|-</span>
              <span class="s">#cloud-config</span>
              <span class="s">password: fedora</span>
              <span class="s">chpasswd: { expire: False }</span>
<span class="s">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">kubevirt.io/v1alpha3</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachine</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">vm-blue-1</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">running</span><span class="pi">:</span> <span class="no">true</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">nodeSelector</span><span class="pi">:</span>
        <span class="na">kubernetes.io/hostname</span><span class="pi">:</span> <span class="s">ovn-worker</span>
      <span class="na">domain</span><span class="pi">:</span>
        <span class="na">devices</span><span class="pi">:</span>
          <span class="na">disks</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
              <span class="na">disk</span><span class="pi">:</span>
                <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
              <span class="na">disk</span><span class="pi">:</span>
                <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
          <span class="na">interfaces</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">physnet-blue</span>
            <span class="na">bridge</span><span class="pi">:</span> <span class="pi">{}</span>
        <span class="na">machine</span><span class="pi">:</span>
          <span class="na">type</span><span class="pi">:</span> <span class="s2">"</span><span class="s">"</span>
        <span class="na">resources</span><span class="pi">:</span>
          <span class="na">requests</span><span class="pi">:</span>
            <span class="na">memory</span><span class="pi">:</span> <span class="s">1024M</span>
      <span class="na">networks</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">physnet-blue</span>
        <span class="na">multus</span><span class="pi">:</span>
          <span class="na">networkName</span><span class="pi">:</span> <span class="s">tenantblue</span>
      <span class="na">terminationGracePeriodSeconds</span><span class="pi">:</span> <span class="m">0</span>
      <span class="na">volumes</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
          <span class="na">containerDisk</span><span class="pi">:</span>
            <span class="na">image</span><span class="pi">:</span> <span class="s">quay.io/kubevirt/fedora-with-test-tooling-container-disk:devel</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
          <span class="na">cloudInitNoCloud</span><span class="pi">:</span>
            <span class="na">networkData</span><span class="pi">:</span> <span class="pi">|</span>
              <span class="s">version: 2</span>
              <span class="s">ethernets:</span>
                <span class="s">eth0:</span>
                  <span class="s">addresses: [ 192.168.124.10/24 ]</span>
            <span class="na">userData</span><span class="pi">:</span> <span class="pi">|-</span>
              <span class="s">#cloud-config</span>
              <span class="s">password: fedora</span>
              <span class="s">chpasswd: { expire: False }</span>
<span class="s">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">kubevirt.io/v1alpha3</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachine</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">vm-blue-2</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">running</span><span class="pi">:</span> <span class="no">true</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">nodeSelector</span><span class="pi">:</span>
        <span class="na">kubernetes.io/hostname</span><span class="pi">:</span> <span class="s">ovn-worker</span>
      <span class="na">domain</span><span class="pi">:</span>
        <span class="na">devices</span><span class="pi">:</span>
          <span class="na">disks</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
              <span class="na">disk</span><span class="pi">:</span>
                <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
              <span class="na">disk</span><span class="pi">:</span>
                <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
          <span class="na">interfaces</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">physnet-blue</span>
            <span class="na">bridge</span><span class="pi">:</span> <span class="pi">{}</span>
        <span class="na">machine</span><span class="pi">:</span>
          <span class="na">type</span><span class="pi">:</span> <span class="s2">"</span><span class="s">"</span>
        <span class="na">resources</span><span class="pi">:</span>
          <span class="na">requests</span><span class="pi">:</span>
            <span class="na">memory</span><span class="pi">:</span> <span class="s">1024M</span>
      <span class="na">networks</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">physnet-blue</span>
        <span class="na">multus</span><span class="pi">:</span>
          <span class="na">networkName</span><span class="pi">:</span> <span class="s">tenantblue</span>
      <span class="na">terminationGracePeriodSeconds</span><span class="pi">:</span> <span class="m">0</span>
      <span class="na">volumes</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
          <span class="na">containerDisk</span><span class="pi">:</span>
            <span class="na">image</span><span class="pi">:</span> <span class="s">quay.io/kubevirt/fedora-with-test-tooling-container-disk:devel</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
          <span class="na">cloudInitNoCloud</span><span class="pi">:</span>
            <span class="na">networkData</span><span class="pi">:</span> <span class="pi">|</span>
              <span class="s">version: 2</span>
              <span class="s">ethernets:</span>
                <span class="s">eth0:</span>
                  <span class="s">addresses: [ 192.168.124.20/24 ]</span>
            <span class="na">userData</span><span class="pi">:</span> <span class="pi">|-</span>
              <span class="s">#cloud-config</span>
              <span class="s">password: fedora</span>
              <span class="s">chpasswd: { expire: False }</span>
</code></pre></div></div>

<h4 id="test-east--west-communication-1">Test East / West communication</h4>
<p>You can check east/west connectivity between both <strong>red</strong> VMs via ICMP:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl get vmi vm-red-2 <span class="nt">-ojsonpath</span><span class="o">=</span><span class="s2">"{ @.status.interfaces }"</span> | jq
<span class="o">[</span>
  <span class="o">{</span>
    <span class="s2">"infoSource"</span>: <span class="s2">"domain, guest-agent"</span>,
    <span class="s2">"interfaceName"</span>: <span class="s2">"eth0"</span>,
    <span class="s2">"ipAddress"</span>: <span class="s2">"192.168.123.20"</span>,
    <span class="s2">"ipAddresses"</span>: <span class="o">[</span>
      <span class="s2">"192.168.123.20"</span>,
      <span class="s2">"fe80::e83d:16ff:fe76:c1bd"</span>
    <span class="o">]</span>,
    <span class="s2">"mac"</span>: <span class="s2">"ea:3d:16:76:c1:bd"</span>,
    <span class="s2">"name"</span>: <span class="s2">"flatl2-overlay"</span>,
    <span class="s2">"queueCount"</span>: 1
  <span class="o">}</span>
<span class="o">]</span>

<span class="nv">$ </span>virtctl console vm-red-1
Successfully connected to vm-red-1 console. The escape sequence is ^]

<span class="o">[</span>fedora@vm-red-1 ~]<span class="nv">$ </span>ping 192.168.123.20
PING 192.168.123.20 <span class="o">(</span>192.168.123.20<span class="o">)</span> 56<span class="o">(</span>84<span class="o">)</span> bytes of data.
64 bytes from 192.168.123.20: <span class="nv">icmp_seq</span><span class="o">=</span>1 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>0.534 ms
64 bytes from 192.168.123.20: <span class="nv">icmp_seq</span><span class="o">=</span>2 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>0.246 ms
64 bytes from 192.168.123.20: <span class="nv">icmp_seq</span><span class="o">=</span>3 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>0.178 ms
64 bytes from 192.168.123.20: <span class="nv">icmp_seq</span><span class="o">=</span>4 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>0.236 ms

<span class="nt">---</span> 192.168.123.20 ping statistics <span class="nt">---</span>
4 packets transmitted, 4 received, 0% packet loss, <span class="nb">time </span>3028ms
rtt min/avg/max/mdev <span class="o">=</span> 0.178/0.298/0.534/0.138 ms
</code></pre></div></div>

<p>The same behavior can be seen on the VMs attached to the <strong>blue</strong> network:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl get vmi vm-blue-2 <span class="nt">-ojsonpath</span><span class="o">=</span><span class="s2">"{ @.status.interfaces }"</span> | jq
<span class="o">[</span>
  <span class="o">{</span>
    <span class="s2">"infoSource"</span>: <span class="s2">"domain, guest-agent"</span>,
    <span class="s2">"interfaceName"</span>: <span class="s2">"eth0"</span>,
    <span class="s2">"ipAddress"</span>: <span class="s2">"192.168.124.20"</span>,
    <span class="s2">"ipAddresses"</span>: <span class="o">[</span>
      <span class="s2">"192.168.124.20"</span>,
      <span class="s2">"fe80::6cae:e4ff:fefc:bd02"</span>
    <span class="o">]</span>,
    <span class="s2">"mac"</span>: <span class="s2">"6e:ae:e4:fc:bd:02"</span>,
    <span class="s2">"name"</span>: <span class="s2">"physnet-blue"</span>,
    <span class="s2">"queueCount"</span>: 1
  <span class="o">}</span>
<span class="o">]</span>

<span class="nv">$ </span>virtctl console vm-blue-1
Successfully connected to vm-blue-1 console. The escape sequence is ^]

<span class="o">[</span>fedora@vm-blue-1 ~]<span class="nv">$ </span>ping 
<span class="o">[</span>fedora@vm-blue-1 ~]<span class="nv">$ </span>ping 192.168.124.20
PING 192.168.124.20 <span class="o">(</span>192.168.124.20<span class="o">)</span> 56<span class="o">(</span>84<span class="o">)</span> bytes of data.
64 bytes from 192.168.124.20: <span class="nv">icmp_seq</span><span class="o">=</span>1 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>0.531 ms
64 bytes from 192.168.124.20: <span class="nv">icmp_seq</span><span class="o">=</span>2 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>0.255 ms
64 bytes from 192.168.124.20: <span class="nv">icmp_seq</span><span class="o">=</span>3 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>0.688 ms
64 bytes from 192.168.124.20: <span class="nv">icmp_seq</span><span class="o">=</span>4 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>0.648 ms

<span class="nt">---</span> 192.168.124.20 ping statistics <span class="nt">---</span>
4 packets transmitted, 4 received, 0% packet loss, <span class="nb">time </span>3047ms
rtt min/avg/max/mdev <span class="o">=</span> 0.255/0.530/0.688/0.169 ms
</code></pre></div></div>

<h3 id="accessing-the-underlay-services">Accessing the underlay services</h3>
<p>We can now start HTTP servers listening to the IPs attached on the VLAN
interfaces:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python3 <span class="nt">-m</span> http.server <span class="nt">--bind</span> 192.168.123.1 9000 &amp;
python3 <span class="nt">-m</span> http.server <span class="nt">--bind</span> 192.168.124.1 9000 &amp;
</code></pre></div></div>

<p>And finally curl this from your client (blue network):</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>fedora@vm-blue-1 ~]<span class="nv">$ </span>curl <span class="nt">-v</span> 192.168.124.1:9000
<span class="k">*</span>   Trying 192.168.124.1:9000...
<span class="k">*</span> Connected to 192.168.124.1 <span class="o">(</span>192.168.124.1<span class="o">)</span> port 9000 <span class="o">(</span><span class="c">#0)</span>
<span class="o">&gt;</span> GET / HTTP/1.1
<span class="o">&gt;</span> Host: 192.168.124.1:9000
<span class="o">&gt;</span> User-Agent: curl/7.69.1
<span class="o">&gt;</span> Accept: <span class="k">*</span>/<span class="k">*</span>
<span class="o">&gt;</span> 
<span class="k">*</span> Mark bundle as not supporting multiuse
<span class="k">*</span> HTTP 1.0, assume close after body
&lt; HTTP/1.0 200 OK
&lt; Server: SimpleHTTP/0.6 Python/3.11.3
&lt; Date: Thu, 01 Jun 2023 16:05:09 GMT
&lt; Content-type: text/html<span class="p">;</span> <span class="nv">charset</span><span class="o">=</span>utf-8
&lt; Content-Length: 2923
...
</code></pre></div></div>

<p>And from the client connected to the red network:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>fedora@vm-red-1 ~]<span class="nv">$ </span>curl <span class="nt">-v</span> 192.168.123.1:9000
<span class="k">*</span>   Trying 192.168.123.1:9000...
<span class="k">*</span> Connected to 192.168.123.1 <span class="o">(</span>192.168.123.1<span class="o">)</span> port 9000 <span class="o">(</span><span class="c">#0)</span>
<span class="o">&gt;</span> GET / HTTP/1.1
<span class="o">&gt;</span> Host: 192.168.123.1:9000
<span class="o">&gt;</span> User-Agent: curl/7.69.1
<span class="o">&gt;</span> Accept: <span class="k">*</span>/<span class="k">*</span>
<span class="o">&gt;</span> 
<span class="k">*</span> Mark bundle as not supporting multiuse
<span class="k">*</span> HTTP 1.0, assume close after body
&lt; HTTP/1.0 200 OK
&lt; Server: SimpleHTTP/0.6 Python/3.11.3
&lt; Date: Thu, 01 Jun 2023 16:06:02 GMT
&lt; Content-type: text/html<span class="p">;</span> <span class="nv">charset</span><span class="o">=</span>utf-8
&lt; Content-Length: 2923
&lt; 
...
</code></pre></div></div>

<h2 id="conclusions">Conclusions</h2>
<p>In this post we have seen how to use OVN-Kubernetes to create secondary
networks connected to the physical underlay, allowing both east/west
communication between VMs, and access to services running outside the
Kubernetes cluster.</p>]]></content><author><name>Miguel Duarte Barroso</name></author><category term="news" /><category term="Kubevirt" /><category term="kubernetes" /><category term="virtual machine" /><category term="VM" /><category term="SDN" /><category term="OVN" /><summary type="html"><![CDATA[This post explains how to configure secondary networks connected to the physical underlay for KubeVirt virtual machines.]]></summary></entry><entry><title type="html">Secondary networks for KubeVirt VMs using OVN-Kubernetes</title><link href="https://kubevirt.io//2023/OVN-kubernetes-secondary-networks.html" rel="alternate" type="text/html" title="Secondary networks for KubeVirt VMs using OVN-Kubernetes" /><published>2023-03-06T00:00:00+00:00</published><updated>2023-03-06T00:00:00+00:00</updated><id>https://kubevirt.io//2023/OVN-kubernetes-secondary-networks</id><content type="html" xml:base="https://kubevirt.io//2023/OVN-kubernetes-secondary-networks.html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>OVN (Open Virtual Network) is a series of daemons for the Open vSwitch that
translate virtual network configurations into OpenFlow. It provides virtual
networking capabilities for any type of workload on a virtualized platform
(virtual machines and containers) using the same API.</p>

<p>OVN provides a higher-layer of abstraction than Open vSwitch, working with
logical routers and logical switches, rather than flows.
More details can be found in the OVN architecture
<a href="https://man7.org/linux/man-pages/man7/ovn-architecture.7.html#DESCRIPTION">man page</a>.</p>

<p>In this post we will repeat the scenario of
<a href="https://kubevirt.io/2020/Multiple-Network-Attachments-with-bridge-CNI.html">its bridge CNI equivalent</a>,
using this SDN approach, which uses virtual networking infrastructure: thus, it
is <strong>not</strong> required to provision VLANs or other physical network resources.</p>

<h2 id="demo">Demo</h2>
<p>To run this demo, you will need a Kubernetes cluster with the following
components installed:</p>
<ul>
  <li>OVN-Kubernetes</li>
  <li>multus-cni</li>
  <li>KubeVirt</li>
</ul>

<p>The <a href="#environment-setup">following section</a> will show you how to create a
<a href="https://kind.sigs.k8s.io/">KinD</a> cluster, with upstream latest OVN-Kubernetes,
and upstream latest multus-cni deployed. Please <strong>skip</strong> this section if your
cluster already features these components (e.g. Openshift).</p>

<h3 id="setup-demo-environment">Setup demo environment</h3>
<p>Refer to the OVN-Kubernetes repo
<a href="https://github.com/ovn-org/ovn-kubernetes/blob/master/docs/kind.md#ovn-kubernetes-kind-setup">KIND documentation</a>
for more details; the gist of it is you should clone the OVN-Kubernetes
repository, and run their kind helper script:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone git@github.com:ovn-org/ovn-kubernetes.git

<span class="nb">cd </span>ovn-kubernetes
<span class="nb">pushd </span>contrib <span class="p">;</span> ./kind.sh <span class="nt">--multi-network-enable</span> <span class="p">;</span> <span class="nb">popd</span>
</code></pre></div></div>

<p>This will get you a running kind cluster, configured to use OVN-Kubernetes as
the default cluster network, configuring the multi-homing OVN-Kubernetes feature
gate, and deploying
<a href="https://github.com/k8snetworkplumbingwg/multus-cni">multus-cni</a> in the cluster.</p>

<h4 id="install-kubevirt-in-the-cluster">Install KubeVirt in the cluster</h4>
<p>Follow Kubevirt’s
<a href="https://kubevirt.io/user-guide/operations/installation/#installing-kubevirt-on-kubernetes">user guide</a>
to install the latest released version (currently, v0.59.0). Please skip this
section if you already have a running cluster with KubeVirt installed in it.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">export </span><span class="nv">RELEASE</span><span class="o">=</span><span class="si">$(</span>curl https://storage.googleapis.com/kubevirt-prow/release/kubevirt/kubevirt/stable.txt<span class="si">)</span>
kubectl apply <span class="nt">-f</span> <span class="s2">"https://github.com/kubevirt/kubevirt/releases/download/</span><span class="k">${</span><span class="nv">RELEASE</span><span class="k">}</span><span class="s2">/kubevirt-operator.yaml"</span>
kubectl apply <span class="nt">-f</span> <span class="s2">"https://github.com/kubevirt/kubevirt/releases/download/</span><span class="k">${</span><span class="nv">RELEASE</span><span class="k">}</span><span class="s2">/kubevirt-cr.yaml"</span>
kubectl <span class="nt">-n</span> kubevirt <span class="nb">wait </span>kv kubevirt <span class="nt">--timeout</span><span class="o">=</span>360s <span class="nt">--for</span> <span class="nv">condition</span><span class="o">=</span>Available
</code></pre></div></div>

<p>Now we have a Kubernetes cluster with all the pieces to start the Demo.</p>

<h3 id="define-the-overlay-network">Define the overlay network</h3>
<p>Provision the following yaml to define the overlay which will configure the
secondary attachment for the KubeVirt VMs. Please refer to the OVN-Kubernetes
user
<a href="https://github.com/ovn-org/ovn-kubernetes/blob/master/docs/multi-homing.md#switched---layer-2---topology">documentation</a>
for details into each of the knobs.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | kubectl apply -f -
apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
metadata:
  name: l2-network
  namespace: default
spec:
  config: |2
    {
            "cniVersion": "0.3.1",
            "name": "l2-network",
            "type": "ovn-k8s-cni-overlay",
            "topology":"layer2",
            "netAttachDefName": "default/l2-network"
    }
</span><span class="no">EOF
</span></code></pre></div></div>

<p>The above example will configure a cluster-wide overlay <strong>without</strong> a subnet
defined. This means the users will have to define static IPs for their VMs.</p>

<p>It is also worth to point out the value of the <code class="language-plaintext highlighter-rouge">netAttachDefName</code> attribute
must match the <code class="language-plaintext highlighter-rouge">&lt;namespace&gt;/&lt;name&gt;</code> of the surrounding
<code class="language-plaintext highlighter-rouge">NetworkAttachmentDefinition</code> object.</p>

<h3 id="spin-up-the-vms">Spin up the VMs</h3>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | kubectl apply -f -
---
apiVersion: kubevirt.io/v1alpha3
kind: VirtualMachine
metadata:
  name: vm-server
spec:
  running: true
  template:
    spec:
      domain:
        devices:
          disks:
            - name: containerdisk
              disk:
                bus: virtio
            - name: cloudinitdisk
              disk:
                bus: virtio
          interfaces:
          - name: default
            masquerade: {}
          - name: flatl2-overlay
            bridge: {}
        machine:
          type: ""
        resources:
          requests:
            memory: 1024M
      networks:
      - name: default
        pod: {}
      - name: flatl2-overlay
        multus:
          networkName: l2-network
      terminationGracePeriodSeconds: 0
      volumes:
        - name: containerdisk
          containerDisk:
            image: quay.io/kubevirt/fedora-with-test-tooling-container-disk:devel
        - name: cloudinitdisk
          cloudInitNoCloud:
            networkData: |
              version: 2
              ethernets:
                eth1:
                  addresses: [ 192.0.2.20/24 ]
            userData: |-
              #cloud-config
              password: fedora
              chpasswd: { expire: False }
---
apiVersion: kubevirt.io/v1alpha3
kind: VirtualMachine
metadata:
  name: vm-client
spec:
  running: true
  template:
    spec:
      domain:
        devices:
          disks:
            - name: containerdisk
              disk:
                bus: virtio
            - name: cloudinitdisk
              disk:
                bus: virtio
          interfaces:
          - name: default
            masquerade: {}
          - name: flatl2-overlay
            bridge: {}
        machine:
          type: ""
        resources:
          requests:
            memory: 1024M
      networks:
      - name: default
        pod: {}
      - name: flatl2-overlay
        multus:
          networkName: l2-network
      terminationGracePeriodSeconds: 0
      volumes:
        - name: containerdisk
          containerDisk:
            image: quay.io/kubevirt/fedora-with-test-tooling-container-disk:devel
        - name: cloudinitdisk
          cloudInitNoCloud:
            networkData: |
              version: 2
              ethernets:
                eth1:
                  addresses: [ 192.0.2.10/24 ]
            userData: |-
              #cloud-config
              password: fedora
              chpasswd: { expire: False }
</span><span class="no">EOF
</span></code></pre></div></div>

<p>Provision these two Virtual Machines, and wait for them to boot up.</p>

<h3 id="test-connectivity">Test connectivity</h3>
<p>To verify connectivity over our layer 2 overlay, we need first to ensure the IP
address of the server VM; let’s query the VMI status for that:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl get vmi vm-server <span class="nt">-ojsonpath</span><span class="o">=</span><span class="s2">"{ @.status.interfaces }"</span> | jq
<span class="o">[</span>
  <span class="o">{</span>
    <span class="s2">"infoSource"</span>: <span class="s2">"domain, guest-agent"</span>,
    <span class="s2">"interfaceName"</span>: <span class="s2">"eth0"</span>,
    <span class="s2">"ipAddress"</span>: <span class="s2">"10.244.2.8"</span>,
    <span class="s2">"ipAddresses"</span>: <span class="o">[</span>
      <span class="s2">"10.244.2.8"</span>
    <span class="o">]</span>,
    <span class="s2">"mac"</span>: <span class="s2">"52:54:00:23:1c:c2"</span>,
    <span class="s2">"name"</span>: <span class="s2">"default"</span>,
    <span class="s2">"queueCount"</span>: 1
  <span class="o">}</span>,
  <span class="o">{</span>
    <span class="s2">"infoSource"</span>: <span class="s2">"domain, guest-agent"</span>,
    <span class="s2">"interfaceName"</span>: <span class="s2">"eth1"</span>,
    <span class="s2">"ipAddress"</span>: <span class="s2">"192.0.2.20"</span>,
    <span class="s2">"ipAddresses"</span>: <span class="o">[</span>
      <span class="s2">"192.0.2.20"</span>,
      <span class="s2">"fe80::7cab:88ff:fe5b:39f"</span>
    <span class="o">]</span>,
    <span class="s2">"mac"</span>: <span class="s2">"7e:ab:88:5b:03:9f"</span>,
    <span class="s2">"name"</span>: <span class="s2">"flatl2-overlay"</span>,
    <span class="s2">"queueCount"</span>: 1
  <span class="o">}</span>
<span class="o">]</span>
</code></pre></div></div>

<p>You can afterwards connect to them via console and ping <code class="language-plaintext highlighter-rouge">vm-server</code>:</p>

<div class="premonition note"><div class="fa fa-check-square"></div><div class="content"><p class="header">Note</p><p>The user and password for this VMs is fedora; check the VM template spec cloudinit userData</p>


</div></div>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>virtctl console vm-client
ip a <span class="c"># confirm the IP address is the one set via cloud-init</span>
<span class="o">[</span>fedora@vm-client ~]<span class="nv">$ </span>ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    <span class="nb">link</span>/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1400 qdisc fq_codel state UP group default qlen 1000
    <span class="nb">link</span>/ether 52:54:00:29:de:53 brd ff:ff:ff:ff:ff:ff
    altname enp1s0
    inet 10.0.2.2/24 brd 10.0.2.255 scope global dynamic noprefixroute eth0
       valid_lft 86313584sec preferred_lft 86313584sec
    inet6 fe80::5054:ff:fe29:de53/64 scope <span class="nb">link
       </span>valid_lft forever preferred_lft forever
3: eth1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1400 qdisc fq_codel state UP group default qlen 1000
    <span class="nb">link</span>/ether 36:f9:29:65:66:55 brd ff:ff:ff:ff:ff:ff
    altname enp2s0
    inet 192.0.2.10/24 brd 192.0.2.255 scope global noprefixroute eth1
       valid_lft forever preferred_lft forever
    inet6 fe80::34f9:29ff:fe65:6655/64 scope <span class="nb">link
       </span>valid_lft forever preferred_lft forever

<span class="o">[</span>fedora@vm-client ~]<span class="nv">$ </span>ping <span class="nt">-c4</span> 192.0.2.20 <span class="c"># ping the vm-server static IP</span>
PING 192.0.2.20 <span class="o">(</span>192.0.2.20<span class="o">)</span> 56<span class="o">(</span>84<span class="o">)</span> bytes of data.
64 bytes from 192.0.2.20: <span class="nv">icmp_seq</span><span class="o">=</span>1 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>1.05 ms
64 bytes from 192.0.2.20: <span class="nv">icmp_seq</span><span class="o">=</span>2 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>1.05 ms
64 bytes from 192.0.2.20: <span class="nv">icmp_seq</span><span class="o">=</span>3 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>0.995 ms
64 bytes from 192.0.2.20: <span class="nv">icmp_seq</span><span class="o">=</span>4 <span class="nv">ttl</span><span class="o">=</span>64 <span class="nb">time</span><span class="o">=</span>0.902 ms

<span class="nt">---</span> 192.0.2.20 ping statistics <span class="nt">---</span>
4 packets transmitted, 4 received, 0% packet loss, <span class="nb">time </span>3006ms
rtt min/avg/max/mdev <span class="o">=</span> 0.902/0.997/1.046/0.058 ms
</code></pre></div></div>
<h2 id="conclusion">Conclusion</h2>
<p>In this post we have seen how to use OVN-Kubernetes to create an overlay to
connect VMs in different nodes using secondary networks, without having to
configure any physical networking infrastructure.</p>]]></content><author><name>Miguel Duarte Barroso</name></author><category term="news" /><category term="Kubevirt" /><category term="kubernetes" /><category term="virtual machine" /><category term="VM" /><category term="SDN" /><category term="OVN" /><summary type="html"><![CDATA[This post explains how to configure cluster-wide overlays as secondary networks for KubeVirt virtual machines.]]></summary></entry><entry><title type="html">KubeVirt Summit 2023!</title><link href="https://kubevirt.io//2023/KubeVirt-Summit-2023.html" rel="alternate" type="text/html" title="KubeVirt Summit 2023!" /><published>2023-03-03T00:00:00+00:00</published><updated>2023-03-03T00:00:00+00:00</updated><id>https://kubevirt.io//2023/KubeVirt-Summit-2023</id><content type="html" xml:base="https://kubevirt.io//2023/KubeVirt-Summit-2023.html"><![CDATA[<p>The third online <a href="/summit/">KubeVirt Summit</a> starts March 29, 2023!</p>

<h2 id="when">When</h2>

<p>The event will take place online over two half-days:</p>

<ul>
  <li>Dates: March 29 and 30, 2023.</li>
  <li>Time: 14:00 – 19:00 UTC (9:00–14:00 EST, 15:00–20:00 CET)</li>
</ul>

<h2 id="register">Register</h2>

<p><a href="/summit/">KubeVirt Summit</a> is hosted on Community.CNCF.io. This is a free event but you need to register in order to attend.</p>

<p><a href="https://community.cncf.io/events/details/cncf-kubevirt-community-presents-kubevirt-summit-2023/">Register for KubeVirt Summit 2023</a></p>

<p>If this is your first time attending, you will need to create an account with CNCF.io.</p>

<h2 id="schedule">Schedule</h2>

<p>The schedule is available on the <a href="https://community.cncf.io/events/details/cncf-kubevirt-community-presents-kubevirt-summit-2023/">CNCF Community Events page</a> where you register, as well as on the <a href="/summit/">KubeVirt Summit page</a>.</p>

<h2 id="keep-up-to-date">Keep up to date</h2>

<p>Connect with the KubeVirt Community through our <a href="/community">community page</a>.</p>

<p>See you there!</p>]]></content><author><name>Andrew Burden</name></author><category term="news" /><category term="kubevirt" /><category term="event" /><category term="community" /><summary type="html"><![CDATA[Join us for the KubeVirt community's third annual dedicated online event]]></summary></entry><entry><title type="html">KubeVirt v0.59.0</title><link href="https://kubevirt.io//2023/changelog-v0.59.0.html" rel="alternate" type="text/html" title="KubeVirt v0.59.0" /><published>2023-03-01T00:00:00+00:00</published><updated>2023-03-01T00:00:00+00:00</updated><id>https://kubevirt.io//2023/changelog-v0.59.0</id><content type="html" xml:base="https://kubevirt.io//2023/changelog-v0.59.0.html"><![CDATA[<h2 id="v0590">v0.59.0</h2>

<p>Released on: Wed Mar 1 16:49:27 2023 +0000</p>

<ul>
  <li>[PR #9311][kubevirt-bot] fixes the requests/limits CPU number mismatch for VMs with isolatedEmulatorThread</li>
  <li>[PR #9276][fossedihelm] Added foreground finalizer to  virtual machine</li>
  <li>[PR #9295][kubevirt-bot] Fix bug of possible re-trigger of memory dump</li>
  <li>[PR #9270][kubevirt-bot] BugFix: Guestfs image url not constructed correctly</li>
  <li>[PR #9234][kubevirt-bot] The <code class="language-plaintext highlighter-rouge">dedicatedCPUPlacement</code> attribute is once again supported within the <code class="language-plaintext highlighter-rouge">VirtualMachineInstancetype</code> and <code class="language-plaintext highlighter-rouge">VirtualMachineClusterInstancetype</code> CRDs after a recent bugfix improved <code class="language-plaintext highlighter-rouge">VirtualMachine</code> validations, ensuring defaults are applied before any attempt to validate.</li>
  <li>[PR #9267][fossedihelm] This version of KubeVirt includes upgraded virtualization technology based on libvirt 9.0.0 and QEMU 7.2.0.</li>
  <li>[PR #9197][kubevirt-bot] Fix addvolume not rejecting adding existing volume source, fix removevolume allowing to remove non hotpluggable volume</li>
  <li>[PR #9120][0xFelix] Fix access to portforwarding on VMs/VMIs with the cluster roles kubevirt.io:admin and kubevirt.io:edit</li>
  <li>[PR #9116][EdDev] Allow the specification of the ACPI Index on a network interface.</li>
  <li>[PR #8774][avlitman] Added new Virtual machines CPU metrics:</li>
  <li>[PR #9087][zhuchenwang] Open <code class="language-plaintext highlighter-rouge">/dev/vhost-vsock</code> explicitly to ensure that the right vsock module is loaded</li>
  <li>[PR #9020][feitnomore] Adding support for status/scale subresources so that VirtualMachinePool now supports HorizontalPodAutoscaler</li>
  <li>[PR #9085][0xFelix] virtctl: Add options to infer instancetype and preference when creating a VM</li>
  <li>[PR #8917][xpivarc] Kubevirt can be configured with Seccomp profile. It now ships a custom profile for the launcher.</li>
  <li>[PR #9054][enp0s3] do not inject LimitRange defaults into VMI</li>
  <li>[PR #7862][vladikr] Store the finalized VMI migration status in the migration objects.</li>
  <li>[PR #8878][0xFelix] Add ‘create vm’ command to virtctl</li>
  <li>[PR #9048][jean-edouard] DisableCustomSELinuxPolicy feature gate introduced to disable our custom SELinux policy</li>
  <li>[PR #8953][awels] VMExport now has endpoint containing entire VM definition.</li>
  <li>[PR #8976][iholder101] Fix podman CRI detection</li>
  <li>[PR #9043][iholder101] Adjust operator functional tests to custom images specification</li>
  <li>[PR #8875][machadovilaca] Rename migration metrics removing ‘total’ keyword</li>
  <li>[PR #9040][lyarwood] <code class="language-plaintext highlighter-rouge">inferFromVolume</code> now uses labels instead of annotations to lookup default instance type and preference details from a referenced <code class="language-plaintext highlighter-rouge">Volume</code>. This has changed in order to provide users with a way of looking up suitably decorated resources through these labels before pointing to them within the <code class="language-plaintext highlighter-rouge">VirtualMachine</code>.</li>
  <li>[PR #9039][orelmisan] client-go: Added context to additional VirtualMachineInstance’s methods.</li>
  <li>[PR #9018][orelmisan] client-go: Added context to additional VirtualMachineInstance’s methods.</li>
  <li>[PR #9025][akalenyu] BugFix: Hotplug pods have hardcoded resource req which don’t comply with LimitRange maxLimitRequestRatio of 1</li>
  <li>[PR #8908][orelmisan] client-go: Added context to some of VirtualMachineInstance’s methods.</li>
  <li>[PR #6863][rmohr] The install strategy job will respect the infra node placement from now on</li>
  <li>[PR #8948][iholder101] Bugfix: virt-handler socket leak</li>
  <li>[PR #8649][acardace] KubeVirt is now able to run VMs inside restricted namespaces.</li>
  <li>[PR #8992][iholder101] Align with k8s fix for default limit range requirements</li>
  <li>[PR #8889][rmohr] Add basic TLS encryption support for vsock websocket connections</li>
  <li>[PR #8660][huyinhou] Fix remoteAddress field in virt-api log being truncated when it is an ipv6 address</li>
  <li>[PR #8961][rmohr] Bump distroless base images</li>
  <li>[PR #8952][rmohr] Fix read-only sata disk validation</li>
  <li>[PR #8657][fossedihelm] Use an increasingly exponential backoff before retrying to start the VM, when an I/O error occurs.</li>
  <li>[PR #8480][lyarwood] New <code class="language-plaintext highlighter-rouge">inferFromVolume</code> attributes have been introduced to the <code class="language-plaintext highlighter-rouge">{Instancetype,Preference}Matchers</code> of a <code class="language-plaintext highlighter-rouge">VirtualMachine</code>. When provided the <code class="language-plaintext highlighter-rouge">Volume</code> referenced by the attribute is checked for the following annotations with which to populate the <code class="language-plaintext highlighter-rouge">{Instancetype,Preference}Matchers</code>:</li>
  <li>[PR #7762][VirrageS] Service <code class="language-plaintext highlighter-rouge">kubevirt-prometheus-metrics</code> now sets <code class="language-plaintext highlighter-rouge">ClusterIP</code> to <code class="language-plaintext highlighter-rouge">None</code> to make it a headless service.</li>
  <li>[PR #8599][machadovilaca] Change KubevirtVmHighMemoryUsage threshold from 20MB to 50MB</li>
  <li>[PR #7761][VirrageS] imagePullSecrets field has been added to KubeVirt CR to support deployments form private registries</li>
  <li>[PR #8887][iholder101] Bugfix: use virt operator image if provided</li>
  <li>[PR #8750][jordigilh] Fixes an issue that prevented running real time workloads in non-root configurations due to libvirt’s dependency on CAP_SYS_NICE to change the vcpu’s thread’s scheduling and priority to FIFO and 1. The change of priority and scheduling is now executed in the virt-launcher for both root and non-root configurations, removing the dependency in libvirt.</li>
  <li>[PR #8845][lyarwood] An empty <code class="language-plaintext highlighter-rouge">Timer</code> is now correctly omitted from <code class="language-plaintext highlighter-rouge">Clock</code> fixing bug #8844.</li>
  <li>[PR #8842][andreabolognani] The virt-launcher pod no longer needs the SYS_PTRACE capability.</li>
  <li>[PR #8734][alicefr] Change libguestfs-tools image using root appliance in qcow2 format</li>
  <li>[PR #8764][ShellyKa13] Add list of included and excluded volumes in vmSnapshot</li>
  <li>[PR #8811][iholder101] Custom components: support gs</li>
  <li>[PR #8770][dhiller] Add Ginkgo V2 Serial decorator to serial tests as preparation to simplify parallel vs. serial test run logic</li>
  <li>[PR #8808][acardace] Apply migration backoff only for evacuation migrations.</li>
  <li>[PR #8525][jean-edouard] CR option mediatedDevicesTypes is deprecated in favor of mediatedDeviceTypes</li>
  <li>[PR #8792][iholder101] Expose new custom components env vars to csv-generator and manifest-templator</li>
  <li>[PR #8701][enp0s3] Consider the ParallelOutboundMigrationsPerNode when evicting VMs</li>
  <li>[PR #8740][iholder101] Fix: Align Reenlightenment flows between converter.go and template.go</li>
  <li>[PR #8530][acardace] Use exponential backoff for failing migrations</li>
  <li>[PR #8720][0xFelix] The expand-spec subresource endpoint was renamed to expand-vm-spec and made namespaced</li>
  <li>[PR #8458][iholder101] Introduce support for clones with a snapshot source (e.g. clone snapshot -&gt; VM)</li>
  <li>[PR #8716][rhrazdil] Add overhead of interface with Passt binding when no ports are specified</li>
  <li>[PR #8619][fossedihelm] virt-launcher: use <code class="language-plaintext highlighter-rouge">virtqemud</code> daemon instead of <code class="language-plaintext highlighter-rouge">libvirtd</code></li>
  <li>[PR #8736][knopt] Added more precise rest_client_request_latency_seconds histogram buckets</li>
  <li>[PR #8624][zhuchenwang] Add the REST API to be able to talk to the application in the guest VM via VSOCK.</li>
  <li>[PR #8625][AlonaKaplan] iptables are no longer used by masquerade binding. Nodes with iptables only won’t be able to run VMs with masquerade binding.</li>
  <li>[PR #8673][iholder101] Allow specifying custom images for core components</li>
  <li>[PR #8622][jean-edouard] Built with golang 1.19</li>
  <li>[PR #8336][alicefr] Flag for setting the guestfs uid and gid</li>
  <li>[PR #8667][huyinhou] connect VM vnc failed when virt-launcher work directory is not /</li>
  <li>[PR #8368][machadovilaca] Use collector to set migration metrics</li>
  <li>[PR #8558][xpivarc] Bug-fix: LimitRange integration now works when VMI is missing namespace</li>
  <li>[PR #8404][andreabolognani] This version of KubeVirt includes upgraded virtualization technology based on libvirt 8.7.0, QEMU 7.1.0 and CentOS Stream 9.</li>
  <li>[PR #8652][akalenyu] BugFix: Exporter pod does not comply with restricted PSA</li>
  <li>[PR #8563][xpivarc] Kubevirt now runs with nonroot user by default</li>
  <li>[PR #8442][kvaps] Add Deckhouse to the Adopters list</li>
  <li>[PR #8546][zhuchenwang] Provides the Vsock feature for KubeVirt VMs.</li>
  <li>[PR #8598][acardace] VMs configured with hugepages can now run using the default container_t SELinux type</li>
  <li>[PR #8594][kylealexlane] Fix permission denied on on selinux relabeling on some kernel versions</li>
  <li>[PR #8521][akalenyu] Add an option to specify a TTL for VMExport objects</li>
  <li>[PR #7918][machadovilaca] Add alerts for VMs unhealthy states</li>
  <li>[PR #8516][rhrazdil] When using Passt binding, virl-launcher has unprivileged_port_start set to 0, so that passt may bind to all ports.</li>
  <li>[PR #7772][jean-edouard] The SELinux policy for virt-launcher is down to 4 rules, 1 for hugepages and 3 for virtiofs.</li>
  <li>[PR #8402][jean-edouard] Most VMIs now run under the SELinux type container_t</li>
  <li>[PR #8513][alromeros] [Bug-fix] Fix error handling in virtctl image-upload</li>
</ul>]]></content><author><name>kube🤖</name></author><category term="releases" /><category term="release notes" /><category term="changelog" /><summary type="html"><![CDATA[This article provides information about KubeVirt release v0.59.0 changes]]></summary></entry><entry><title type="html">KubeVirt v0.58.0</title><link href="https://kubevirt.io//2022/changelog-v0.58.0.html" rel="alternate" type="text/html" title="KubeVirt v0.58.0" /><published>2022-10-13T00:00:00+00:00</published><updated>2022-10-13T00:00:00+00:00</updated><id>https://kubevirt.io//2022/changelog-v0.58.0</id><content type="html" xml:base="https://kubevirt.io//2022/changelog-v0.58.0.html"><![CDATA[<h2 id="v0580">v0.58.0</h2>

<p>Released on: Thu Oct 13 00:24:51 2022 +0000</p>

<ul>
  <li>[PR #8578][rhrazdil] When using Passt binding, virl-launcher has unprivileged_port_start set to 0, so that passt may bind to all ports.</li>
  <li>[PR #8463][Barakmor1] Improve metrics documentation</li>
  <li>[PR #8282][akrejcir] Improves instancetype and preference controller revisions. This is a backwards incompatible change and introduces a new v1alpha2 api for instancetype and preferences.</li>
  <li>[PR #8272][jean-edouard] No more empty section in the kubevirt-cr manifest</li>
  <li>[PR #8536][qinqon] Don’t show a failure if ConfigDrive cloud init has UserDataSecretRef and not NetworkDataSecretRef</li>
  <li>[PR #8375][xpivarc] Virtiofs can be used with Nonroot feature gate</li>
  <li>[PR #8465][rmohr] Add a vnc screenshot REST endpoint and a “virtctl vnc screenshot” command for UI and script integration</li>
  <li>[PR #8418][alromeros] Enable automatic token generation for VirtualMachineExport objects</li>
  <li>[PR #8488][0xFelix] virtctl: Be less verbose when using the local ssh client</li>
  <li>[PR #8396][alicefr] Add group flag for setting the gid and fsgroup in guestfs</li>
  <li>[PR #8476][iholder-redhat] Allow setting virt-operator log verbosity through Kubevirt CR</li>
  <li>[PR #8366][rthallisey] Move KubeVirt to a 15 week release cadence</li>
  <li>[PR #8479][arnongilboa] Enable DataVolume GC by default in cluster-deploy</li>
  <li>[PR #8474][vasiliy-ul] Fixed migration failure of VMs with containerdisks on systems with containerd</li>
  <li>[PR #8316][ShellyKa13] Fix possible race when deleting unready vmsnapshot and the vm remaining frozen</li>
  <li>[PR #8436][xpivarc] Kubevirt is able to run with restricted Pod Security Standard enabled with an automatic escalation of namespace privileges.</li>
  <li>[PR #8197][alromeros] Add vmexport command to virtctl</li>
  <li>[PR #8252][fossedihelm] Add <code class="language-plaintext highlighter-rouge">tlsConfiguration</code> to Kubevirt Configuration</li>
  <li>[PR #8431][rmohr] Fix shadow status updates and periodic status updates on VMs, performed by the snapshot controller</li>
  <li>[PR #8359][iholder-redhat] [Bugfix]: HyperV Reenlightenment VMIs should be able to start when TSC Frequency is not exposed</li>
  <li>[PR #8330][jean-edouard] Important: If you use docker with SELinux enabled, set the <code class="language-plaintext highlighter-rouge">DockerSELinuxMCSWorkaround</code> feature gate before upgrading</li>
  <li>[PR #8401][machadovilaca] Rename metrics to follow the naming convention</li>
</ul>]]></content><author><name>kube🤖</name></author><category term="releases" /><category term="release notes" /><category term="changelog" /><summary type="html"><![CDATA[This article provides information about KubeVirt release v0.58.0 changes]]></summary></entry><entry><title type="html">KubeVirt v0.57.0</title><link href="https://kubevirt.io//2022/changelog-v0.57.0.html" rel="alternate" type="text/html" title="KubeVirt v0.57.0" /><published>2022-09-12T00:00:00+00:00</published><updated>2022-09-12T00:00:00+00:00</updated><id>https://kubevirt.io//2022/changelog-v0.57.0</id><content type="html" xml:base="https://kubevirt.io//2022/changelog-v0.57.0.html"><![CDATA[<h2 id="v0570">v0.57.0</h2>

<p>Released on: Mon Sep 12 14:00:44 2022 +0000</p>

<ul>
  <li>[PR #8129][mlhnono68] Fixes virtctl to support connection to clusters proxied by RANCHER or having special paths</li>
  <li>[PR #8337][0xFelix] virtctl’s native SSH client is now useable in the Windows console without workarounds</li>
  <li>[PR #8257][awels] VirtualMachineExport now supports VM export source type.</li>
  <li>[PR #8367][vladikr] fix the guest memory conversion by setting it to resources.requests.memory when guest memory is not explicitly provided</li>
  <li>[PR #7990][ormergi] Deprecate SR-IOV live migration feature gate.</li>
  <li>[PR #8069][lyarwood] The VirtualMachineInstancePreset resource has been deprecated ahead of removal in a future release. Users should instead use the VirtualMachineInstancetype and VirtualMachinePreference resources to encapsulate any shared resource or preferences characteristics shared by their VirtualMachines.</li>
  <li>[PR #8326][0xFelix] virtctl: Do not log wrapped ssh command by default</li>
  <li>[PR #8325][rhrazdil] Enable route_localnet sysctl option for masquerade binding at virt-handler</li>
  <li>[PR #8159][acardace] Add support for USB disks</li>
  <li>[PR #8006][lyarwood] <code class="language-plaintext highlighter-rouge">AutoattachInputDevice</code> has been added to <code class="language-plaintext highlighter-rouge">Devices</code> allowing an <code class="language-plaintext highlighter-rouge">Input</code> device to be automatically attached to a <code class="language-plaintext highlighter-rouge">VirtualMachine</code> on start up.  <code class="language-plaintext highlighter-rouge">PreferredAutoattachInputDevice</code> has also been added to <code class="language-plaintext highlighter-rouge">DevicePreferences</code> allowing users to control this behaviour with a set of preferences.</li>
  <li>[PR #8134][arnongilboa] Support DataVolume garbage collection</li>
  <li>[PR #8157][StefanKro] TrilioVault for Kubernetes now supports KubeVirt for backup and recovery.</li>
  <li>[PR #8273][alaypatel07] add server-side validations for spec.topologySpreadConstraints during object creation</li>
  <li>[PR #8049][alicefr] Set RunAsNonRoot as default for the guestfs pod</li>
  <li>[PR #8107][awels] Allow VirtualMachineSnapshot as a VirtualMachineExport source</li>
  <li>[PR #7846][janeczku] Added support for configuring topology spread constraints for virtual machines.</li>
  <li>[PR #8215][alaypatel07] support validation for spec.affinity fields during vmi creation</li>
  <li>[PR #8071][oshoval] Relax networkInterfaceMultiqueue semantics: multi queue will configure only what it can (virtio interfaces).</li>
  <li>[PR #7549][akrejcir] Added new API subresources to expand instancetype and preference.</li>
</ul>]]></content><author><name>kube🤖</name></author><category term="releases" /><category term="release notes" /><category term="changelog" /><summary type="html"><![CDATA[This article provides information about KubeVirt release v0.57.0 changes]]></summary></entry><entry><title type="html">KubeVirt v0.56.0</title><link href="https://kubevirt.io//2022/changelog-v0.56.0.html" rel="alternate" type="text/html" title="KubeVirt v0.56.0" /><published>2022-08-18T00:00:00+00:00</published><updated>2022-08-18T00:00:00+00:00</updated><id>https://kubevirt.io//2022/changelog-v0.56.0</id><content type="html" xml:base="https://kubevirt.io//2022/changelog-v0.56.0.html"><![CDATA[<h2 id="v0560">v0.56.0</h2>

<p>Released on: Thu Aug 18 20:10:29 2022 +0000</p>

<ul>
  <li>[PR #7599][iholder-redhat] Introduce a mechanism to abort non-running migrations - fixes “Unable to cancel live-migration if virt-launcher pod in pending state” bug</li>
  <li>[PR #8027][alaypatel07] Wait deletion to succeed all the way till objects are finalized in perfscale tests</li>
  <li>[PR #8198][rmohr] Improve path handling for non-root virt-launcher workloads</li>
  <li>[PR #8136][iholder-redhat] Fix cgroups unit tests: mock out underlying runc cgroup manager</li>
  <li>[PR #8047][iholder-redhat] Deprecate live migration feature gate</li>
  <li>[PR #7986][iholder-redhat] [Bug-fix]: Windows VM with WSL2 guest fails to migrate</li>
  <li>[PR #7814][machadovilaca] Add VMI filesystem usage metrics</li>
  <li>[PR #7849][AlonaKaplan] [TECH PREVIEW] Introducing passt - a new approach to user-mode networking for virtual machines</li>
  <li>[PR #7991][ShellyKa13] Virtctl memory dump with create flag to create a new pvc</li>
  <li>[PR #8039][lyarwood] The flavor API and associated CRDs of <code class="language-plaintext highlighter-rouge">VirtualMachine{Flavor,ClusterFlavor}</code> are renamed to instancetype and <code class="language-plaintext highlighter-rouge">VirtualMachine{Instancetype,ClusterInstancetype}</code>.</li>
  <li>[PR #8112][AlonaKaplan] Changing the default of <code class="language-plaintext highlighter-rouge">virtctl expose</code> <code class="language-plaintext highlighter-rouge">ip-family</code> parameter to be empty value instead of IPv4.</li>
  <li>[PR #8073][orenc1] Bump runc to v1.1.2</li>
  <li>[PR #8092][Barakmor1] Bump the version of emicklei/go-restful from 2.15.0 to 2.16.0</li>
  <li>[PR #8053][alromeros] [Bug-fix]: Fix mechanism to fetch fs overhead when CDI resource has a different name</li>
  <li>[PR #8035][0xFelix] Add option to wrap local scp client to scp command</li>
  <li>[PR #7981][lyarwood] Conflicts will now be raised when using flavors if the <code class="language-plaintext highlighter-rouge">VirtualMachine</code> defines any <code class="language-plaintext highlighter-rouge">CPU</code> or <code class="language-plaintext highlighter-rouge">Memory</code> resource requests.</li>
  <li>[PR #8068][awels] Set cache mode to match regular disks on hotplugged disks.</li>
</ul>]]></content><author><name>kube🤖</name></author><category term="releases" /><category term="release notes" /><category term="changelog" /><summary type="html"><![CDATA[This article provides information about KubeVirt release v0.56.0 changes]]></summary></entry><entry><title type="html">Simplifying KubeVirt’s `VirtualMachine` UX with Instancetypes and Preferences</title><link href="https://kubevirt.io//2022/KubeVirt-Introduction-of-instancetypes.html" rel="alternate" type="text/html" title="Simplifying KubeVirt’s `VirtualMachine` UX with Instancetypes and Preferences" /><published>2022-08-12T00:00:00+00:00</published><updated>2022-08-12T00:00:00+00:00</updated><id>https://kubevirt.io//2022/KubeVirt-Introduction-of-instancetypes</id><content type="html" xml:base="https://kubevirt.io//2022/KubeVirt-Introduction-of-instancetypes.html"><![CDATA[<p>KubeVirt’s <a href="https://kubevirt.io/api-reference/main/definitions.html#_v1_virtualmachine"><code class="language-plaintext highlighter-rouge">VirtualMachine</code></a> API contains many advanced options for tuning a virtual machine’s resources and performance that go beyond what typical users need to be aware of. Users have until now been unable to simply define the storage/network they want assigned to their VM and then declare in broad terms what quality of resources and kind of performance they need for their VM. Instead, the user has to be keenly aware how to request specific compute resources alongside all of the performance tunings available on the <a href="https://kubevirt.io/api-reference/main/definitions.html#_v1_virtualmachine"><code class="language-plaintext highlighter-rouge">VirtualMachine</code></a> API and how those tunings impact their guest’s operating system in order to get a desired result.</p>

<p>A common pattern for IaaS is to have abstractions separating the resource sizing and performance of a workload from the user-defined values related to launching their custom application. This pattern is evident across all the major cloud providers (also known as hyperscalers) as well as open source IaaS projects like OpenStack. AWS has <a href="https://aws.amazon.com/ec2/instance-types/">instance types</a>, GCP has <a href="https://cloud.google.com/compute/docs/machine-types#custom_machine_types">machine types</a>, Azure has <a href="https://docs.microsoft.com/en-us/azure/virtual-machines/sizes">instance VM sizes</a>, and OpenStack has <a href="https://docs.openstack.org/nova/latest/user/flavors.html">flavors</a>.</p>

<p>Let’s take AWS for example to help visualize what this abstraction enables. Launching an EC2 instance only requires a few top level arguments; the disk image, instance type, keypair, security group, and subnet:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>aws ec2 run-instances <span class="nt">--image-id</span> ami-xxxxxxxx <span class="se">\</span>
                        <span class="nt">--count</span> 1 <span class="se">\</span>
                        <span class="nt">--instance-type</span> c4.xlarge <span class="se">\</span>
                        <span class="nt">--key-name</span> MyKeyPair <span class="se">\</span>
                        <span class="nt">--security-group-ids</span> sg-903004f8 <span class="se">\</span>
                        <span class="nt">--subnet-id</span> subnet-6e7f829e
</code></pre></div></div>

<p>When creating the EC2 instance the user doesn’t define the amount of resources, what processor to use, how to optimize the performance of the instance, or what hardware to schedule the instance on. Instead, all of that information is wrapped up in that single <code class="language-plaintext highlighter-rouge">--instance-type c4.xlarge</code> CLI argument. <code class="language-plaintext highlighter-rouge">c4</code> denotes a specific performance profile version, in this case from the <code class="language-plaintext highlighter-rouge">Compute Optimized</code> family and <code class="language-plaintext highlighter-rouge">xlarge</code> denotes a specific amount of compute resources provided by the instance type, in this case 4 vCPUs, 7.5 GiB of RAM, 750 Mbps EBS bandwidth, etc.</p>

<p>While hyperscalers can provide predefined types with performance profiles and compute resources already assigned IaaS and virtualization projects such as OpenStack and KubeVirt can only provide the raw abstractions for operators, admins, and even vendors to then create instances of these abstractions specific to each deployment.</p>

<h2 id="instancetype-api">Instancetype API</h2>

<p>The recently renamed instancetype API and associated <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/"><code class="language-plaintext highlighter-rouge">CRDs</code></a> aim to address this by providing KubeVirt users with a set of APIs and abstractions that allow them to make fewer choices when creating a <a href="https://kubevirt.io/api-reference/main/definitions.html#_v1_virtualmachine"><code class="language-plaintext highlighter-rouge">VirtualMachine</code></a> while still ending up with a working, performant guest at runtime.</p>

<h2 id="virtualmachineinstancetype">VirtualMachineInstancetype</h2>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">instancetype.kubevirt.io/v1alpha1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachineInstancetype</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">example-instancetype</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">cpu</span><span class="pi">:</span>
    <span class="na">guest</span><span class="pi">:</span> <span class="m">1</span>
  <span class="na">memory</span><span class="pi">:</span>
    <span class="na">guest</span><span class="pi">:</span> <span class="s">128Mi</span>
</code></pre></div></div>

<p>KubeVirt now provides two instancetype based <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/"><code class="language-plaintext highlighter-rouge">CRDs</code></a>, a cluster wide <a href="http://kubevirt.io/api-reference/main/definitions.html#_v1alpha1_virtualmachineclusterinstancetype"><code class="language-plaintext highlighter-rouge">VirtualMachineClusterInstancetype</code></a> and a namespaced <a href="http://kubevirt.io/api-reference/main/definitions.html#_v1alpha1_virtualmachineinstancetype"><code class="language-plaintext highlighter-rouge">VirtualMachineInstancetype</code></a>. These <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/"><code class="language-plaintext highlighter-rouge">CRDs</code></a> encapsulate the following resource related characteristics of a <a href="http://kubevirt.io/api-reference/main/definitions.html#_v1alpha1_virtualmachine"><code class="language-plaintext highlighter-rouge">VirtualMachine</code></a> through a shared <a href="http://kubevirt.io/api-reference/main/definitions.html#_v1alpha1_virtualmachineinstancetypespec"><code class="language-plaintext highlighter-rouge">VirtualMachineInstancetypeSpec</code></a>:</p>

<ul>
  <li><a href="http://kubevirt.io/api-reference/main/definitions.html#_v1alpha1_cpuinstancetype">CPU</a> : Required number of vCPUs presented to the guest</li>
  <li><a href="http://kubevirt.io/api-reference/main/definitions.html#_v1alpha1_memoryinstancetype">Memory</a> : Required amount of memory presented to the guest</li>
  <li><a href="http://kubevirt.io/api-reference/main/definitions.html#_v1_gpu">GPUs</a> : Optional list of vGPUs to passthrough</li>
  <li><a href="http://kubevirt.io/api-reference/main/definitions.html#_v1_hostdevice">HostDevices</a>: Optional list of HostDevices to passthrough</li>
  <li><a href="`string`">IOThreadsPolicy</a> : Optional IOThreadsPolicy to be used</li>
  <li><a href="http://kubevirt.io/api-reference/main/definitions.html#_v1_launchsecurity">LaunchSecurity</a>: Optional LaunchSecurity to be used</li>
</ul>

<p>Anything provided within an instancetype cannot be overridden within a <a href="http://kubevirt.io/api-reference/main/definitions.html#_v1alpha1_virtualmachine"><code class="language-plaintext highlighter-rouge">VirtualMachine</code></a>. For example, <code class="language-plaintext highlighter-rouge">CPU</code> and <code class="language-plaintext highlighter-rouge">Memory</code> are both required attributes of an instancetype. If a user makes any requests for <code class="language-plaintext highlighter-rouge">CPU</code> or <code class="language-plaintext highlighter-rouge">Memory</code> resources within their <a href="http://kubevirt.io/api-reference/main/definitions.html#_v1alpha1_virtualmachine"><code class="language-plaintext highlighter-rouge">VirtualMachine</code></a>, the instancetype will conflict and the request will be rejected.</p>

<h2 id="virtualmachinepreference">VirtualMachinePreference</h2>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">instancetype.kubevirt.io/v1alpha1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachinePreference</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">example-preference</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">devices</span><span class="pi">:</span>
    <span class="na">preferredDiskBus</span><span class="pi">:</span> <span class="s">virtio</span>
    <span class="na">preferredInterfaceModel</span><span class="pi">:</span> <span class="s">virtio</span>
</code></pre></div></div>

<p>KubeVirt also provides two further preference based <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/"><code class="language-plaintext highlighter-rouge">CRDs</code></a>, again a cluster-wide <a href="https://kubevirt.io/api-reference/main/definitions.html#_v1alpha1_virtualmachineclusterpreference"><code class="language-plaintext highlighter-rouge">VirtualMachineClusterPreference</code></a> and namespaced <a href="https://kubevirt.io/api-reference/main/definitions.html#_v1alpha1_virtualmachinepreference"><code class="language-plaintext highlighter-rouge">VirtualMachinePreference</code></a>. These <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/"><code class="language-plaintext highlighter-rouge">CRDs</code></a> encapsulate the preferred value of any remaining attributes of a <a href="http://kubevirt.io/api-reference/main/definitions.html#_v1alpha1_virtualmachine"><code class="language-plaintext highlighter-rouge">VirtualMachine</code></a> required to run a given workload, again this is through a shared <a href="http://kubevirt.io/api-reference/main/definitions.html#_v1alpha1_virtualmachinepreferencespec"><code class="language-plaintext highlighter-rouge">VirtualMachinePreferenceSpec</code></a>.</p>

<p>Unlike instancetypes, preferences only represent the preferred values and as such can be overridden by values in the <a href="http://kubevirt.io/api-reference/main/definitions.html#_v1alpha1_virtualmachine"><code class="language-plaintext highlighter-rouge">VirtualMachine</code></a> provided by the user.</p>

<h2 id="virtualmachineinstancetypepreferencematcher">VirtualMachine{Instancetype,Preference}Matcher</h2>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">kubevirt.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachine</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">example-vm</span>
<span class="na">spec</span><span class="pi">:</span>
<span class="pi">[</span><span class="nv">..</span><span class="pi">]</span>
  <span class="na">instancetype</span><span class="pi">:</span>
    <span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachineInstancetype</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">example-instancetype</span>
  <span class="na">preference</span><span class="pi">:</span>
    <span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachinePreference</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">example-preference</span>
<span class="pi">[</span><span class="nv">..</span><span class="pi">]</span>
</code></pre></div></div>

<p>The previous instancetype and preference CRDs are matched to a given <a href="http://kubevirt.io/api-reference/main/definitions.html#_v1alpha1_virtualmachine"><code class="language-plaintext highlighter-rouge">VirtualMachine</code></a> through the use of a matcher. Each matcher consists of the following:</p>

<ul>
  <li>Name (string): Name of the resource being referenced</li>
  <li>Kind (string):  Optional, defaults to the cluster wide CRD kinds of <code class="language-plaintext highlighter-rouge">VirtualMachineClusterInstancetype</code> or <code class="language-plaintext highlighter-rouge">VirtualMachineClusterPreference</code></li>
  <li>RevisionName (string) : Optional, name of a <a href="https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/controller-revision-v1/">ControllerRevision</a> containing a copy of the <a href="http://kubevirt.io/api-reference/main/definitions.html#_v1alpha1_virtualmachineinstancetypespec"><code class="language-plaintext highlighter-rouge">VirtualMachineInstancetypeSpec</code></a> or <a href="http://kubevirt.io/api-reference/main/definitions.html#_v1alpha1_virtualmachinepreferencespec"><code class="language-plaintext highlighter-rouge">VirtualMachinePreferenceSpec</code></a> taken when the <a href="http://kubevirt.io/api-reference/main/definitions.html#_v1alpha1_virtualmachine"><code class="language-plaintext highlighter-rouge">VirtualMachine</code></a> is first started.</li>
</ul>

<h2 id="virtualmachineinstancepreset-deprecation">VirtualMachineInstancePreset Deprecation</h2>

<p>The new instancetype API and CRDs conflict somewhat with the existing <a href="https://kubevirt.io/api-reference/main/definitions.html#_v1_virtualmachineinstancepreset"><code class="language-plaintext highlighter-rouge">VirtualMachineInstancePreset</code></a> CRD. The approach taken by the CRD has also been removed in core k8s so, as advertised on the <a href="https://groups.google.com/g/kubevirt-dev/c/eM7JaDV_EU8">mailing list</a>, I have started the <a href="https://github.com/kubevirt/kubevirt/pull/8069">process of deprecating</a> <a href="https://kubevirt.io/api-reference/main/definitions.html#_v1_virtualmachineinstancepreset"><code class="language-plaintext highlighter-rouge">VirtualMachineInstancePreset</code></a> in favor of the Instancetype CRDs listed above.</p>

<h2 id="examples">Examples</h2>

<p>The following example is taken from the <a href="https://kubevirt.io/user-guide/virtual_machines/instancetypes/">KubeVirt User Guide</a>:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">$ cat &lt;&lt; EOF | kubectl apply -f -</span> 
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">instancetype.kubevirt.io/v1alpha1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachineInstancetype</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">cmedium</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">cpu</span><span class="pi">:</span>
    <span class="na">guest</span><span class="pi">:</span> <span class="m">1</span>
  <span class="na">memory</span><span class="pi">:</span>
    <span class="na">guest</span><span class="pi">:</span> <span class="s">1Gi</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">instancetype.kubevirt.io/v1alpha1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachinePreference</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">fedora</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">devices</span><span class="pi">:</span>
    <span class="na">preferredDiskBus</span><span class="pi">:</span> <span class="s">virtio</span>
    <span class="na">preferredInterfaceModel</span><span class="pi">:</span> <span class="s">virtio</span>
    <span class="na">preferredRng</span><span class="pi">:</span> <span class="pi">{}</span>
  <span class="na">features</span><span class="pi">:</span>
    <span class="na">preferredAcpi</span><span class="pi">:</span> <span class="pi">{}</span>
    <span class="na">preferredSmm</span><span class="pi">:</span> <span class="pi">{}</span>
  <span class="na">firmware</span><span class="pi">:</span>
    <span class="na">preferredUseEfi</span><span class="pi">:</span> <span class="no">true</span>
    <span class="na">preferredUseSecureBoot</span><span class="pi">:</span> <span class="no">true</span>    
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">kubevirt.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachine</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">creationTimestamp</span><span class="pi">:</span> <span class="no">null</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">fedora</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">instancetype</span><span class="pi">:</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">cmedium</span>
    <span class="na">kind</span><span class="pi">:</span> <span class="s">virtualMachineInstancetype</span>
  <span class="na">preference</span><span class="pi">:</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">fedora</span>
    <span class="na">kind</span><span class="pi">:</span> <span class="s">virtualMachinePreference</span>
  <span class="na">runStrategy</span><span class="pi">:</span> <span class="s">Always</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">creationTimestamp</span><span class="pi">:</span> <span class="no">null</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">domain</span><span class="pi">:</span>
        <span class="na">devices</span><span class="pi">:</span> <span class="pi">{}</span>
      <span class="na">volumes</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">containerDisk</span><span class="pi">:</span>
          <span class="na">image</span><span class="pi">:</span> <span class="s">quay.io/containerdisks/fedora:latest</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
      <span class="pi">-</span> <span class="na">cloudInitNoCloud</span><span class="pi">:</span>
          <span class="na">userData</span><span class="pi">:</span> <span class="pi">|-</span>
            <span class="s">#cloud-config</span>
            <span class="s">users:</span>
              <span class="s">- name: admin</span>
                <span class="s">sudo: ALL=(ALL) NOPASSWD:ALL</span>
                <span class="s">ssh_authorized_keys:</span>
                  <span class="s">- ssh-rsa AAAA...</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinit</span>
<span class="s">EOF</span>
</code></pre></div></div>

<p>We can compare the original <code class="language-plaintext highlighter-rouge">VirtualMachine</code> spec with that of the running <code class="language-plaintext highlighter-rouge">VirtualMachineInstance</code> to confirm our instancetype and preferences have been applied using the following <code class="language-plaintext highlighter-rouge">diff</code> command:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>diff <span class="nt">--color</span> <span class="nt">-u</span> &lt;<span class="o">(</span> kubectl get vms/fedora <span class="nt">-o</span> json | jq .spec.template.spec<span class="o">)</span> &lt;<span class="o">(</span> kubectl get vmis/fedora <span class="nt">-o</span> json | jq .spec<span class="o">)</span>
<span class="o">[</span>..]
 <span class="o">{</span>
   <span class="s2">"domain"</span>: <span class="o">{</span>
-    <span class="s2">"devices"</span>: <span class="o">{}</span>,
+    <span class="s2">"cpu"</span>: <span class="o">{</span>
+      <span class="s2">"cores"</span>: 1,
+      <span class="s2">"model"</span>: <span class="s2">"host-model"</span>,
+      <span class="s2">"sockets"</span>: 1,
+      <span class="s2">"threads"</span>: 1
+    <span class="o">}</span>,
+    <span class="s2">"devices"</span>: <span class="o">{</span>
+      <span class="s2">"disks"</span>: <span class="o">[</span>
+        <span class="o">{</span>
+          <span class="s2">"disk"</span>: <span class="o">{</span>
+            <span class="s2">"bus"</span>: <span class="s2">"virtio"</span>
+          <span class="o">}</span>,
+          <span class="s2">"name"</span>: <span class="s2">"containerdisk"</span>
+        <span class="o">}</span>,
+        <span class="o">{</span>
+          <span class="s2">"disk"</span>: <span class="o">{</span>
+            <span class="s2">"bus"</span>: <span class="s2">"virtio"</span>
+          <span class="o">}</span>,
+          <span class="s2">"name"</span>: <span class="s2">"cloudinit"</span>
+        <span class="o">}</span>
+      <span class="o">]</span>,
+      <span class="s2">"interfaces"</span>: <span class="o">[</span>
+        <span class="o">{</span>
+          <span class="s2">"bridge"</span>: <span class="o">{}</span>,
+          <span class="s2">"model"</span>: <span class="s2">"virtio"</span>,
+          <span class="s2">"name"</span>: <span class="s2">"default"</span>
+        <span class="o">}</span>
+      <span class="o">]</span>,
+      <span class="s2">"rng"</span>: <span class="o">{}</span>
+    <span class="o">}</span>,
+    <span class="s2">"features"</span>: <span class="o">{</span>
+      <span class="s2">"acpi"</span>: <span class="o">{</span>
+        <span class="s2">"enabled"</span>: <span class="nb">true</span>
+      <span class="o">}</span>,
+      <span class="s2">"smm"</span>: <span class="o">{</span>
+        <span class="s2">"enabled"</span>: <span class="nb">true</span>
+      <span class="o">}</span>
+    <span class="o">}</span>,
+    <span class="s2">"firmware"</span>: <span class="o">{</span>
+      <span class="s2">"bootloader"</span>: <span class="o">{</span>
+        <span class="s2">"efi"</span>: <span class="o">{</span>
+          <span class="s2">"secureBoot"</span>: <span class="nb">true</span>
+        <span class="o">}</span>
+      <span class="o">}</span>,
+      <span class="s2">"uuid"</span>: <span class="s2">"98f07cdd-96da-5880-b6c7-1a5700b73dc4"</span>
+    <span class="o">}</span>,
     <span class="s2">"machine"</span>: <span class="o">{</span>
       <span class="s2">"type"</span>: <span class="s2">"q35"</span>
     <span class="o">}</span>,
-    <span class="s2">"resources"</span>: <span class="o">{}</span>
+    <span class="s2">"memory"</span>: <span class="o">{</span>
+      <span class="s2">"guest"</span>: <span class="s2">"1Gi"</span>
+    <span class="o">}</span>,
+    <span class="s2">"resources"</span>: <span class="o">{</span>
+      <span class="s2">"requests"</span>: <span class="o">{</span>
+        <span class="s2">"memory"</span>: <span class="s2">"1Gi"</span>
+      <span class="o">}</span>
+    <span class="o">}</span>
   <span class="o">}</span>,
+  <span class="s2">"networks"</span>: <span class="o">[</span>
+    <span class="o">{</span>
+      <span class="s2">"name"</span>: <span class="s2">"default"</span>,
+      <span class="s2">"pod"</span>: <span class="o">{}</span>
+    <span class="o">}</span>
+  <span class="o">]</span>,
   <span class="s2">"volumes"</span>: <span class="o">[</span>
     <span class="o">{</span>
       <span class="s2">"containerDisk"</span>: <span class="o">{</span>
-        <span class="s2">"image"</span>: <span class="s2">"quay.io/containerdisks/fedora:latest"</span>
+        <span class="s2">"image"</span>: <span class="s2">"quay.io/containerdisks/fedora:latest"</span>,
+        <span class="s2">"imagePullPolicy"</span>: <span class="s2">"Always"</span>
       <span class="o">}</span>,
       <span class="s2">"name"</span>: <span class="s2">"containerdisk"</span>
     <span class="o">}</span>,
</code></pre></div></div>

<h2 id="future-work">Future work</h2>

<p>There’s still plenty of work required before the API and CRDs can move from their current <code class="language-plaintext highlighter-rouge">alpha</code> version to <code class="language-plaintext highlighter-rouge">beta</code>. We have a specific <a href="https://github.com/kubevirt/kubevirt/issues/8235"><code class="language-plaintext highlighter-rouge">kubevirt/kubevirt</code> issue tracking our progress to <code class="language-plaintext highlighter-rouge">beta</code></a>. As set out there and in the <a href="https://github.com/kubevirt/community/blob/main/docs/api-graduation-guidelines.md">KubeVirt community API Graduation Phase Expecations</a>, part of this work is to seek feedback from the wider community so please do feel free to chime in there with any and all feedback on the API and CRDs.</p>

<p>You can also track our work on this API through the <a href="https://github.com/kubevirt/kubevirt/labels/area%2Finstancetype"><code class="language-plaintext highlighter-rouge">area/instancetype</code> tag</a> or my <a href="https://blog.yarwood.me.uk/tags/instancetypes/">personal blog</a> where I will be posting <a href="https://blog.yarwood.me.uk/2022/07/21/kubevirt_instancetype_update_2/">regular updates</a> and <a href="https://blog.yarwood.me.uk/2022/08/03/kubevirt_instancetype_demo_2/">demos</a> for instancetypes.</p>]]></content><author><name>Lee Yarwood</name></author><category term="news" /><category term="kubevirt" /><category term="kubernetes" /><category term="virtual machine" /><category term="VM" /><category term="instancetypes" /><category term="preferences" /><category term="VirtualMachine" /><category term="VirtualMachineInstancetype" /><category term="VirtualMachinePreference" /><summary type="html"><![CDATA[An introduction to Instancetypes and preferences in KubeVirt]]></summary></entry><entry><title type="html">KubeVirt: installing Microsoft Windows 11 from an ISO</title><link href="https://kubevirt.io//2022/KubeVirt-installing_Microsoft_Windows_11_from_an_iso.html" rel="alternate" type="text/html" title="KubeVirt: installing Microsoft Windows 11 from an ISO" /><published>2022-08-02T00:00:00+00:00</published><updated>2022-08-02T00:00:00+00:00</updated><id>https://kubevirt.io//2022/KubeVirt-installing_Microsoft_Windows_11_from_an_iso</id><content type="html" xml:base="https://kubevirt.io//2022/KubeVirt-installing_Microsoft_Windows_11_from_an_iso.html"><![CDATA[<p>This blog post describes a simple way to deploy a Windows 11 VM with KubeVirt, using an installation ISO as a starting point.<br />
Although only tested with Windows 11, the steps described here should also work to deploy other recent versions of Windows.</p>

<h2 id="pre-requisites">Pre-requisites</h2>

<ul>
  <li>You’ll need a Kubernetes cluster with worker node(s) that have at least 6GB of available memory</li>
  <li><a href="https://kubevirt.io/user-guide">KubeVirt</a> and <a href="https://github.com/kubevirt/containerized-data-importer/blob/main/README.md">CDI</a> both deployed on the cluster</li>
  <li>A storage backend, such as <a href="https://ceph.com/">Rook Ceph</a></li>
  <li>A Windows iso. One can be found at <a href="https://www.microsoft.com/software-download/windows11">https://www.microsoft.com/software-download/windows11</a></li>
</ul>

<p>A suitable test cluster can easily be deployed thanks to KubeVirtCI by running the following commands from the <a href="https://github.com/kubevirt/kubevirt">KubeVirt source repository</a>:</p>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">export </span><span class="nv">KUBEVIRT_MEMORY_SIZE</span><span class="o">=</span>8192M
<span class="nv">$ </span><span class="nb">export </span><span class="nv">KUBEVIRT_STORAGE</span><span class="o">=</span>rook-ceph-default
<span class="nv">$ </span>make cluster-up <span class="o">&amp;&amp;</span> make cluster-sync
</code></pre></div></div>

<h2 id="preparation">Preparation</h2>

<p>Before the virtual machine can be created, we need to setup storage volumes for the ISO and the drive, and write the appropriate VM(I) yaml.</p>

<ol>
  <li>
    <p>Uploading the ISO to a PVC</p>

    <p>KubeVirt provides a simple tool that is able to do that for us: <code class="language-plaintext highlighter-rouge">virtctl</code>.<br />
Here’s the command to upload the ISO, just replace <code class="language-plaintext highlighter-rouge">/storage/win11.iso</code> with the path to your Windows 11 ISO:
<code class="language-plaintext highlighter-rouge">virtctl image-upload pvc win11cd-pvc --size 6Gi --image-path=/storage/win11.iso --insecure</code></p>
  </li>
  <li>
    <p>Creating a persistent volume to use as the Windows drive</p>

    <p>This will depend on the storage configuration of your cluster.
The following yaml, to apply to the cluster using <code class="language-plaintext highlighter-rouge">kubectl create</code>, should work just fine on a KubeVirtCI cluster:</p>

    <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">PersistentVolume</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">task-pv-volume</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">type</span><span class="pi">:</span> <span class="s">local</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">storageClassName</span><span class="pi">:</span> <span class="s">hostpath</span>
  <span class="na">capacity</span><span class="pi">:</span>
    <span class="na">storage</span><span class="pi">:</span> <span class="s">15Gi</span>
  <span class="na">accessModes</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">ReadWriteOnce</span>
  <span class="na">hostPath</span><span class="pi">:</span>
    <span class="na">path</span><span class="pi">:</span> <span class="s2">"</span><span class="s">/tmp/hostImages/win11"</span>
</code></pre></div>    </div>
  </li>
</ol>

<div class="premonition note"><div class="fa fa-check-square"></div><div class="content"><p class="header">Note</p><p>Microsoft actually <a href="https://docs.microsoft.com/en-us/windows/whats-new/windows-11-requirements">recommends</a> at least 64GB of storage.
But, unlike some other requirements, the installer will accept smaller disks.
This is convenient when testing with KubeVirtCI, as nodes only have about 20GB of free space.
However, please bear in mind that such a small drive should only be used for testing purposes, and might lead to instabilities.</p>


</div></div>
<ol>
  <li>
    <p>Creating a persistent volume claim (PVC) for the drive</p>

    <p>Once again, your milage may vary, but the following PVC yaml works fine on KubeVirtCI:</p>

    <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">PersistentVolumeClaim</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">disk-windows</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">accessModes</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">ReadWriteOnce</span>
  <span class="na">resources</span><span class="pi">:</span>
    <span class="na">requests</span><span class="pi">:</span>
      <span class="na">storage</span><span class="pi">:</span> <span class="s">15Gi</span>
  <span class="na">storageClassName</span><span class="pi">:</span> <span class="s">hostpath</span>
</code></pre></div>    </div>

    <p>The name of PVC, <code class="language-plaintext highlighter-rouge">disk-windows</code> here, will be used in the yaml of the VM(I) as the main volume.</p>
  </li>
  <li>
    <p>Creating the VM(I) yaml file</p>

    <p>KubeVirt already includes an example <a href="https://github.com/kubevirt/kubevirt/blob/main/examples/vmi-windows.yaml">Windows VMI yaml file</a>, which we’ll use as a starting point here for convenience.<br />
Using a VMI yaml is more than enough for testing purposes, however for more serious applications you might want to consider changing it into a VM.</p>

    <p>First, in the yaml above, bump the memory up to 4Gi, which is a hard requirement of Windows 11. (Windows 10 is happy with 2Gi).</p>

    <p>Then, let’s add the ISO created above.
Add is as a cdrom in the disks section:</p>
    <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="pi">-</span> <span class="na">cdrom</span><span class="pi">:</span>
    <span class="na">bus</span><span class="pi">:</span> <span class="s">sata</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">winiso</span>
</code></pre></div>    </div>
    <p>And the corresponding volume at the bottom:</p>
    <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">winiso</span>
    <span class="na">persistentVolumeClaim</span><span class="pi">:</span>
      <span class="na">claimName</span><span class="pi">:</span> <span class="s">win11cd-pvc</span>
</code></pre></div>    </div>
    <p>Note that the names should match, and that the <code class="language-plaintext highlighter-rouge">claimName</code> is what we used in the <code class="language-plaintext highlighter-rouge">virtctl</code> command above.</p>

    <p>Here is what the VMI looks like after those changes:</p>
    <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">kubevirt.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachineInstance</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">special</span><span class="pi">:</span> <span class="s">vmi-windows</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">vmi-windows</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">domain</span><span class="pi">:</span>
    <span class="na">clock</span><span class="pi">:</span>
      <span class="na">timer</span><span class="pi">:</span>
        <span class="na">hpet</span><span class="pi">:</span>
          <span class="na">present</span><span class="pi">:</span> <span class="no">false</span>
        <span class="na">hyperv</span><span class="pi">:</span> <span class="pi">{}</span>
        <span class="na">pit</span><span class="pi">:</span>
          <span class="na">tickPolicy</span><span class="pi">:</span> <span class="s">delay</span>
        <span class="na">rtc</span><span class="pi">:</span>
          <span class="na">tickPolicy</span><span class="pi">:</span> <span class="s">catchup</span>
      <span class="na">utc</span><span class="pi">:</span> <span class="pi">{}</span>
    <span class="na">cpu</span><span class="pi">:</span>
      <span class="na">cores</span><span class="pi">:</span> <span class="m">2</span>
    <span class="na">devices</span><span class="pi">:</span>
      <span class="na">disks</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">disk</span><span class="pi">:</span>
          <span class="na">bus</span><span class="pi">:</span> <span class="s">sata</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">pvcdisk</span>
      <span class="pi">-</span> <span class="na">cdrom</span><span class="pi">:</span>
          <span class="na">bus</span><span class="pi">:</span> <span class="s">sata</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">winiso</span>
      <span class="na">interfaces</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">masquerade</span><span class="pi">:</span> <span class="pi">{}</span>
        <span class="na">model</span><span class="pi">:</span> <span class="s">e1000</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">default</span>
      <span class="na">tpm</span><span class="pi">:</span> <span class="pi">{}</span>
    <span class="na">features</span><span class="pi">:</span>
      <span class="na">acpi</span><span class="pi">:</span> <span class="pi">{}</span>
      <span class="na">apic</span><span class="pi">:</span> <span class="pi">{}</span>
      <span class="na">hyperv</span><span class="pi">:</span>
        <span class="na">relaxed</span><span class="pi">:</span> <span class="pi">{}</span>
        <span class="na">spinlocks</span><span class="pi">:</span>
          <span class="na">spinlocks</span><span class="pi">:</span> <span class="m">8191</span>
        <span class="na">vapic</span><span class="pi">:</span> <span class="pi">{}</span>
      <span class="na">smm</span><span class="pi">:</span> <span class="pi">{}</span>
    <span class="na">firmware</span><span class="pi">:</span>
      <span class="na">bootloader</span><span class="pi">:</span>
        <span class="na">efi</span><span class="pi">:</span>
          <span class="na">secureBoot</span><span class="pi">:</span> <span class="no">true</span>
      <span class="na">uuid</span><span class="pi">:</span> <span class="s">5d307ca9-b3ef-428c-8861-06e72d69f223</span>
    <span class="na">resources</span><span class="pi">:</span>
      <span class="na">requests</span><span class="pi">:</span>
        <span class="na">memory</span><span class="pi">:</span> <span class="s">4Gi</span>
  <span class="na">networks</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">default</span>
    <span class="na">pod</span><span class="pi">:</span> <span class="pi">{}</span>
  <span class="na">terminationGracePeriodSeconds</span><span class="pi">:</span> <span class="m">0</span>
  <span class="na">volumes</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">pvcdisk</span>
    <span class="na">persistentVolumeClaim</span><span class="pi">:</span>
      <span class="na">claimName</span><span class="pi">:</span> <span class="s">disk-windows</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">winiso</span>
    <span class="na">persistentVolumeClaim</span><span class="pi">:</span>
      <span class="na">claimName</span><span class="pi">:</span> <span class="s">win11cd-pvc</span>
</code></pre></div>    </div>
  </li>
</ol>

<div class="premonition note"><div class="fa fa-check-square"></div><div class="content"><p class="header">Note</p><p>When customizing this VMI definition or creating your own, please keep in mind that the TPM device and the UEFI firmware with SecureBoot are both hard requirements of Windows 11.
Not having them will cause the Windows 11 installation to fail early. Please also note that the SMM CPU feature is required for UEFI + SecureBoot.
However, they can all be omitted in the case of a Windows 10 VM(I).
Finally, we do not currently support TPM persistence, so any secret stored in the emulated TPM will be lost next time you boot the VMI.
For example, do not enable BitLocker, as it will fail to find the encryption key next boot and you will have to manually enter the (55 characters!) recovery key each boot.</p>


</div></div>
<h2 id="windows-installation">Windows installation</h2>

<p>You should now be able to create the VMI and start the Windows installation process.<br />
Just use kubectl to start the VMI created above: <code class="language-plaintext highlighter-rouge">kubectl create -f vmi-windows.yaml</code>.<br />
Shortly after, open a VNC session to it using <code class="language-plaintext highlighter-rouge">virtctl vnc vmi-windows</code> (keep trying until the VMI is running and the VNC session pops up).<br />
You should now see the boot screen, and shortly after a prompt to “Press any key to boot from CD or DVD…”. You have a few seconds to do so or the VM will fail to boot.
Then just follow the steps to install Windows.</p>

<h2 id="virtio-drivers-installation-optional">VirtIO drivers installation (optional)</h2>

<p>Once Windows is installed, it’s a good ideas to install the <a href="http://www.linux-kvm.org/page/Virtio">VirtIO</a> drivers inside the VM, as they can drastically improve performance.
The latest version can be downloaded <a href="https://fedorapeople.org/groups/virt/virtio-win/direct-downloads/latest-virtio/">here</a>.
<code class="language-plaintext highlighter-rouge">virtio-win-gt-x64.msi</code> is the simplest package to install, as you just have to run it as Administrator.</p>

<p>Alternatively, KubeVirt has a containerdisk image that can be mounted inside the VM.<br />
To use it, just add a simple cdrom disk to the VMI, like:</p>
<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="pi">-</span> <span class="na">cdrom</span><span class="pi">:</span>
    <span class="na">bus</span><span class="pi">:</span> <span class="s">sata</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">virtio</span>
</code></pre></div></div>
<p>and the volume:</p>
<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="pi">-</span> <span class="na">containerDisk</span><span class="pi">:</span>
      <span class="na">image</span><span class="pi">:</span> <span class="s">kubevirt/virtio-container-disk</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">virtio</span>
</code></pre></div></div>
<p>When using KubeVirtCI, a local copy of the image is also available at <code class="language-plaintext highlighter-rouge">registry:5000/kubevirt/virtio-container-disk:devel</code>.</p>

<h2 id="further-performance-improvements">Further performance improvements</h2>

<p>Windows is quite resource-hungry, and you might find that the VM created above is too slow, even with the VirtIO drivers installed.<br />
Here are a few steps you can take to improve things:</p>
<ul>
  <li>Increasing the RAM is always a good idea, if you have enough available of course.</li>
  <li>Increasing the number of CPUs, and/or using CPUManager to assign dedicated CPU to the VM should also help a lot.</li>
  <li>Once the VirtIO drivers are installed, the main drive can also be switched from <code class="language-plaintext highlighter-rouge">sata</code> to <code class="language-plaintext highlighter-rouge">virtio</code>, and the attached CDROMs can be removed.</li>
</ul>]]></content><author><name>Jed Lejosne</name></author><category term="news" /><category term="kubevirt" /><category term="kubernetes" /><category term="virtual machine" /><category term="Microsoft Windows kubernetes" /><category term="Microsoft Windows container" /><category term="Windows" /><summary type="html"><![CDATA[This blog post describes how to create a Microsoft Windows 11 virtual machine with KubeVirt]]></summary></entry><entry><title type="html">KubeVirt v0.55.0</title><link href="https://kubevirt.io//2022/changelog-v0.55.0.html" rel="alternate" type="text/html" title="KubeVirt v0.55.0" /><published>2022-07-14T00:00:00+00:00</published><updated>2022-07-14T00:00:00+00:00</updated><id>https://kubevirt.io//2022/changelog-v0.55.0</id><content type="html" xml:base="https://kubevirt.io//2022/changelog-v0.55.0.html"><![CDATA[<h2 id="v0550">v0.55.0</h2>

<p>Released on: Thu Jul 14 16:33:25 2022 +0000</p>

<ul>
  <li>[PR #7336][iholder-redhat] Introduce clone CRD, controller and API</li>
  <li>[PR #7791][iholder-redhat] Introduction of an initial deprecation policy</li>
  <li>[PR #7875][lyarwood] <code class="language-plaintext highlighter-rouge">ControllerRevisions</code> of any <code class="language-plaintext highlighter-rouge">VirtualMachineFlavorSpec</code> or <code class="language-plaintext highlighter-rouge">VirtualMachinePreferenceSpec</code> are stored during the initial start of a <code class="language-plaintext highlighter-rouge">VirtualMachine</code> and used for subsequent restarts ensuring changes to the original <code class="language-plaintext highlighter-rouge">VirtualMachineFlavor</code> or <code class="language-plaintext highlighter-rouge">VirtualMachinePreference</code> do not modify the <code class="language-plaintext highlighter-rouge">VirtualMachine</code> and the <code class="language-plaintext highlighter-rouge">VirtualMachineInstance</code> it creates.</li>
  <li>[PR #8011][fossedihelm] Increase virt-launcher memory overhead</li>
  <li>[PR #7963][qinqon] Bump alpine_with_test_tooling</li>
  <li>[PR #7881][ShellyKa13] Enable memory dump to be included in VMSnapshot</li>
  <li>[PR #7926][qinqon] tests: Move main clean function to global AfterEach and create a VM per each infra_test.go Entry.</li>
  <li>[PR #7845][janeczku] Fixed a bug that caused <code class="language-plaintext highlighter-rouge">make generate</code> to fail when API code comments contain backticks. (#7844, @janeczku)</li>
  <li>[PR #7932][marceloamaral] Addition of kubevirt_vmi_migration_phase_transition_time_from_creation_seconds metric to monitor how long it takes to transition a VMI Migration object to a specific phase from creation time.</li>
  <li>[PR #7879][marceloamaral] Faster VM phase transitions thanks to an increased virt-controller QPS/Burst</li>
  <li>[PR #7807][acardace] make cloud-init ‘instance-id’ persistent across reboots</li>
  <li>[PR #7928][iholder-redhat] bugfix: node-labeller now removes “host-model-cpu.node.kubevirt.io/” and “host-model-required-features.node.kubevirt.io/” prefixes</li>
  <li>[PR #7841][jean-edouard] Non-root VMs will now migrate to root VMs after a cluster disables non-root.</li>
  <li>[PR #7933][akalenyu] BugFix: Fix vm restore in case of restore size bigger then PVC requested size</li>
  <li>[PR #7919][lyarwood] Device preferences are now applied to any default network interfaces or missing volume disks added to a <code class="language-plaintext highlighter-rouge">VirtualMachineInstance</code> at runtime.</li>
  <li>[PR #7910][qinqon] tests: Create the expected readiness probe instead of liveness</li>
  <li>[PR #7732][acardace] Prevent virt-handler from starting a migration twice</li>
  <li>[PR #7594][alicefr] Enable to run libguestfs-tools pod to run as noroot user</li>
  <li>[PR #7811][raspbeep] User now gets information about the type of commands which the guest agent does not support.</li>
  <li>[PR #7590][awels] VMExport allows filesystem PVCs to be exported as either disks or directories.</li>
  <li>[PR #7683][alicefr] Add –command and –local-ssh-opts” options to virtctl ssh to execute remote command using local ssh method</li>
</ul>]]></content><author><name>kube🤖</name></author><category term="releases" /><category term="release notes" /><category term="changelog" /><summary type="html"><![CDATA[This article provides information about KubeVirt release v0.55.0 changes]]></summary></entry></feed>